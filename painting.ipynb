{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import cuml.umap as cuml_umap  # GPU ç‰ˆ UMAP\n",
    "from cuml.decomposition import PCA  # GPU ç‰ˆ PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os\n",
    "\n",
    "# 1. è¯»å–åµŒå…¥æ•°æ®ï¼ˆåªè®¡ç®—ç”Ÿæˆæ•°æ®çš„åˆ†å¸ƒï¼‰\n",
    "original_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_original_text.npy\")  \n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_response.npy\")  \n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_knowledge.npy\")  \n",
    "\n",
    "# 2. è®¡ç®—â€œç”Ÿæˆæ•°æ®â€ä¸â€œåŸå§‹æ•°æ®â€çš„ç›¸ä¼¼æ€§ï¼ˆCosine Similarityï¼‰\n",
    "print(\"ğŸ”„ Computing Cosine Similarity with Original Data (GPU)...\")\n",
    "response_similarity = cosine_similarity(generated_response_embeddings, original_embeddings)  \n",
    "knowledge_similarity = cosine_similarity(generated_knowledge_embeddings, original_embeddings)\n",
    "\n",
    "# 3. è·å–æœ€é«˜ç›¸ä¼¼åº¦ï¼ˆå–ä¸æœ€ç›¸ä¼¼çš„åŸå§‹æ•°æ®çš„ç›¸ä¼¼åº¦ï¼‰\n",
    "max_response_similarity = np.max(response_similarity, axis=1)\n",
    "max_knowledge_similarity = np.max(knowledge_similarity, axis=1)\n",
    "\n",
    "# 4. è®¡ç®— Sequence Identity to Trainingï¼ˆè½¬æ¢ä¸º 0-100%ï¼‰\n",
    "sequence_identity = np.concatenate((max_response_similarity, max_knowledge_similarity)) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”\n",
    "\n",
    "# 5. **åˆå¹¶â€œç”Ÿæˆæ•°æ®â€åµŒå…¥**\n",
    "generated_embeddings = np.vstack((generated_response_embeddings, generated_knowledge_embeddings))\n",
    "generated_labels = np.array([\"generated_response\"] * len(generated_response_embeddings) + \n",
    "                            [\"generated_knowledge\"] * len(generated_knowledge_embeddings))\n",
    "\n",
    "# 6. **ä½¿ç”¨ GPU ç‰ˆ PCA é¢„é™ç»´ï¼ˆ4096D â†’ 50Dï¼‰**\n",
    "pca_path = \"outputs/emb_data/reduced_pca_50_generated.npy\"\n",
    "if os.path.exists(pca_path):\n",
    "    print(\"âœ… Loading PCA-reduced data from file...\")\n",
    "    pca_50_embeddings = np.load(pca_path)\n",
    "else:\n",
    "    print(\"ğŸ”„ Applying GPU PCA (4096 â†’ 50) ...\")\n",
    "    pca = PCA(n_components=50, random_state=42)\n",
    "    pca_50_embeddings = pca.fit_transform(generated_embeddings)\n",
    "    np.save(pca_path, pca_50_embeddings)\n",
    "    print(f\"âœ… PCA embeddings saved to {pca_path}\")\n",
    "\n",
    "# 7. **ä½¿ç”¨ GPU åŠ é€Ÿ UMAP é™ç»´**\n",
    "umap_path = \"outputs/emb_data/reduced_umap_2d_generated.npy\"\n",
    "if os.path.exists(umap_path):\n",
    "    print(\"âœ… Loading UMAP-reduced embeddings from file...\")\n",
    "    low_dim_embeddings = np.load(umap_path)\n",
    "else:\n",
    "    print(\"ğŸ”„ Applying GPU UMAP on PCA-reduced data ...\")\n",
    "    umap_reducer = cuml_umap.UMAP(n_components=2, metric=\"cosine\", n_neighbors=15, min_dist=0.3, random_state=42)\n",
    "    low_dim_embeddings = umap_reducer.fit_transform(pca_50_embeddings)\n",
    "    np.save(umap_path, low_dim_embeddings)\n",
    "    print(f\"âœ… UMAP embeddings saved to {umap_path}\")\n",
    "\n",
    "# 8. **å®šä¹‰é¢œè‰²æ˜ å°„ï¼ˆæ¸å˜è‰²ï¼ŒåŸºäºç›¸ä¼¼åº¦ï¼‰**\n",
    "import matplotlib.colors as mcolors\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", [\n",
    "    (100/255, 125/255, 125/255),  # ä½ç›¸ä¼¼åº¦ï¼ˆæ·±è‰²ï¼‰\n",
    "    (160/255, 200/255, 180/255)   # é«˜ç›¸ä¼¼åº¦ï¼ˆäº®è‰²ï¼‰\n",
    "])\n",
    "\n",
    "# 9. **ç»˜åˆ¶å¯è§†åŒ–**\n",
    "plt.figure(figsize=(10, 6))\n",
    "sc = plt.scatter(\n",
    "    low_dim_embeddings[:, 0],  \n",
    "    low_dim_embeddings[:, 1],  \n",
    "    c=sequence_identity,  # é¢œè‰²åŸºäºç›¸ä¼¼åº¦\n",
    "    cmap=cmap,  \n",
    "    alpha=0.7,  \n",
    "    s=30,  \n",
    "    edgecolors=\"black\"\n",
    ")\n",
    "\n",
    "# 10. **æ·»åŠ é¢œè‰²æ¡ï¼ˆColorbarï¼‰**\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"% Sequence Identity to Training\")\n",
    "\n",
    "plt.title(\"Generated Data UMAP (Colored by Similarity to Training Data)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "\n",
    "# **ä¿å­˜å›¾ç‰‡**\n",
    "img_path = \"outputs/emb_data/embedding_umap_generated_similarity_gpu.png\"\n",
    "plt.savefig(img_path, dpi=300)\n",
    "print(f\"ğŸ“¸ Visualization saved as {img_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ç»˜åˆ¶ æŸ±çŠ¶å›¾\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ä¿®æ­£åçš„åŸºå‡†åˆ—è¡¨ï¼ˆä¸ data è¡Œæ•°åŒ¹é…ï¼‰\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"FRAMES\", \"AlpacaEval2.0\",\n",
    "    \"ArenaHard\", \"LiveCodeBench\", \"MATH-500\", \"CNMO 2024\", \"C-Eval\"\n",
    "]\n",
    "\n",
    "# æ¨¡å‹åˆ—è¡¨\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "# 10 è¡Œæ•°æ®\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8]   # C-Eval\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# è®¾ç½®å­å›¾å¸ƒå±€ï¼ˆ2è¡Œ5åˆ—ï¼‰\n",
    "num_rows, num_cols = 2, 5\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "fig.suptitle(\"Benchmark Comparison Across AI Models\", fontsize=20)\n",
    "\n",
    "# éå† benchmarks ç»˜åˆ¶å­å›¾\n",
    "for i, (benchmark, ax) in enumerate(zip(benchmarks, axes.flatten())):\n",
    "    scores = df.iloc[i, :]\n",
    "    \n",
    "    # é¢œè‰²é£æ ¼ï¼ˆæœ€é«˜åˆ†ç”¨æ·±ç´«è‰²ï¼‰\n",
    "    colors = [\"#A7A2FF\" if score < max(scores) else \"#4A3DA3\" for score in scores]\n",
    "\n",
    "    # ç»˜åˆ¶æŸ±çŠ¶å›¾\n",
    "    bars = ax.bar(models, scores, color=colors)\n",
    "\n",
    "    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ–¹æ ‡æ³¨æ•°å€¼\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}%',\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # è®¾ç½®å­å›¾æ ‡é¢˜\n",
    "    ax.set_title(benchmark, fontsize=12)\n",
    "    \n",
    "    # ç¡®ä¿ x è½´åˆ»åº¦æ­£ç¡®\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=20, ha='right', fontsize=8)\n",
    "\n",
    "    # ç»Ÿä¸€ y è½´èŒƒå›´\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "# è°ƒæ•´å¸ƒå±€\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ç»˜åˆ¶å•ç‹¬çš„æŸ±çŠ¶å›¾\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# æ•°æ®æå–è‡ªè¡¨æ ¼\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# é€ä¸ªåŸºå‡†æµ‹è¯•ç»˜åˆ¶å•ç‹¬çš„æŸ±çŠ¶å›¾\n",
    "for i, benchmark in enumerate(benchmarks):\n",
    "    plt.figure(figsize=(6, 4))  # è®¾ç½®å•ä¸ªå›¾è¡¨å¤§å°\n",
    "    \n",
    "    # è·å–å½“å‰åŸºå‡†æµ‹è¯•çš„æ•°æ®\n",
    "    scores = df.iloc[i, :]\n",
    "    \n",
    "    # é¢œè‰²é£æ ¼ï¼ˆæ·¡ç´«è‰² & æ·±ç´«è‰²ï¼‰ï¼Œæœ€é«˜åˆ†ç”¨æ·±ç´«è‰²ï¼Œå…¶ä½™ç”¨æ·¡ç´«è‰²\n",
    "    # colors = [\"#A7A2FF\" if score < max(scores) else \"#4A3DA3\" for score in scores]\n",
    "    colors = [\"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#4A3DA3\"]\n",
    "    # ç»˜åˆ¶æŸ±çŠ¶å›¾\n",
    "    bars = plt.bar(models, scores, color=colors)\n",
    "\n",
    "    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ–¹æ ‡æ³¨æ•°å€¼\n",
    "    for bar, score in zip(bars, scores):\n",
    "        if score == 0:\n",
    "            pass\n",
    "        else:\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}%',\n",
    "                    ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # å›¾è¡¨æ ‡é¢˜\n",
    "    plt.title(benchmark, fontsize=12)\n",
    "\n",
    "    # y è½´æ ‡ç­¾\n",
    "    plt.ylabel(\"Score (%)\", fontsize=8)\n",
    "    \n",
    "    # éšè— x è½´æ ‡ç­¾ï¼Œä»…å±•ç¤ºæ¨¡å‹å\n",
    "    plt.xticks(rotation=20, ha='right', fontsize=8)\n",
    "\n",
    "    # è°ƒæ•´å¸ƒå±€\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # æ˜¾ç¤ºå›¾è¡¨\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç»˜åˆ¶å¤§æŸ±çŠ¶å›¾\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# æ•°æ®æå–è‡ªè¡¨æ ¼\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# ç»˜åˆ¶å¤šä¸ªæŸ±çŠ¶å›¾\n",
    "num_benchmarks = len(benchmarks)\n",
    "num_models = len(models)\n",
    "x = np.arange(num_benchmarks)\n",
    "\n",
    "# è®¾ç½®é¢œè‰²\n",
    "colors = [\"#7ea8be\", \"#4a6fa5\", \"#1f4e79\", \"#7ea8be\", \"#4a6fa5\", \"#FFA500\"]\n",
    "\n",
    "# è°ƒæ•´å›¾è¡¨å°ºå¯¸\n",
    "plt.figure(figsize=(18, 12))  # å¢å¤§å›¾è¡¨å°ºå¯¸\n",
    "\n",
    "for i in range(num_models):\n",
    "    plt.bar(x + i * 0.12, df.iloc[:, i], width=0.12, label=models[i], color=colors[i])\n",
    "\n",
    "plt.xticks(x + 0.3, benchmarks, rotation=90, fontsize=12)  # è°ƒæ•´ x è½´å­—ä½“å¤§å°\n",
    "plt.yticks(fontsize=12)  # è°ƒæ•´ y è½´å­—ä½“å¤§å°\n",
    "plt.ylabel(\"Score (%)\", fontsize=14)  # è°ƒæ•´ y è½´æ ‡ç­¾å­—ä½“\n",
    "plt.title(\"Benchmark Comparison Across AI Models\", fontsize=18)  # å¢å¤§æ ‡é¢˜å­—ä½“\n",
    "plt.legend(fontsize=12)  # å¢å¤§å›¾ä¾‹å­—ä½“\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
