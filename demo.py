# from modelscope import AutoModelForCausalLM, AutoTokenizer

# model_name = "/mnt/sda/wyp/models/Qwen3-8B-Base"

# # load the tokenizer and the model
# tokenizer = AutoTokenizer.from_pretrained(model_name)
# model = AutoModelForCausalLM.from_pretrained(
#     model_name,
#     torch_dtype="auto",
#     device_map="auto"
# )

# # prepare the model input
# prompt = "Give me a short introduction to large language model."
# messages = [
#     {"role": "user", "content": prompt}
# ]
# text = tokenizer.apply_chat_template(
#     messages,
#     tokenize=False,
#     add_generation_prompt=True,
#     enable_thinking=True # Switches between thinking and non-thinking modes. Default is True.
# )
# model_inputs = tokenizer([text], return_tensors="pt").to(model.device)

# # conduct text completion
# generated_ids = model.generate(
#     **model_inputs,
#     max_new_tokens=32768
# )
# output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() 

# # parsing thinking content
# try:
#     # rindex finding 151668 (</think>)
#     index = len(output_ids) - output_ids[::-1].index(151668)
# except ValueError:
#     index = 0

# thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip("\n")
# content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip("\n")

# print("thinking content:", thinking_content)
# print("content:", content)

import os
import csv
import argparse
from glob import glob
import re

# ä¸»å­—æ®µé¡ºåºï¼ˆå…¶ä½™é™„åŠ å­—æ®µåœ¨åé¢åŸæ ·ä¿ç•™ï¼‰
MAIN_FIELDS = ["id", "question", "A", "B", "C", "D", "answer", "explanation", "source_file"]

def clean_option_text(text):
    """ç§»é™¤ A. B. A) B) å½¢å¼çš„å‰ç¼€"""
    return re.sub(r"^[A-D][\.\)]\s*", "", text.strip())

def merge_csvs_with_cleaning(input_dir, output_csv, selected_keys=None):
    all_rows = []
    all_fields = set()  # ç»Ÿè®¡æ‰€æœ‰å¯èƒ½å‡ºç°çš„å­—æ®µ
    csv_files = glob(os.path.join(input_dir, "*.csv"))

    print(f"ğŸ” æ‰¾åˆ° {len(csv_files)} ä¸ª CSV æ–‡ä»¶ï¼š")
    for file_path in csv_files:
        file_name = os.path.basename(file_path)
        print(f" - {file_name}")
        file_key = file_name.replace("correct_", "").replace(".csv", "")

        if selected_keys and file_key not in selected_keys:
            continue

        with open(file_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                cleaned_row = dict(row)  # ä¿ç•™åŸå§‹æ‰€æœ‰å­—æ®µ

                # æ¸…æ´—é€‰é¡¹
                cleaned_row["A"] = clean_option_text(row.get("A", ""))
                cleaned_row["B"] = clean_option_text(row.get("B", ""))
                cleaned_row["C"] = clean_option_text(row.get("C", ""))
                cleaned_row["D"] = clean_option_text(row.get("D", ""))

                # æ ‡å‡†åŒ–å­—æ®µ
                cleaned_row["question"] = row.get("question", "").strip()
                cleaned_row["answer"] = row.get("answer", "").strip().upper()
                cleaned_row["explanation"] = row.get("explanation", "").strip()
                cleaned_row["source_file"] = file_name

                all_rows.append(cleaned_row)
                all_fields.update(cleaned_row.keys())

    # ç”Ÿæˆå­—æ®µé¡ºåºï¼ˆä¸»å­—æ®µ + å…¶ä½™é™„åŠ å­—æ®µï¼‰
    additional_fields = sorted(set(all_fields) - set(MAIN_FIELDS))
    final_fields = MAIN_FIELDS + additional_fields

    # é‡æ–°ç¼–å· ID
    for i, row in enumerate(all_rows, 1):
        row["id"] = i

    # å†™å…¥ CSV
    with open(output_csv, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=final_fields)
        writer.writeheader()
        writer.writerows(all_rows)

    print(f"âœ… åˆå¹¶å¹¶æ¸…æ´—å®Œæˆï¼Œå…± {len(all_rows)} æ¡ï¼Œå­—æ®µæ•°ï¼š{len(final_fields)}ï¼Œå·²ä¿å­˜åˆ°ï¼š{output_csv}")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--input_dir", type=str, required=True, help="åŒ…å«å¤šä¸ªCSVæ–‡ä»¶çš„ç›®å½•")
    parser.add_argument("--output_csv", type=str, required=True, help="è¾“å‡ºåˆå¹¶åçš„CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--selected_keys", nargs="*", help="ä»…åˆå¹¶æŒ‡å®škeyå‘½åçš„CSVï¼ˆå¦‚000 111ï¼‰")
    args = parser.parse_args()

    merge_csvs_with_cleaning(args.input_dir, args.output_csv, selected_keys=args.selected_keys)

if __name__ == "__main__":
    main()


"""
python demo.py \
  --input_dir /mnt/sda/wyp/forestllm-main/outputs/compare_subsets/ \
  --output_csv /mnt/sda/wyp/forestllm-main/outputs/compare_subsets/merged_cleaned.csv \
  --selected_keys 000 100 111 101 110
"""