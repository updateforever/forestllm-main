import json
import csv
import argparse
import os

def load_results(jsonl_path):
    """åŠ è½½ results.jsonl æ–‡ä»¶ï¼Œè¿”å› id åˆ° record çš„æ˜ å°„"""
    records = {}
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            record = json.loads(line)
            records[record["id"]] = record
    return records

def load_original_csv(csv_path):
    """åŠ è½½åŸå§‹æ•°æ®é›†ï¼Œè¿”å› id åˆ°è¡Œæ•°æ®çš„æ˜ å°„"""
    id_to_row = {}
    with open(csv_path, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        for row in reader:
            id_str = row.get("id")
            if id_str is not None:
                id_to_row[int(id_str)] = row
    return id_to_row

def extract_subsets(model1_results, model2_results, model3_results):
    """æå–å››ç±»å­é›†çš„ id åˆ—è¡¨"""
    all_correct_ids = []
    model1_correct_23_wrong_ids = []
    model1_wrong_23_correct_ids = []
    all_wrong_ids = []

    for id_, rec1 in model1_results.items():
        rec2 = model2_results.get(id_)
        rec3 = model3_results.get(id_)
        if not rec2 or not rec3:
            continue

        correct1 = rec1.get("correct", False)
        correct2 = rec2.get("correct", False)
        correct3 = rec3.get("correct", False)

        if correct1 and correct2 and correct3:
            all_correct_ids.append(id_)
        elif correct1 and not correct2 and not correct3:
            model1_correct_23_wrong_ids.append(id_)
        elif not correct1 and correct2 and correct3:
            model1_wrong_23_correct_ids.append(id_)
        elif not correct1 and not correct2 and not correct3:
            all_wrong_ids.append(id_)

    return all_correct_ids, model1_correct_23_wrong_ids, model1_wrong_23_correct_ids, all_wrong_ids


def save_subset_csv(selected_ids, original_rows, output_csv_path, fieldnames):
    """æ ¹æ® id åˆ—è¡¨ç­›é€‰åŸå§‹ CSVï¼Œå¹¶ä¿å­˜"""
    subset_rows = [original_rows[i] for i in selected_ids if i in original_rows]
    with open(output_csv_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(subset_rows)

def extract_all_combinations(model1_results, model2_results, model3_results):
    """å°†æ‰€æœ‰æ ·æœ¬æŒ‰8ç§ç»„åˆåˆ†ç±»ï¼Œè¿”å›ï¼šç»„åˆé”® -> idåˆ—è¡¨"""
    combo_to_ids = {
        "111": [], "110": [], "101": [], "100": [],
        "011": [], "010": [], "001": [], "000": []
    }

    for id_, rec1 in model1_results.items():
        rec2 = model2_results.get(id_)
        rec3 = model3_results.get(id_)
        if not rec2 or not rec3:
            continue

        b1 = int(bool(rec1.get("correct", False)))
        b2 = int(bool(rec2.get("correct", False)))
        b3 = int(bool(rec3.get("correct", False)))

        key = f"{b1}{b2}{b3}"
        combo_to_ids[key].append(id_)

    return combo_to_ids

def merge_selected_combinations(combo_to_ids, original_rows, output_dir, fieldnames, selected_keys, output_filename):
    """å°†æŒ‡å®šç»„åˆåˆå¹¶æˆä¸€ä¸ªæ–°çš„æµ‹è¯•é›†"""
    merged_rows = []

    for key in selected_keys:
        ids = combo_to_ids.get(key, [])
        for id_ in ids:
            if id_ in original_rows:
                row = dict(original_rows[id_])  # æ‹·è´ä¸€ä»½
                merged_rows.append(row)

    merged_fieldnames = fieldnames

    output_path = os.path.join(output_dir, output_filename)
    with open(output_path, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=merged_fieldnames)
        writer.writeheader()
        writer.writerows(merged_rows)

    print(f"âœ… åˆå¹¶åçš„æ–°æµ‹è¯•é›† {output_filename}ï¼š{len(merged_rows)} æ¡æ ·æœ¬")


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--model1_result", type=str, required=True, help="æ¨¡å‹1çš„ results.jsonl")
    parser.add_argument("--model2_result", type=str, required=True, help="æ¨¡å‹2çš„ results.jsonl")
    parser.add_argument("--model3_result", type=str, required=True, help="æ¨¡å‹3çš„ results.jsonl")
    parser.add_argument("--original_csv", type=str, required=True, help="åŸå§‹å®Œæ•´è¯„æµ‹æ•°æ®é›† CSV")
    parser.add_argument("--output_dir", type=str, default="outputs/compare_subsets", help="è¾“å‡ºä¿å­˜ç›®å½•")
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)

    # åŠ è½½
    model1_results = load_results(args.model1_result)
    model2_results = load_results(args.model2_result)
    model3_results = load_results(args.model3_result)
    original_rows = load_original_csv(args.original_csv)

    # è·å–åˆ—å¤´
    fieldnames = list(next(iter(original_rows.values())).keys())

    # å…¨ç»„åˆæå–
    combo_to_ids = extract_all_combinations(model1_results, model2_results, model3_results)

    # æŒ‰ç»„åˆä¿å­˜
    for key, id_list in combo_to_ids.items():
        filename = f"correct_{key}.csv"
        out_path = os.path.join(args.output_dir, filename)
        save_subset_csv(id_list, original_rows, out_path, fieldnames)
        print(f"âœ… å·²ä¿å­˜ {filename}ï¼š{len(id_list)} æ¡")

    print(f"ğŸ“‚ å…¨éƒ¨ç»„åˆå·²ä¿å­˜åˆ°ç›®å½•ï¼š{args.output_dir}")

    # ä¿å­˜åˆå¹¶åçš„æ–°æµ‹è¯•é›†
    selected_keys = ["000", "100", "110", "101", "111"]
    merge_selected_combinations(
        combo_to_ids,
        original_rows,
        args.output_dir,
        fieldnames,
        selected_keys,
        output_filename="merged_testset.csv"
    )


if __name__ == "__main__":
    main()


"""
python extract_sub_data.py \
  --model2_result /mnt/sda/wyp/forestllm-main/outputs/eval/DeepSeek-R1-Distill-Qwen-7B/eval_multiple_choice_filtered/results.jsonl\
  --model1_result /mnt/sda/wyp/forestllm-main/outputs/eval/checkpoint-12277/eval_multiple_choice_filtered/results.jsonl \
  --model3_result /mnt/sda/wyp/forestllm-main/outputs/eval_api/gpt4/eval_multiple_choice_filtered/results.jsonl \
  --original_csv /mnt/sda/wyp/forestllm-main/forest_eval/book/eval_multiple_choice_filtered.csv \
  --output_dir /mnt/sda/wyp/forestllm-main/outputs/compare_subsets
"""