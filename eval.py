import os
import sys
import json
import torch
import argparse
import logging
import csv
from transformers import AutoModelForCausalLM, AutoTokenizer
import evaluate
from utils.global_methods import *  # ç¡®ä¿ GPT-4 è¯„ä¼°å¯ç”¨
from tqdm import tqdm 
from data.dataset import get_dataloader
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def get_device():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"  # Mac ä¸Šçš„ Metal è®¾å¤‡
    else:
        return "cpu"

def load_model(model_path, temperature=1.0):
    """ä½¿ç”¨ Hugging Face Transformers åŠ è½½æœ¬åœ°æ¨¡å‹"""
    logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
    device = get_device()

    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side='left')
    # print(tokenizer.truncation_side)  # è¾“å‡º "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # è§£å†³ padding é—®é¢˜

    # åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹ `.safetensors`ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU æˆ– CPU
        torch_dtype="auto",
        trust_remote_code=True  # ä»…ä½¿ç”¨æœ¬åœ°ä»£ç 
    ).eval()  # è®¾ä¸º eval æ¨¡å¼ï¼Œé¿å…è®­ç»ƒæ—¶çš„ Dropout

    logger.info(f"æ¨¡å‹æˆåŠŸåŠ è½½è‡³ {device}ï¼Œ temperature={temperature}")
    return model, tokenizer


def extract_answer_from_tags(output_text):
    """
    ä»ç”Ÿæˆç»“æœä¸­æå– `<think>` å’Œ `<answer>` å†…å®¹ã€‚
    æ”¯æŒéƒ¨åˆ†ç¼ºå¤±æƒ…å†µï¼š
    - æœ‰å®Œæ•´çš„ <think>...</think> å’Œ <answer>...</answer>ï¼šæ­£å¸¸è§£æ
    - ç¼ºå¤± <answer>ï¼šå›é€€åˆ° think åçš„æ–‡æœ¬å°è¯•æå–
    - ç¼ºå¤± <think>ï¼šä»…æå– answer æ ‡ç­¾
    - å…¨éƒ¨ç¼ºå¤±ï¼šåŸæ ·è¿”å›
    """
    output_text = output_text.strip()
    think_match = re.search(r"<think>(.*?)</think>", output_text, re.DOTALL)
    answer_match = re.search(r"<answer>(.*?)</answer>", output_text, re.DOTALL)

    thought_process = None
    answer_candidate = None

    if think_match:
        thought_process = think_match.group(1).strip()
    if answer_match:
        answer_candidate = answer_match.group(1).strip()

    # å¦‚æœåªæœ‰ <think>ï¼Œåˆ™å°è¯•è·å–å®ƒåé¢çš„å†…å®¹ä½œä¸º answer_candidate
    if think_match and not answer_match:
        after_think = output_text[think_match.end():].strip()
        answer_candidate = after_think

    # å¦‚æœä¸¤ä¸ªéƒ½æ²¡æœ‰ï¼Œå°±è¿”å›åŸå§‹å†…å®¹
    if not thought_process and not answer_candidate:
        return None, output_text

    return thought_process, answer_candidate or ""

def extract_first_option(text):
    """
    ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆé€‰é¡¹ï¼ˆA/B/C/Dï¼‰ï¼Œ
    æ”¯æŒæ ¼å¼ï¼šAã€A)ã€A.ã€é€‰é¡¹Aã€ç­”æ¡ˆæ˜¯A ç­‰ã€‚
    """
    short_text = text.strip()[:100].upper()
    match = re.search(r"\b([ABCD])[\s\).ï¼Œã€ã€‚]?", short_text)
    if match:
        return match.group(1)
    return None

def extract_first_option(text):
    """
    ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆé€‰é¡¹ï¼ˆA/B/C/Dï¼‰ï¼ŒæŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§ï¼š
    1. æå– \boxed{A} å½¢å¼ä¸­çš„å†…å®¹ï¼›
    2. æå– <answer>A</answer> å½¢å¼ä¸­çš„å†…å®¹ï¼›
    3. ä»æ–‡æœ¬å¼€å¤´å‰100å­—ç¬¦ä¸­è§£æ A/B/C/Dã€‚
    """
    text_upper = text.strip().upper()

    # 1ï¸âƒ£ å°è¯•æå– \boxed{A}
    boxed_match = re.search(r"\\boxed\{([ABCD])\}", text)
    if boxed_match:
        return boxed_match.group(1)

    # 2ï¸âƒ£ å°è¯•æå– <answer>A</answer>
    tag_match = re.search(r"<answer>\s*([ABCD])\s*</answer>", text)
    if tag_match:
        return tag_match.group(1)

    # 3ï¸âƒ£ å›é€€åˆ°å‰100å­—ç¬¦ä¸­æœç´¢ A/B/C/D
    short_text = text[:100]
    fallback_match = re.search(r"\b([ABCD])[\s\).ï¼Œã€ã€‚]?", short_text)
    if fallback_match:
        return fallback_match.group(1)

    # éƒ½å¤±è´¥è¿”å› None
    return None

def is_valid_option(answer):
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é€‰é¡¹ A/B/C/Dã€‚
    - ä¼˜å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    - å†ä½¿ç”¨å®½æ¾è§„åˆ™æå–
    """
    cleaned = answer.strip().upper()
    if cleaned in ["A", "B", "C", "D"]:
        return True
    return extract_first_option(cleaned) is not None

def infer_answer_with_gpt4(model_output):
    """å¦‚æœç­”æ¡ˆä¸æ˜¯æœ‰æ•ˆé€‰é¡¹ï¼Œè°ƒç”¨ GPT-4 è¿›è¡Œæ¨ç†"""
    logger.info(f"è°ƒç”¨ GPT-4 è¿›è¡Œç­”æ¡ˆä¿®æ­£: {model_output}")
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„åˆ†å‘˜ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ¨¡å‹è¾“å‡ºï¼Œåˆ¤æ–­æ¨¡å‹æœ€æœ‰å¯èƒ½é€‰æ‹©çš„é€‰é¡¹ï¼ˆA/B/C/Dï¼‰ã€‚
    - **æ¨¡å‹åŸå§‹è¾“å‡º**: {model_output}
    
    é¢˜ç›®ä¸ºå•é€‰é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡ºæœ€å¯èƒ½çš„é€‰é¡¹ï¼ˆA/B/C/Dï¼‰ï¼Œä¸éœ€è¦è§£é‡Šã€‚
    """
    try:
        gpt4_answer = run_agent(prompt, model="qwen")
        if is_valid_option(gpt4_answer):
            return gpt4_answer
    except Exception as e:
        logger.error(f"GPT-4 æ¨ç†å¤±è´¥: {e}")
    
    return "æœªçŸ¥"  # GPT-4 å¤±è´¥æ—¶è¿”å› "æœªçŸ¥"

def extract_answer(output_text):
    """æå–ç­”æ¡ˆ (ä» <think> æ ‡ç­¾ä¸­è§£æ)"""
    start_tag, end_tag = "<think>", "</think>"
    start_idx, end_idx = output_text.find(start_tag), output_text.find(end_tag)
    if start_idx != -1 and end_idx != -1:
        return output_text[start_idx + len(start_tag):end_idx].strip()
    return output_text.strip()

def compute_mcq_accuracy(predictions, references):
    """è®¡ç®—å¤šé€‰é¢˜å‡†ç¡®ç‡"""
    correct = sum(1 for pred, ref in zip(predictions, references) if pred == ref)
    acc = correct / len(references) if references else 0
    logger.info(f"å•é€‰é¢˜å‡†ç¡®ç‡: {acc:.4f}")
    return acc

def compute_qa_metrics(predictions, references):
    """è®¡ç®—é—®ç­”é¢˜çš„è¯„ä¼°æŒ‡æ ‡ (ROUGE å’Œ BLEU)"""
    rouge = evaluate.load("rouge")
    bleu = evaluate.load("bleu")
    rouge_scores = rouge.compute(predictions=predictions, references=references)
    bleu_scores = bleu.compute(predictions=predictions, references=references)
    logger.info(f"ROUGE è¯„åˆ†: {rouge_scores}")
    logger.info(f"BLEU è¯„åˆ†: {bleu_scores}")
    return {"ROUGE": rouge_scores, "BLEU": bleu_scores}

def call_gpt4_eval(predictions, references, task_type):
    """è°ƒç”¨ GPT-4 è¿›è¡Œè¯„ä¼°"""
    logger.info("æ­£åœ¨è°ƒç”¨ GPT-4 è¿›è¡Œè¯„åˆ†...")
    responses = []
    for pred, ref in zip(predictions, references):
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„åˆ†å‘˜ï¼Œè¯·å¯¹ä»¥ä¸‹{task_type}ä»»åŠ¡çš„å›ç­”è¿›è¡Œæ‰“åˆ†ï¼ˆ0-100ï¼‰ã€‚
        è¯„åˆ†æ ‡å‡†ï¼š
        1. **æ­£ç¡®æ€§**ï¼šç­”æ¡ˆæ˜¯å¦ç¬¦åˆå‚è€ƒç­”æ¡ˆã€‚
        2. **å®Œæ•´æ€§**ï¼šå›ç­”æ˜¯å¦è¦†ç›–äº†æ ¸å¿ƒä¿¡æ¯ã€‚
        3. **è¡¨è¾¾æ¸…æ™°åº¦**ï¼šå›ç­”æ˜¯å¦æµç•…æ˜“æ‡‚ã€‚

        **å‚è€ƒç­”æ¡ˆ**: {ref}
        **æ¨¡å‹è¾“å‡º**: {pred}

        è¯·æŒ‰ç…§ä¸Šè¿°è¯„åˆ†æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼Œå¹¶ç»™å‡ºç®€è¦è§£é‡Šï¼š
        """
        try:
            score_text = run_chatgpt(query=prompt, model="gpt-4", num_tokens_request=100)
            responses.append(score_text.strip())
        except Exception as e:
            logger.error(f"GPT-4 è¯„åˆ†å¤±è´¥: {e}")
            responses.append("è¯„åˆ†å¤±è´¥")
    logger.info(f"GPT-4 è¯„åˆ†ç»“æœ: {responses}")
    return responses

def save_results(output_dir, predictions, references, evaluation_results, raw_outputs, original_inputs=None):
    """ä¿å­˜ JSON å’Œ CSV è¯„ä¼°ç»“æœ"""
    # ==== 1ï¸âƒ£ JSON ä¿å­˜ ====
    predictions_file = os.path.join(output_dir, "predictions.json")
    metrics_file = os.path.join(output_dir, "metrics.json")

    with open(predictions_file, "w", encoding="utf-8") as f:
        json.dump({
            "inputs": original_inputs,
            "predictions": predictions,
            "references": references,
            "raw_outputs": raw_outputs
        }, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ”¹ JSON æ¨ç†ç»“æœå·²ä¿å­˜è‡³ {predictions_file}")

    with open(metrics_file, "w", encoding="utf-8") as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ”¹ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³ {metrics_file}")

    # ==== 2ï¸âƒ£ CSV ä¿å­˜ ====
    csv_file = os.path.join(output_dir, "predictions.csv")
    with open(csv_file, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["id", "input", "raw_output", "predicted", "reference", "correct"])
        for idx, (inp, raw, pred, ref) in enumerate(zip(original_inputs, raw_outputs, predictions, references)):
            correct = "âœ“" if pred == ref else "âœ—"
            writer.writerow([idx + 1, inp, raw, pred, ref, correct])
    logger.info(f"ğŸ“„ CSV æ¨ç†ç»“æœå·²ä¿å­˜è‡³ {csv_file}")


def generate_text(model, tokenizer, input_texts, max_new_tokens=4096, temperature=0.7):
    """ä½¿ç”¨ Hugging Face è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆæ”¯æŒæ‰¹é‡æ¨ç†ï¼Œè¿”å›åŸå§‹è¾“å‡ºï¼‰"""
    inputs = tokenizer(
        input_texts, padding=True, truncation=True, max_length=1024,
        padding_side='left', return_tensors="pt"
    ).to(model.device)
    input_length = inputs["input_ids"].shape[1]

    with torch.no_grad():
        output_sequences = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True
        )

    new_tokens = output_sequences[:, input_length:]
    output_texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)
    output_texts = [text.strip() for text in output_texts]

    final_answers = []
    for output in output_texts:
        thought_process, answer_candidate = extract_answer_from_tags(output)
        option = extract_first_option(answer_candidate) if not answer_candidate.strip() in ["A", "B", "C", "D"] else answer_candidate.strip()
        if option:
            final_answers.append(option)
        else:
            gpt4_answer = infer_answer_with_gpt4(thought_process, answer_candidate)
            final_answers.append(gpt4_answer)

    return final_answers, output_texts

def evaluate_model(model, tokenizer, dataloader, total_batches, task_type, evaluation_method, output_dir, temperature):
    """ä½¿ç”¨ Hugging Face è¿›è¡Œè¯„ä¼°ï¼ˆæ”¯æŒ PyTorch DataLoaderï¼‰"""
    logger.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {task_type}")

    os.makedirs(output_dir, exist_ok=True)
    predictions, references_list, raw_outputs, input_texts = [], [], [], []

    with tqdm(total=total_batches, desc="æ¨¡å‹æ¨ç†ä¸­", unit="batch") as pbar:
        # for batch_inputs, batch_references in dataloader:
        #     batch_preds, batch_raw = generate_text(model, tokenizer, batch_inputs, temperature=temperature)
        for batch_items in dataloader:
            prompts = [item["prompt"] for item in batch_items]
            batch_references = [item["answer"] for item in batch_items]

            batch_preds, batch_raw = generate_text(model, tokenizer, prompts, temperature=temperature)

            predictions.extend(batch_preds)
            raw_outputs.extend(batch_raw)
            references_list.extend(batch_references)
            input_texts.extend(prompts)  
            pbar.update(1)

    evaluation_results = {}
    if task_type == "mcq":
        evaluation_results = {"accuracy": compute_mcq_accuracy(predictions, references_list)}
    elif task_type == "qa":
        evaluation_results = (
            compute_qa_metrics(predictions, references_list) if evaluation_method == "metrics"
            else {"GPT-4 Scores": call_gpt4_eval(predictions, references_list, task_type)}
            if evaluation_method == "gpt4"
            else {"predictions": predictions, "references": references_list}
        )

    save_results(output_dir, predictions, references_list, evaluation_results, raw_outputs)
    logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤§æ¨¡å‹è¯„ä¼°ç®¡çº¿")
    parser.add_argument("--model_path", type=str, default="/mnt/sda/wyp/models/ds-qwen7b-sft", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_file", type=str, default="/mnt/sda/wyp/forestllm-main/forest_eval/forest_val_v1.csv", help="è¾“å…¥æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="outputs/eval_data/", help="è¯„ä¼°ç»“æœå­˜å‚¨ç›®å½•")
    parser.add_argument("--task_type", type=str, choices=["mcq", "qa"], default="mcq", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--evaluation_method", type=str, choices=["metrics", "gpt4", "manual"], default="metrics", help="è¯„ä¼°æ–¹å¼")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹é‡æ¨ç†å¤§å°")
    parser.add_argument("--max_new_tokens", type=int, default=2048, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.3, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--model_mode", type=str, default='cot', help="æ¨¡æ¿é€‰æ‹©")
    

    args = parser.parse_args()

    model, tokenizer = load_model(args.model_path, temperature=args.temperature)
    output_dir = os.path.join(args.output_dir, args.model_path.split("/")[-1])
    os.makedirs(output_dir, exist_ok=True)
    print("è¯„ä¼°ç»“æœå°†å­˜å‚¨åœ¨ï¼š", output_dir)

    dataloader, total_batches  = get_dataloader(args.input_file, batch_size=args.batch_size, task_type=args.task_type, model_mode=args.model_mode)
    evaluate_model(
        model, tokenizer, dataloader, total_batches, args.task_type, args.evaluation_method, output_dir, args.temperature
    )


if __name__ == "__main__":
    main()
