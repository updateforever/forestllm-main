import os
import sys
import json
import torch
import argparse
import logging
import csv
from transformers import AutoModelForCausalLM, AutoTokenizer
import evaluate
from utils.global_methods import *  # ç¡®ä¿ GPT-4 è¯„ä¼°å¯ç”¨
from tqdm import tqdm 
from data.dataset import get_dataloader
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def get_device():
    """è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¡ç®—è®¾å¤‡"""
    if torch.cuda.is_available():
        return "cuda"
    elif torch.backends.mps.is_available():
        return "mps"  # Mac ä¸Šçš„ Metal è®¾å¤‡
    else:
        return "cpu"

def load_model(model_path, temperature=1.0):
    """ä½¿ç”¨ Hugging Face Transformers åŠ è½½æœ¬åœ°æ¨¡å‹"""
    logger.info(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {model_path}")
    device = get_device()

    # åŠ è½½ tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side='left')
    # print(tokenizer.truncation_side)  # è¾“å‡º "left"
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token  # è§£å†³ padding é—®é¢˜

    # åŠ è½½æ¨¡å‹ï¼ˆè‡ªåŠ¨æ£€æµ‹ `.safetensors`ï¼‰
    model = AutoModelForCausalLM.from_pretrained(
        model_path, 
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU æˆ– CPU
        torch_dtype="auto",
        trust_remote_code=True  # ä»…ä½¿ç”¨æœ¬åœ°ä»£ç 
    ).eval()  # è®¾ä¸º eval æ¨¡å¼ï¼Œé¿å…è®­ç»ƒæ—¶çš„ Dropout

    logger.info(f"æ¨¡å‹æˆåŠŸåŠ è½½è‡³ {device}ï¼Œ temperature={temperature}")
    return model, tokenizer


def extract_answer_from_think(output_text):
    """æå– `<think>...</think>` ä¹‹åçš„ç­”æ¡ˆ"""
    match = re.search(r"<think>(.*?)</think>(.*)", output_text, re.DOTALL)
    if match:
        thought_process = match.group(1).strip()  # `<think>` å†…çš„å†…å®¹
        answer_candidate = match.group(2).strip()  # `<think>` ä¹‹åçš„å†…å®¹
        return thought_process, answer_candidate
    return None, output_text.strip()  # æ²¡æœ‰ `<think>`ï¼Œç›´æ¥è¿”å›æ•´ä¸ªè¾“å‡º

def extract_first_option(text):
    """
    ä»æ–‡æœ¬ä¸­æå–ç¬¬ä¸€ä¸ªæœ‰æ•ˆé€‰é¡¹ï¼ˆA/B/C/Dï¼‰ï¼Œ
    æ”¯æŒæ ¼å¼ï¼šAã€A)ã€A.ã€é€‰é¡¹Aã€ç­”æ¡ˆæ˜¯A ç­‰ã€‚
    """
    short_text = text.strip()[:100].upper()
    match = re.search(r"\b([ABCD])[\s\).ï¼Œã€ã€‚]?", short_text)
    if match:
        return match.group(1)
    return None

def is_valid_option(answer):
    """
    åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„é€‰é¡¹ A/B/C/Dã€‚
    - ä¼˜å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    - å†ä½¿ç”¨å®½æ¾è§„åˆ™æå–
    """
    cleaned = answer.strip().upper()
    if cleaned in ["A", "B", "C", "D"]:
        return True
    return extract_first_option(cleaned) is not None

def infer_answer_with_gpt4(thought_process, answer_candidate):
    """å¦‚æœç­”æ¡ˆä¸æ˜¯æœ‰æ•ˆé€‰é¡¹ï¼Œè°ƒç”¨ GPT-4 è¿›è¡Œæ¨ç†"""
    logger.info(f"è°ƒç”¨ GPT-4 è¿›è¡Œç­”æ¡ˆä¿®æ­£: {answer_candidate}")
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„åˆ†å‘˜ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ¨ç†è¿‡ç¨‹ï¼Œåˆ¤æ–­æ¨¡å‹æœ€æœ‰å¯èƒ½é€‰æ‹©çš„é€‰é¡¹ï¼ˆA/B/C/Dï¼‰ã€‚
    - **æ¨ç†è¿‡ç¨‹**: {thought_process}
    - **æ¨¡å‹åŸå§‹ç­”æ¡ˆ**: {answer_candidate}

    è¯·ç›´æ¥è¾“å‡ºæœ€å¯èƒ½çš„é€‰é¡¹ï¼ˆA/B/C/Dï¼‰ï¼Œä¸éœ€è¦è§£é‡Šã€‚
    """
    try:
        gpt4_answer = run_agent(prompt, model="qwen")
        if is_valid_option(gpt4_answer):
            return gpt4_answer
    except Exception as e:
        logger.error(f"GPT-4 æ¨ç†å¤±è´¥: {e}")
    
    return "æœªçŸ¥"  # GPT-4 å¤±è´¥æ—¶è¿”å› "æœªçŸ¥"

def extract_answer(output_text):
    """æå–ç­”æ¡ˆ (ä» <think> æ ‡ç­¾ä¸­è§£æ)"""
    start_tag, end_tag = "<think>", "</think>"
    start_idx, end_idx = output_text.find(start_tag), output_text.find(end_tag)
    if start_idx != -1 and end_idx != -1:
        return output_text[start_idx + len(start_tag):end_idx].strip()
    return output_text.strip()

def compute_mcq_accuracy(predictions, references):
    """è®¡ç®—å¤šé€‰é¢˜å‡†ç¡®ç‡"""
    correct = sum(1 for pred, ref in zip(predictions, references) if pred == ref)
    acc = correct / len(references) if references else 0
    logger.info(f"å•é€‰é¢˜å‡†ç¡®ç‡: {acc:.4f}")
    return acc

def compute_qa_metrics(predictions, references):
    """è®¡ç®—é—®ç­”é¢˜çš„è¯„ä¼°æŒ‡æ ‡ (ROUGE å’Œ BLEU)"""
    rouge = evaluate.load("rouge")
    bleu = evaluate.load("bleu")
    rouge_scores = rouge.compute(predictions=predictions, references=references)
    bleu_scores = bleu.compute(predictions=predictions, references=references)
    logger.info(f"ROUGE è¯„åˆ†: {rouge_scores}")
    logger.info(f"BLEU è¯„åˆ†: {bleu_scores}")
    return {"ROUGE": rouge_scores, "BLEU": bleu_scores}

def call_gpt4_eval(predictions, references, task_type):
    """è°ƒç”¨ GPT-4 è¿›è¡Œè¯„ä¼°"""
    logger.info("æ­£åœ¨è°ƒç”¨ GPT-4 è¿›è¡Œè¯„åˆ†...")
    responses = []
    for pred, ref in zip(predictions, references):
        prompt = f"""
        ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„åˆ†å‘˜ï¼Œè¯·å¯¹ä»¥ä¸‹{task_type}ä»»åŠ¡çš„å›ç­”è¿›è¡Œæ‰“åˆ†ï¼ˆ0-100ï¼‰ã€‚
        è¯„åˆ†æ ‡å‡†ï¼š
        1. **æ­£ç¡®æ€§**ï¼šç­”æ¡ˆæ˜¯å¦ç¬¦åˆå‚è€ƒç­”æ¡ˆã€‚
        2. **å®Œæ•´æ€§**ï¼šå›ç­”æ˜¯å¦è¦†ç›–äº†æ ¸å¿ƒä¿¡æ¯ã€‚
        3. **è¡¨è¾¾æ¸…æ™°åº¦**ï¼šå›ç­”æ˜¯å¦æµç•…æ˜“æ‡‚ã€‚

        **å‚è€ƒç­”æ¡ˆ**: {ref}
        **æ¨¡å‹è¾“å‡º**: {pred}

        è¯·æŒ‰ç…§ä¸Šè¿°è¯„åˆ†æ ‡å‡†è¿›è¡Œè¯„åˆ†ï¼Œå¹¶ç»™å‡ºç®€è¦è§£é‡Šï¼š
        """
        try:
            score_text = run_chatgpt(query=prompt, model="gpt-4", num_tokens_request=100)
            responses.append(score_text.strip())
        except Exception as e:
            logger.error(f"GPT-4 è¯„åˆ†å¤±è´¥: {e}")
            responses.append("è¯„åˆ†å¤±è´¥")
    logger.info(f"GPT-4 è¯„åˆ†ç»“æœ: {responses}")
    return responses

def save_results(output_dir, predictions, references, evaluation_results):
    """ä¿å­˜æ¨ç†ç»“æœå’Œè¯„ä¼°æŒ‡æ ‡"""
    predictions_file = os.path.join(output_dir, "predictions.json")
    metrics_file = os.path.join(output_dir, "metrics.json")

    with open(predictions_file, "w", encoding="utf-8") as f:
        json.dump({"predictions": predictions, "references": references}, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ”¹ æ¨ç†ç»“æœå·²ä¿å­˜è‡³ {predictions_file}")

    with open(metrics_file, "w", encoding="utf-8") as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=4)
    logger.info(f"ğŸ”¹ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜è‡³ {metrics_file}")

def generate_text(model, tokenizer, input_texts, max_new_tokens=256, temperature=0.7):
    """ä½¿ç”¨ Hugging Face è¿›è¡Œæ–‡æœ¬ç”Ÿæˆï¼ˆæ”¯æŒæ‰¹é‡æ¨ç†ï¼‰"""
    # if 'mini' in model.model_dir or 'llama' in model.model_dir or 'Mini' in model.model_dir:
    #     tokenizer.pad_token = tokenizer.eos_token
    inputs = tokenizer(input_texts, padding=True, truncation=True, max_length=1024, padding_side='left', return_tensors="pt").to(model.device)
    input_length = inputs["input_ids"].shape[1]  # è·å–è¾“å…¥ token çš„é•¿åº¦

    with torch.no_grad():
        output_sequences = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,  # åªé™åˆ¶ç”Ÿæˆéƒ¨åˆ†çš„é•¿åº¦
            temperature=temperature,
            do_sample=True  # å…è®¸ä¸€å®šçš„éšæœºæ€§
        )

    # å»æ‰è¾“å…¥éƒ¨åˆ†ï¼Œä»…ä¿ç•™æ¨¡å‹ç”Ÿæˆçš„å†…å®¹
    new_tokens = output_sequences[:, input_length:]  # åªä¿ç•™ç”Ÿæˆéƒ¨åˆ†
    output_texts = tokenizer.batch_decode(new_tokens, skip_special_tokens=True)
    output_texts = [text.strip() for text in output_texts]  # è¿›ä¸€æ­¥æ¸…ç†

    final_answers = []
    for output in output_texts:
        thought_process, answer_candidate = extract_answer_from_think(output)

        # if is_valid_option(answer_candidate):
        #     final_answers.append(answer_candidate)  # ç›´æ¥è¿”å› A/B/C/D
        # else:
        #     gpt4_answer = infer_answer_with_gpt4(thought_process, answer_candidate)
        #     final_answers.append(gpt4_answer)
        option = extract_first_option(answer_candidate) if not answer_candidate.strip() in ["A", "B", "C", "D"] else answer_candidate.strip()

        if option:
            final_answers.append(option)
        else:
            gpt4_answer = infer_answer_with_gpt4(thought_process, answer_candidate)
            final_answers.append(gpt4_answer)
    return final_answers

def evaluate_model(model, tokenizer, dataloader, total_batches, task_type, evaluation_method, output_dir, temperature):
    """ä½¿ç”¨ Hugging Face è¿›è¡Œè¯„ä¼°ï¼ˆæ”¯æŒ PyTorch DataLoaderï¼‰"""
    logger.info(f"å¼€å§‹è¯„ä¼°æ¨¡å‹: {task_type}")

    os.makedirs(output_dir, exist_ok=True)
    predictions, references_list = [], []

    with tqdm(total=total_batches, desc="æ¨¡å‹æ¨ç†ä¸­", unit="batch") as pbar:
        for batch_inputs, batch_references in dataloader:
            batch_outputs = generate_text(model, tokenizer, batch_inputs, max_new_tokens=512, temperature=temperature)
            predictions.extend(batch_outputs)
            references_list.extend(batch_references)
            pbar.update(1)  # è¿›åº¦æ¡æ›´æ–°

    evaluation_results = {}
    if task_type == "mcq":
        evaluation_results = {"accuracy": compute_mcq_accuracy(predictions, references_list)}
    elif task_type == "qa":
        evaluation_results = (
            compute_qa_metrics(predictions, references_list) if evaluation_method == "metrics"
            else {"GPT-4 Scores": call_gpt4_eval(predictions, references_list, task_type)}
            if evaluation_method == "gpt4"
            else {"predictions": predictions, "references": references_list}
        )

    save_results(output_dir, predictions, references_list, evaluation_results)
    logger.info(f"è¯„ä¼°ç»“æœå·²ä¿å­˜è‡³ {output_dir}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="å¤§æ¨¡å‹è¯„ä¼°ç®¡çº¿")
    parser.add_argument("--model_path", type=str, default="/mnt/sda/wyp/models/ds-qwen7b-sft", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_file", type=str, default="/mnt/sda/wyp/forestllm-main/output/forest_val_v1.csv", help="è¾“å…¥æ•°æ®æ–‡ä»¶")
    parser.add_argument("--output_dir", type=str, default="outputs/eval_data/", help="è¯„ä¼°ç»“æœå­˜å‚¨ç›®å½•")
    parser.add_argument("--task_type", type=str, choices=["mcq", "qa"], default="mcq", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--evaluation_method", type=str, choices=["metrics", "gpt4", "manual"], default="metrics", help="è¯„ä¼°æ–¹å¼")
    parser.add_argument("--batch_size", type=int, default=2, help="æ‰¹é‡æ¨ç†å¤§å°")
    parser.add_argument("--max_new_tokens", type=int, default=2048, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.3, help="ç”Ÿæˆæ¸©åº¦")

    args = parser.parse_args()

    model, tokenizer = load_model(args.model_path, temperature=args.temperature)
    output_dir = os.path.join(args.output_dir, args.model_path.split("/")[-1])
    os.makedirs(output_dir, exist_ok=True)
    print("è¯„ä¼°ç»“æœå°†å­˜å‚¨åœ¨ï¼š", output_dir)

    dataloader, total_batches  = get_dataloader(args.input_file, batch_size=args.batch_size, task_type=args.task_type)
    evaluate_model(
        model, tokenizer, dataloader, total_batches, args.task_type, args.evaluation_method, output_dir, args.temperature
    )


if __name__ == "__main__":
    main()
