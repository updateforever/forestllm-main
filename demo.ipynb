{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "合并后总条数: 5718\n",
      "✅ 已保存至: /mnt/sda/wyp/forestllm-main/output/step5/merged_step5_dedup.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"加载 jsonl 文件\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "def save_jsonl(file_path, data):\n",
    "    \"\"\"保存 jsonl 文件\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def merge_and_dedup_jsonl(files, output_path):\n",
    "    all_entries = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for file in files:\n",
    "        data = load_jsonl(file)\n",
    "        for entry in data:\n",
    "            entry_id = entry.get(\"id\")\n",
    "            if entry_id and entry_id not in seen_ids:\n",
    "                seen_ids.add(entry_id)\n",
    "                all_entries.append(entry)\n",
    "\n",
    "    print(f\"合并后总条数: {len(all_entries)}\")\n",
    "    save_jsonl(output_path, all_entries)\n",
    "    print(f\"✅ 已保存至: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = [\n",
    "        \"/mnt/sda/wyp/forestllm-main/output/step5/part1_step5.jsonl\",\n",
    "        \"/mnt/sda/wyp/forestllm-main/output/step5/part2_step5.jsonl\",\n",
    "        \"/mnt/sda/wyp/forestllm-main/output/step5/part3_step5.jsonl\"\n",
    "    ]\n",
    "    output_file = \"/mnt/sda/wyp/forestllm-main/output/step5/merged_step5_dedup.jsonl\"\n",
    "    merge_and_dedup_jsonl(input_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Part 1: /mnt/sda/wyp/forestllm-main/output/step5/part1.jsonl (1906 lines)\n",
      "📄 Part 2: /mnt/sda/wyp/forestllm-main/output/step5/part2.jsonl (1906 lines)\n",
      "📄 Part 3: /mnt/sda/wyp/forestllm-main/output/step5/part3.jsonl (1906 lines)\n",
      "✅ Split completed: 5718 lines into 3 parts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def split_jsonl_three_parts(file_path, output_dir=None):\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    total = len(lines)\n",
    "    part_size = math.ceil(total / 3)\n",
    "\n",
    "    parts = [\n",
    "        lines[:part_size],\n",
    "        lines[part_size:part_size * 2],\n",
    "        lines[part_size * 2:]\n",
    "    ]\n",
    "\n",
    "    for i, part in enumerate(parts, 1):\n",
    "        part_path = os.path.join(output_dir, f\"part{i}.jsonl\")\n",
    "        with open(part_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(part)\n",
    "        print(f\"📄 Part {i}: {part_path} ({len(part)} lines)\")\n",
    "\n",
    "    print(f\"✅ Split completed: {total} lines into 3 parts.\")\n",
    "\n",
    "# 调用\n",
    "split_jsonl_three_parts(\"/mnt/sda/wyp/forestllm-main/output/step5/merged_step4_dedup.jsonl\", \"/mnt/sda/wyp/forestllm-main/output/step5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义模型数据\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAR-T\", \"MSAR-B\"\n",
    "]\n",
    "\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "params = [41.7, 69.4, 2.5, 3.0, 2.7, 2.6, 9.1, 11.1, 8.0, 9.4, 42.7, 36.4, 47.5, 5.5, 20.7]\n",
    "\n",
    "# 归一化气泡大小\n",
    "bubble_size = np.array(params) * 10\n",
    "\n",
    "# 颜色区分不同算法\n",
    "colors = [\n",
    "    'blue', 'blue', 'green', 'green', 'green', 'green', 'orange', 'orange', 'orange', 'orange', \n",
    "    'red', 'red', 'purple', 'cyan', 'cyan'\n",
    "]\n",
    "\n",
    "# 创建主图\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    ax.scatter(gflops[i], map_values[i], s=bubble_size[i], color=colors[i], alpha=0.6, edgecolors=\"k\", label=model if model not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# 标注模型名称\n",
    "for i, model in enumerate(models):\n",
    "    ax.annotate(model, (gflops[i], map_values[i] - 1), fontsize=9, ha='center')\n",
    "\n",
    "# 画对比虚线，并在中间标注差值\n",
    "comparison_pairs = [\n",
    "    (\"RT-DETRv2\", \"MSAR-B\"),\n",
    "    (\"YOLOV8n\", \"MSAR-T\")\n",
    "]\n",
    "\n",
    "for model1, model2 in comparison_pairs:\n",
    "    i1, i2 = models.index(model1), models.index(model2)\n",
    "    gflops_diff = gflops[i2] - gflops[i1]\n",
    "    map_diff = map_values[i2] - map_values[i1]\n",
    "\n",
    "    if model1 == \"RT-DETRv2\" and model2 == \"MSAR-B\":\n",
    "        ax.plot([gflops[i1], gflops[i2]], [map_values[i1], map_values[i1]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "        ax.plot([gflops[i2], gflops[i2]], [map_values[i1], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.text((gflops[i1] + gflops[i2]) / 2, map_values[i1] - 1, f\"ΔGFLOPs: {gflops_diff:.1f}\", color='blue', fontsize=10, ha='center')\n",
    "        ax.text(gflops[i2] + 1, (map_values[i1] + map_values[i2]) / 2, f\"ΔmAP: {map_diff:.1f}\", color='red', fontsize=10, ha='left')\n",
    "\n",
    "    elif model1 == \"YOLOV8n\" and model2 == \"MSAR-T\":\n",
    "        ax.plot([gflops[i1], gflops[i1]], [map_values[i1], map_values[i2]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "        ax.plot([gflops[i1], gflops[i2]], [map_values[i2], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.text(gflops[i1] - 2, (map_values[i1] + map_values[i2]) / 2, f\"ΔmAP: {map_diff:.1f}\", color='blue', fontsize=10, ha='right')\n",
    "        ax.text((gflops[i1] + gflops[i2]) / 2, map_values[i2] + 1, f\"ΔGFLOPs: {gflops_diff:.1f}\", color='red', fontsize=10, ha='center')\n",
    "\n",
    "# 设定坐标轴标签和标题\n",
    "ax.set_xlabel(\"GFLOPs\")\n",
    "ax.set_ylabel(\"mAP\")\n",
    "ax.set_title(\"GFLOPs vs. mAP (Bubble Size Represents Model Parameters)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义函数将数据坐标转换为 figure 归一化坐标\n",
    "def data_to_fig_coords(ax, x, y):\n",
    "    return ax.transData.transform((x, y))  # 将数据坐标转换为屏幕坐标\n",
    "\n",
    "def fig_to_axes_coords(ax, x, y):\n",
    "    inv = ax.transAxes.inverted()  # 获取 figure 归一化坐标\n",
    "    return inv.transform((x, y))  # 转换为 axes 坐标\n",
    "\n",
    "# 定义模型数据\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAD-T\", \"MSAD-B\"\n",
    "]\n",
    "\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "params = [41.7, 69.4, 2.5, 3.0, 2.7, 2.6, 9.1, 11.1, 8.0, 9.4, 42.7, 36.4, 47.5, 5.5, 20.7]\n",
    "\n",
    "# 归一化气泡大小\n",
    "bubble_size = np.array(params) * 10\n",
    "\n",
    "# 颜色区分不同算法\n",
    "colors = [\n",
    "    'blue', 'blue', 'green', 'green', 'green', 'green', 'orange', 'orange', 'orange', 'orange', \n",
    "    'red', 'red', 'purple', 'cyan', 'cyan'\n",
    "]\n",
    "\n",
    "# 创建主图\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    ax.scatter(gflops[i], map_values[i], s=bubble_size[i], color=colors[i], alpha=0.6, edgecolors=\"k\", label=model if model not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# 标注模型名称\n",
    "for i, model in enumerate(models):\n",
    "    ax.annotate(model, (gflops[i], map_values[i] - 1), fontsize=9, ha='center')\n",
    "\n",
    "# 画对比虚线，并在中间标注差值\n",
    "comparison_pairs = [\n",
    "    (\"RT-DETRv2\", \"MSAD-B\"),\n",
    "    (\"YOLOV8n\", \"MSAD-T\")\n",
    "]\n",
    "\n",
    "for model1, model2 in comparison_pairs:\n",
    "    i1, i2 = models.index(model1), models.index(model2)\n",
    "    gflops_diff = gflops[i2] - gflops[i1]\n",
    "    map_diff = map_values[i2] - map_values[i1]\n",
    "\n",
    "    if model1 == \"RT-DETRv2\" and model2 == \"MSAD-B\":\n",
    "        ax.plot([gflops[i1], gflops[i2]], [map_values[i1], map_values[i1]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "        ax.plot([gflops[i2], gflops[i2]], [map_values[i1], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.text((gflops[i1] + gflops[i2]) / 2, map_values[i1] - 1.2, f\"ΔGFLOPs: {np.abs(gflops_diff):.1f}\", color='blue', fontsize=10, ha='center')\n",
    "        ax.text(gflops[i2] + 1, (map_values[i1] + map_values[i2]) / 2, f\"ΔmAP: {map_diff:.1f}\", color='red', fontsize=10, ha='left')\n",
    "\n",
    "# 计算放大区域的边界\n",
    "i1, i2 = models.index(\"YOLOV8n\"), models.index(\"MSAD-T\")\n",
    "x_min, x_max = min(gflops[i1], gflops[i2]) - 1, max(gflops[i1], gflops[i2]) + 2\n",
    "y_min, y_max = min(map_values[i1], map_values[i2]) - 1.5, max(map_values[i1], map_values[i2]) + 1\n",
    "\n",
    "# 在主图上框选出放大区域\n",
    "ax.plot([x_min, x_max, x_max, x_min, x_min], [y_min, y_min, y_max, y_max, y_min], linestyle=\"dashed\", color=\"black\", alpha=0.4)\n",
    "\n",
    "# 计算主图框选区域的右上角和右下角（数据坐标系）\n",
    "ax.plot([x_max, 84.2], [y_max, 79.5], linestyle=\"dashed\", color=\"black\", alpha=0.4)\n",
    "ax.plot([x_max, 84.2], [y_min, 70], linestyle=\"dashed\", color=\"black\", alpha=0.4)\n",
    "\n",
    "# 添加放大图，并调整大小\n",
    "axins = fig.add_axes([0.6, 0.6, 0.2, 0.2])  \n",
    "\n",
    "# 只绘制YOLOV8n 和 MSAR-T\n",
    "axins.scatter(gflops[i1], map_values[i1], s=bubble_size[i1], color='green', alpha=0.6, edgecolors=\"k\", label=\"YOLOV8n\")\n",
    "axins.scatter(gflops[i2], map_values[i2], s=bubble_size[i2], color='cyan', alpha=0.6, edgecolors=\"k\", label=\"MSAR-T\")\n",
    "\n",
    "# 缩放放大图范围，使其与主图框选区域一致\n",
    "axins.set_xlim(x_min, x_max)\n",
    "axins.set_ylim(y_min, y_max)\n",
    "\n",
    "# 画虚线对比（先竖直后水平）\n",
    "axins.plot([gflops[i1], gflops[i1]], [map_values[i1], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "axins.plot([gflops[i1], gflops[i2]], [map_values[i2], map_values[i2]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "\n",
    "# 在放大图上标注差值\n",
    "axins.text(gflops[i1] + 2.2, (map_values[i1] + map_values[i2]) / 2 -0.2, f\"ΔmAP: {map_diff:.1f}\", color='red', fontsize=8, ha='right')\n",
    "axins.text((gflops[i1] + gflops[i2]) / 2, map_values[i2] + 0.2, f\"ΔGFLOPs: {gflops_diff:.1f}\", color='blue', fontsize=8, ha='center')\n",
    "\n",
    "# 放大图内部标题\n",
    "axins.set_title(\"YOLOV8n vs. MSAD-T\", fontsize=10)\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "\n",
    "# 设定坐标轴标签和标题\n",
    "ax.set_xlabel(\"GFLOPs\")\n",
    "ax.set_ylabel(\"mAP(%)\")\n",
    "# ax.set_title(\"GFLOPs vs. mAP (Bubble Size Represents Model Parameters)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# 定义模型数据\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAR-T\", \"MSAR-B\"\n",
    "]\n",
    "\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "fps_values = [55.6, 43.3, 158.7, 82.0, 238.1, 86.2, 156.3, 70.9, 188.7, 78.7, 59.9, 65.8, 55.6, 125.0, 64.1]\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "\n",
    "# 归一化气泡大小（基于 GFLOPs）\n",
    "bubble_size = np.array(gflops) * 5\n",
    "\n",
    "# 颜色区分不同算法\n",
    "colors = [\n",
    "    'blue', 'blue', 'green', 'green', 'green', 'green', 'orange', 'orange', 'orange', 'orange', \n",
    "    'red', 'red', 'purple', 'cyan', 'cyan'\n",
    "]\n",
    "\n",
    "# 创建主图\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    ax.scatter(fps_values[i], map_values[i], s=bubble_size[i], color=colors[i], alpha=0.6, edgecolors=\"k\", label=model if model not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# 标注模型名称\n",
    "for i, model in enumerate(models):\n",
    "    ax.annotate(model, (fps_values[i], map_values[i] - 1), fontsize=9, ha='center')\n",
    "\n",
    "# 设定坐标轴标签和标题\n",
    "ax.set_xlabel(\"FPS\")\n",
    "ax.set_ylabel(\"mAP\")\n",
    "ax.set_title(\"FPS vs. mAP (Bubble Size Represents GFLOPs)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# 显示图像\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define model data\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAR-T\", \"MSAR-B\"\n",
    "]\n",
    "\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "params = [41.7, 69.4, 2.5, 3.0, 2.7, 2.6, 9.1, 11.1, 8.0, 9.4, 42.7, 36.4, 47.5, 5.5, 20.7]\n",
    "\n",
    "# Normalize bubble sizes based on parameters (params M)\n",
    "bubble_size = np.array(params) * 10\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(gflops, map_values, s=bubble_size, alpha=0.6, edgecolors=\"k\")\n",
    "\n",
    "# Add annotations for each model (position below circles)\n",
    "for i, model in enumerate(models):\n",
    "    plt.annotate(model, (gflops[i], map_values[i] - 1), fontsize=9, ha='center')  # Shift text below\n",
    "\n",
    "# Add comparison arrows with value difference\n",
    "comparison_pairs = [\n",
    "    (\"RT-DETRv2\", \"MSAR-B\"),  # Comparing RT-DETRv2 and MSAR-B\n",
    "    (\"YOLOV8n\", \"MSAR-T\")     # Comparing YOLOV8n and MSAR-T\n",
    "]\n",
    "\n",
    "for model1, model2 in comparison_pairs:\n",
    "    i1, i2 = models.index(model1), models.index(model2)\n",
    "\n",
    "    # Compute differences\n",
    "    gflops_diff = np.abs(gflops[i2] - gflops[i1])\n",
    "    map_diff = map_values[i2] - map_values[i1]\n",
    "\n",
    "    # Draw horizontal arrow for GFLOPs difference\n",
    "    plt.arrow(\n",
    "        gflops[i1], map_values[i1],\n",
    "        gflops_diff, 0,\n",
    "        head_width=0.8, head_length=1, fc='blue', ec='blue', alpha=0.7, linestyle='dotted'\n",
    "    )\n",
    "    plt.text(\n",
    "        (gflops[i1] + gflops[i2]) / 2, map_values[i1] - 1.5,\n",
    "        f\"ΔGFLOPs: {gflops_diff:.1f}\", color='blue', fontsize=10, ha='center'\n",
    "    )\n",
    "\n",
    "    # Draw vertical arrow for mAP difference\n",
    "    plt.arrow(\n",
    "        gflops[i2], map_values[i1],\n",
    "        0, map_diff,\n",
    "        head_width=1, head_length=0.8, fc='red', ec='red', alpha=0.7, linestyle='dotted'\n",
    "    )\n",
    "    plt.text(\n",
    "        gflops[i2] + 2, (map_values[i1] + map_values[i2]) / 2,\n",
    "        f\"ΔmAP: {map_diff:.1f}\", color='red', fontsize=10, ha='left'\n",
    "    )\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"GFLOPs\")\n",
    "plt.ylabel(\"mAP\")\n",
    "plt.title(\"GFLOPs vs. mAP (Bubble Size Represents Model Parameters)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 合并json文件\n",
    "import json\n",
    "\n",
    "# 需要合并的 JSON 文件\n",
    "files = [\"outputs/sft_data/qwen_article_output.json\", \"outputs/sft_data/qwen_book_output.json\", \"outputs/sft_data/qwen_web_output.json\"]\n",
    "\n",
    "# 存储合并后的数据\n",
    "merged_data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # 读取 JSON\n",
    "        if isinstance(data, list):\n",
    "            merged_data.extend(data)  # 合并列表\n",
    "        else:\n",
    "            merged_data.append(data)  # 如果是字典，作为列表项添加\n",
    "\n",
    "# 写入合并后的 JSON 文件\n",
    "with open(\"outputs/sft_data/sft_output_all_250220.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 文件合并完成，结果已保存为 merged_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 合并jsonl文件\n",
    "import json\n",
    "\n",
    "# 需要合并的 JSONL 文件\n",
    "# files = [\"mateinfo/articals-1112.jsonl\", \"mateinfo/books-1113.jsonl\", \"mateinfo/web_deduped-1114.jsonl\"]\n",
    "\n",
    "files = [\"outputs/sft_data/final/train_article_data.jsonl\", \"outputs/sft_data/final/train_book_data.jsonl\", \"outputs/sft_data/final/train_web_data.jsonl\"]\n",
    "\n",
    "# 目标合并后的文件\n",
    "output_file = \"outputs/sft_data/final/train_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for line in infile:  # 逐行读取\n",
    "                outfile.write(line)  # 直接写入，不改变格式\n",
    "\n",
    "print(f\"JSONL 文件合并完成，结果已保存为 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_article_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_book_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_web_output.json\"\n",
    "file_path = \"sft_output_all_250220.json\"  # 59576\n",
    "# 从文件中读取 JSON 数据\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# 打印 JSON 数据的长度\n",
    "print(len(json_data))  # 如果是数组，输出数组长度；如果是对象，输出键值对数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"outputs/sft_data/final/train_data.jsonl\"  # 替换为你的 JSONL 文件名 31162  4001  516  26645   59576 \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"文件 {file_path} 共有 {line_count} 条数据\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/home/wyp/project/ForestLLM/outputs/article/qwen_article_output.json\"\n",
    "# \"/home/wyp/project/ForestLLM/outputs/0113/qwen_web_output.json\"\n",
    "\n",
    "\n",
    "# 查找重复 ID 的函数\n",
    "def find_duplicate_ids_with_consistency(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # 提取所有的 ID 和对应数据\n",
    "        id_map = defaultdict(list)\n",
    "        for entry in data:\n",
    "            if \"id\" in entry:\n",
    "                id_map[entry[\"id\"]].append(entry)\n",
    "\n",
    "        # 统计每个 ID 的出现次数\n",
    "        duplicates = {\n",
    "            id_: entries for id_, entries in id_map.items() if len(entries) > 1\n",
    "        }\n",
    "        duplicate_count = len(duplicates)\n",
    "\n",
    "        # 检查每组重复 ID 数据是否完全一致\n",
    "        consistency_results = {}\n",
    "        for id_, entries in duplicates.items():\n",
    "            # 使用第一条数据作为参考，逐条比对\n",
    "            reference_entry = json.dumps(entries[0], sort_keys=True)\n",
    "            all_consistent = all(\n",
    "                json.dumps(entry, sort_keys=True) == reference_entry\n",
    "                for entry in entries\n",
    "            )\n",
    "            consistency_results[id_] = {\n",
    "                \"count\": len(entries),\n",
    "                \"consistent\": all_consistent,\n",
    "            }\n",
    "\n",
    "        return consistency_results, duplicate_count\n",
    "    except Exception as e:\n",
    "        return str(e), 0\n",
    "\n",
    "\n",
    "# 执行检查\n",
    "consistency_results, duplicate_count = find_duplicate_ids_with_consistency(file_path)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"重复的 ID 总数: {duplicate_count}\")\n",
    "print(\"重复的 ID 检查结果:\")\n",
    "for id_, result in consistency_results.items():\n",
    "    status = \"一致\" if result[\"consistent\"] else \"不一致\"\n",
    "    print(f\"- ID: {id_}, 出现次数: {result['count']}, 数据是否一致: {status}\")\n",
    "print(f\"重复的 ID 总数: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_mixed_jsonl(input_file, train_output_file, eval_output_file, batch_size=1000):\n",
    "    \"\"\"\n",
    "    将混合的 JSONL 文件拆分成训练数据 (train) 和 评测数据 (eval)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    eval_data = []\n",
    "\n",
    "    # 读取原始 JSONL 文件\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())  # 解析 JSON\n",
    "                if \"messages\" in data:  # **训练数据**\n",
    "                    train_data.append(data)\n",
    "                elif \"history\" in data:  # **评测数据**\n",
    "                    eval_data.append(data)\n",
    "                else:\n",
    "                    print(f\"⚠️ 未知数据格式，跳过：{data}\")  # 遇到无法解析的数据，跳过\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"❌ JSON 解析失败，跳过：{line.strip()}\")\n",
    "\n",
    "            # **批量写入，减少 I/O 操作**\n",
    "            if len(train_data) >= batch_size:\n",
    "                with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "                    for entry in train_data:\n",
    "                        train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                train_data = []\n",
    "\n",
    "            if len(eval_data) >= batch_size:\n",
    "                with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "                    for entry in eval_data:\n",
    "                        eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                eval_data = []\n",
    "\n",
    "    # **写入剩余数据**\n",
    "    if train_data:\n",
    "        with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "            for entry in train_data:\n",
    "                train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if eval_data:\n",
    "        with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "            for entry in eval_data:\n",
    "                eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ 训练数据已保存到 {train_output_file}\")\n",
    "    print(f\"✅ 评测数据已保存到 {eval_output_file}\")\n",
    "\n",
    "# 示例调用\n",
    "split_mixed_jsonl(\"/home/wyp/project/ForestLLM/data_sft/eval_general_qa_readable.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/train_general_qa.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/eval_general_qa.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import os\n",
    "\n",
    "# 1. 读取嵌入数据\n",
    "original_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_original_text.npy\")  \n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_response.npy\")  \n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_knowledge.npy\")  \n",
    "\n",
    "# 2. 给数据添加标签\n",
    "original_labels = [\"original\"] * len(original_embeddings)\n",
    "generated_response_labels = [\"generated_response\"] * len(generated_response_embeddings)\n",
    "generated_knowledge_labels = [\"generated_knowledge\"] * len(generated_knowledge_embeddings)\n",
    "\n",
    "# 3. 合并数据\n",
    "all_embeddings = np.vstack((original_embeddings, generated_response_embeddings, generated_knowledge_embeddings))\n",
    "all_labels = np.array(original_labels + generated_response_labels + generated_knowledge_labels)\n",
    "\n",
    "# **4. 颜色渐进设置**\n",
    "gradient_palette = sns.blend_palette([\"blue\", \"purple\", \"red\"], n_colors=3, as_cmap=False)\n",
    "\n",
    "# **5. 形状控制**\n",
    "style_map = {\n",
    "    \"original\": \"o\",\n",
    "    \"generated_response\": \"s\",\n",
    "    \"generated_knowledge\": \"D\"\n",
    "}\n",
    "\n",
    "# 6. 先用 PCA 降到 50 维（加速 t-SNE）\n",
    "pca_path = \"outputs/emb_data/reduced_pca_50.npy\"\n",
    "if os.path.exists(pca_path):\n",
    "    pca_50_embeddings = np.load(pca_path)\n",
    "else:\n",
    "    pca_50 = PCA(n_components=50, random_state=42)\n",
    "    pca_50_embeddings = pca_50.fit_transform(all_embeddings)\n",
    "    np.save(pca_path, pca_50_embeddings)\n",
    "\n",
    "# 7. 用 t-SNE 或 UMAP 进一步降到 2 维\n",
    "method = \"tsne\"\n",
    "low_dim_path = f\"outputs/emb_data/reduced_{method}_2d.npy\"\n",
    "\n",
    "if os.path.exists(low_dim_path):\n",
    "    low_dim_embeddings = np.load(low_dim_path)\n",
    "else:\n",
    "    if method == \"tsne\":\n",
    "        tsne_reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "        low_dim_embeddings = tsne_reducer.fit_transform(pca_50_embeddings)\n",
    "    elif method == \"umap\":\n",
    "        umap_reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.3, random_state=42)\n",
    "        low_dim_embeddings = umap_reducer.fit_transform(pca_50_embeddings)\n",
    "    np.save(low_dim_path, low_dim_embeddings)\n",
    "\n",
    "# 8. 绘制可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x=low_dim_embeddings[:, 0], \n",
    "    y=low_dim_embeddings[:, 1], \n",
    "    hue=all_labels, \n",
    "    style=all_labels,  # **控制形状**\n",
    "    palette=gradient_palette,  # **渐变颜色**\n",
    "    markers=style_map,  # **自定义形状**\n",
    "    alpha=0.6,  # **透明度**\n",
    "    s=20  # **点大小**\n",
    ")\n",
    "\n",
    "plt.title(f\"Embedding Visualization ({method.upper()})\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(title=\"Data Type\", loc=\"best\")\n",
    "\n",
    "# **保存图片**\n",
    "img_path = f\"outputs/emb_data/embedding_{method}.png\"\n",
    "plt.savefig(img_path, dpi=300)\n",
    "print(f\"📸 Visualization saved as {img_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# 1. 读取嵌入数据\n",
    "original_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_original_text.npy\")  \n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_response.npy\")  \n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_knowledge.npy\")  \n",
    "\n",
    "# 2. 计算“生成数据”与“原始数据”的相似性\n",
    "print(\"🔄 Computing Cosine Similarity with Original Data ...\")\n",
    "response_similarity = cosine_similarity(generated_response_embeddings, original_embeddings)\n",
    "knowledge_similarity = cosine_similarity(generated_knowledge_embeddings, original_embeddings)\n",
    "\n",
    "# 3. 获取最高相似度（最接近的训练数据）\n",
    "max_response_similarity = np.max(response_similarity, axis=1)  # 每个 response 取最高相似度\n",
    "max_knowledge_similarity = np.max(knowledge_similarity, axis=1)  # 每个 knowledge 取最高相似度\n",
    "\n",
    "# 4. 计算 Sequence Identity to Training（转换为 0-100%）\n",
    "sequence_identity = np.concatenate((max_response_similarity, max_knowledge_similarity)) * 100  # 转换为百分比\n",
    "\n",
    "# 5. UMAP 降维\n",
    "umap_path = \"outputs/emb_data/reduced_umap_2d.npy\"\n",
    "if os.path.exists(umap_path):\n",
    "    print(\"✅ Loading UMAP-reduced embeddings from file...\")\n",
    "    low_dim_embeddings = np.load(umap_path)\n",
    "else:\n",
    "    print(\"🔄 Applying UMAP on Similarity Data ...\")\n",
    "    umap_reducer = umap.UMAP(n_components=2, metric=\"cosine\", n_neighbors=15, min_dist=0.3, random_state=42)\n",
    "    low_dim_embeddings = umap_reducer.fit_transform(sequence_identity.reshape(-1, 1))\n",
    "    np.save(umap_path, low_dim_embeddings)\n",
    "    print(f\"✅ UMAP embeddings saved to {umap_path}\")\n",
    "\n",
    "# 6. 定义颜色映射（渐变色）\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# 颜色范围：从 (100, 125, 125) 深色 → (160, 200, 180) 亮色\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", [\n",
    "    (100/255, 125/255, 125/255),  # 低相似度\n",
    "    (160/255, 200/255, 180/255)   # 高相似度\n",
    "])\n",
    "\n",
    "# 7. 绘制可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "sc = plt.scatter(\n",
    "    low_dim_embeddings[:, 0], \n",
    "    low_dim_embeddings[:, 1], \n",
    "    c=sequence_identity,  # 用 Sequence Identity 作为颜色\n",
    "    cmap=cmap,  # 渐变色\n",
    "    alpha=0.7,  \n",
    "    s=30,  \n",
    "    edgecolors=\"black\"\n",
    ")\n",
    "\n",
    "# 8. 添加颜色条（Colorbar）\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"% Sequence Identity to Training\")\n",
    "\n",
    "plt.title(\"UMAP Visualization Based on Similarity to Original Data\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "\n",
    "# **保存图片**\n",
    "img_path = \"outputs/emb_data/embedding_umap_similarity.png\"\n",
    "plt.savefig(img_path, dpi=300)\n",
    "print(f\"📸 Visualization saved as {img_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def is_empty_block(block: dict, keys: list):\n",
    "    \"\"\"判断指定 keys 中的字段是否都为空\"\"\"\n",
    "    return all(k in block and (not block[k]) for k in keys)\n",
    "\n",
    "# 读取原始 JSON 文件\n",
    "with open('/home/wyp/project/forest/forestllm-main/outputs/0321/欠费中断的第一版qwen_book_output.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# 如果顶层是列表，处理每个元素\n",
    "if isinstance(data, list):\n",
    "    for item in data:\n",
    "        if (\n",
    "            is_empty_block(item.get(\"question_setter\", {}), [\"questions\"]) and\n",
    "            is_empty_block(item.get(\"expert_agent\", {}), [\"refined_questions\"]) and\n",
    "            is_empty_block(item.get(\"virtual_teacher\", {}), [\"processed_results\"])\n",
    "        ):\n",
    "            item.pop(\"question_setter\", None)\n",
    "            item.pop(\"expert_agent\", None)\n",
    "            item.pop(\"virtual_teacher\", None)\n",
    "else:\n",
    "    # 单个对象（非列表）情况\n",
    "    if (\n",
    "        is_empty_block(data.get(\"question_setter\", {}), [\"questions\"]) and\n",
    "        is_empty_block(data.get(\"expert_agent\", {}), [\"refined_questions\"]) and\n",
    "        is_empty_block(data.get(\"virtual_teacher\", {}), [\"processed_results\"])\n",
    "    ):\n",
    "        data.pop(\"question_setter\", None)\n",
    "        data.pop(\"expert_agent\", None)\n",
    "        data.pop(\"virtual_teacher\", None)\n",
    "\n",
    "# 写入处理结果\n",
    "with open('qwen_book_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"处理完成：已清理所有完全为空的三元字段。\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"outputs/0321/qwen_book_output.json\"\n",
    "output_path = \"outputs/0321/qwen_book_output.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)  # 加载整个 JSON 数组\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for item in data:\n",
    "        outfile.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"已成功将 {len(data)} 条数据从 JSON 转换为 JSONL 格式：{output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
