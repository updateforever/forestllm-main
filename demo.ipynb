{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 合并json文件\n",
    "import json\n",
    "\n",
    "# 需要合并的 JSON 文件\n",
    "files = [\"qwen_article_output.json\", \"qwen_book_output.json\", \"qwen_web_output.json\"]\n",
    "\n",
    "# 存储合并后的数据\n",
    "merged_data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # 读取 JSON\n",
    "        if isinstance(data, list):\n",
    "            merged_data.extend(data)  # 合并列表\n",
    "        else:\n",
    "            merged_data.append(data)  # 如果是字典，作为列表项添加\n",
    "\n",
    "# 写入合并后的 JSON 文件\n",
    "with open(\"sft_output_all_250220.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON 文件合并完成，结果已保存为 merged_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL 文件合并完成，结果已保存为 mateinfo/merged_org_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "## 合并jsonl文件\n",
    "import json\n",
    "\n",
    "# 需要合并的 JSONL 文件\n",
    "files = [\"mateinfo/articals-1112.jsonl\", \"mateinfo/books-1113.jsonl\", \"mateinfo/web_deduped-1114.jsonl\"]\n",
    "\n",
    "# 目标合并后的文件\n",
    "output_file = \"mateinfo/merged_org_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for line in infile:  # 逐行读取\n",
    "                outfile.write(line)  # 直接写入，不改变格式\n",
    "\n",
    "print(f\"JSONL 文件合并完成，结果已保存为 {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59576\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_article_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_book_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_web_output.json\"\n",
    "file_path = \"sft_output_all_250220.json\"  # 59576\n",
    "# 从文件中读取 JSON 数据\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# 打印 JSON 数据的长度\n",
    "print(len(json_data))  # 如果是数组，输出数组长度；如果是对象，输出键值对数量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "文件 mateinfo/merged_org_data.jsonl 共有 31162 条数据\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"mateinfo/merged_org_data.jsonl\"  # 替换为你的 JSONL 文件名 31162  4001  516  26645 \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"文件 {file_path} 共有 {line_count} 条数据\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/home/wyp/project/ForestLLM/outputs/article/qwen_article_output.json\"\n",
    "# \"/home/wyp/project/ForestLLM/outputs/0113/qwen_web_output.json\"\n",
    "\n",
    "\n",
    "# 查找重复 ID 的函数\n",
    "def find_duplicate_ids_with_consistency(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # 提取所有的 ID 和对应数据\n",
    "        id_map = defaultdict(list)\n",
    "        for entry in data:\n",
    "            if \"id\" in entry:\n",
    "                id_map[entry[\"id\"]].append(entry)\n",
    "\n",
    "        # 统计每个 ID 的出现次数\n",
    "        duplicates = {\n",
    "            id_: entries for id_, entries in id_map.items() if len(entries) > 1\n",
    "        }\n",
    "        duplicate_count = len(duplicates)\n",
    "\n",
    "        # 检查每组重复 ID 数据是否完全一致\n",
    "        consistency_results = {}\n",
    "        for id_, entries in duplicates.items():\n",
    "            # 使用第一条数据作为参考，逐条比对\n",
    "            reference_entry = json.dumps(entries[0], sort_keys=True)\n",
    "            all_consistent = all(\n",
    "                json.dumps(entry, sort_keys=True) == reference_entry\n",
    "                for entry in entries\n",
    "            )\n",
    "            consistency_results[id_] = {\n",
    "                \"count\": len(entries),\n",
    "                \"consistent\": all_consistent,\n",
    "            }\n",
    "\n",
    "        return consistency_results, duplicate_count\n",
    "    except Exception as e:\n",
    "        return str(e), 0\n",
    "\n",
    "\n",
    "# 执行检查\n",
    "consistency_results, duplicate_count = find_duplicate_ids_with_consistency(file_path)\n",
    "\n",
    "# 输出结果\n",
    "print(f\"重复的 ID 总数: {duplicate_count}\")\n",
    "print(\"重复的 ID 检查结果:\")\n",
    "for id_, result in consistency_results.items():\n",
    "    status = \"一致\" if result[\"consistent\"] else \"不一致\"\n",
    "    print(f\"- ID: {id_}, 出现次数: {result['count']}, 数据是否一致: {status}\")\n",
    "print(f\"重复的 ID 总数: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_mixed_jsonl(input_file, train_output_file, eval_output_file, batch_size=1000):\n",
    "    \"\"\"\n",
    "    将混合的 JSONL 文件拆分成训练数据 (train) 和 评测数据 (eval)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    eval_data = []\n",
    "\n",
    "    # 读取原始 JSONL 文件\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())  # 解析 JSON\n",
    "                if \"messages\" in data:  # **训练数据**\n",
    "                    train_data.append(data)\n",
    "                elif \"history\" in data:  # **评测数据**\n",
    "                    eval_data.append(data)\n",
    "                else:\n",
    "                    print(f\"⚠️ 未知数据格式，跳过：{data}\")  # 遇到无法解析的数据，跳过\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"❌ JSON 解析失败，跳过：{line.strip()}\")\n",
    "\n",
    "            # **批量写入，减少 I/O 操作**\n",
    "            if len(train_data) >= batch_size:\n",
    "                with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "                    for entry in train_data:\n",
    "                        train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                train_data = []\n",
    "\n",
    "            if len(eval_data) >= batch_size:\n",
    "                with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "                    for entry in eval_data:\n",
    "                        eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                eval_data = []\n",
    "\n",
    "    # **写入剩余数据**\n",
    "    if train_data:\n",
    "        with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "            for entry in train_data:\n",
    "                train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if eval_data:\n",
    "        with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "            for entry in eval_data:\n",
    "                eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"✅ 训练数据已保存到 {train_output_file}\")\n",
    "    print(f\"✅ 评测数据已保存到 {eval_output_file}\")\n",
    "\n",
    "# 示例调用\n",
    "split_mixed_jsonl(\"/home/wyp/project/ForestLLM/data_sft/eval_general_qa_readable.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/train_general_qa.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/eval_general_qa.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据提取自表格\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# 绘制多个柱状图\n",
    "num_benchmarks = len(benchmarks)\n",
    "num_models = len(models)\n",
    "x = np.arange(num_benchmarks)\n",
    "\n",
    "# 设置颜色\n",
    "colors = [\"#7ea8be\", \"#4a6fa5\", \"#1f4e79\", \"#7ea8be\", \"#4a6fa5\", \"#1f4e79\"]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(num_models):\n",
    "    plt.bar(x + i * 0.12, df.iloc[:, i], width=0.12, label=models[i], color=colors[i])\n",
    "\n",
    "plt.xticks(x + 0.3, benchmarks, rotation=90)\n",
    "plt.ylabel(\"Score (%)\")\n",
    "plt.title(\"Benchmark Comparison Across AI Models\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据提取自表格\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# 绘制多个柱状图\n",
    "num_benchmarks = len(benchmarks)\n",
    "num_models = len(models)\n",
    "x = np.arange(num_benchmarks)\n",
    "\n",
    "# 设置颜色\n",
    "colors = [\"#7ea8be\", \"#4a6fa5\", \"#1f4e79\", \"#7ea8be\", \"#4a6fa5\", \"#FFA500\"]\n",
    "\n",
    "# 调整图表尺寸\n",
    "plt.figure(figsize=(18, 12))  # 增大图表尺寸\n",
    "\n",
    "for i in range(num_models):\n",
    "    plt.bar(x + i * 0.12, df.iloc[:, i], width=0.12, label=models[i], color=colors[i])\n",
    "\n",
    "plt.xticks(x + 0.3, benchmarks, rotation=90, fontsize=12)  # 调整 x 轴字体大小\n",
    "plt.yticks(fontsize=12)  # 调整 y 轴字体大小\n",
    "plt.ylabel(\"Score (%)\", fontsize=14)  # 调整 y 轴标签字体\n",
    "plt.title(\"Benchmark Comparison Across AI Models\", fontsize=18)  # 增大标题字体\n",
    "plt.legend(fontsize=12)  # 增大图例字体\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 数据提取自表格\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# 逐个基准测试绘制单独的柱状图\n",
    "for i, benchmark in enumerate(benchmarks):\n",
    "    plt.figure(figsize=(6, 4))  # 设置单个图表大小\n",
    "    \n",
    "    # 获取当前基准测试的数据\n",
    "    scores = df.iloc[i, :]\n",
    "    \n",
    "    # 颜色风格（淡紫色 & 深紫色），最高分用深紫色，其余用淡紫色\n",
    "    # colors = [\"#A7A2FF\" if score < max(scores) else \"#4A3DA3\" for score in scores]\n",
    "    colors = [\"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#4A3DA3\"]\n",
    "    # 绘制柱状图\n",
    "    bars = plt.bar(models, scores, color=colors)\n",
    "\n",
    "    # 在柱状图上方标注数值\n",
    "    for bar, score in zip(bars, scores):\n",
    "        if score == 0:\n",
    "            pass\n",
    "        else:\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}%',\n",
    "                    ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # 图表标题\n",
    "    plt.title(benchmark, fontsize=12)\n",
    "\n",
    "    # y 轴标签\n",
    "    plt.ylabel(\"Score (%)\", fontsize=8)\n",
    "    \n",
    "    # 隐藏 x 轴标签，仅展示模型名\n",
    "    plt.xticks(rotation=20, ha='right', fontsize=8)\n",
    "\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # 显示图表\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 修正后的基准列表（与 data 行数匹配）\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"FRAMES\", \"AlpacaEval2.0\",\n",
    "    \"ArenaHard\", \"LiveCodeBench\", \"MATH-500\", \"CNMO 2024\", \"C-Eval\"\n",
    "]\n",
    "\n",
    "# 模型列表\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "# 10 行数据\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8]   # C-Eval\n",
    "])\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# 设置子图布局（2行5列）\n",
    "num_rows, num_cols = 2, 5\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "fig.suptitle(\"Benchmark Comparison Across AI Models\", fontsize=20)\n",
    "\n",
    "# 遍历 benchmarks 绘制子图\n",
    "for i, (benchmark, ax) in enumerate(zip(benchmarks, axes.flatten())):\n",
    "    scores = df.iloc[i, :]\n",
    "    \n",
    "    # 颜色风格（最高分用深紫色）\n",
    "    colors = [\"#A7A2FF\" if score < max(scores) else \"#4A3DA3\" for score in scores]\n",
    "\n",
    "    # 绘制柱状图\n",
    "    bars = ax.bar(models, scores, color=colors)\n",
    "\n",
    "    # 在柱状图上方标注数值\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}%',\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # 设置子图标题\n",
    "    ax.set_title(benchmark, fontsize=12)\n",
    "    \n",
    "    # 确保 x 轴刻度正确\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=20, ha='right', fontsize=8)\n",
    "\n",
    "    # 统一 y 轴范围\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "# 调整布局\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# 显示图表\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# 1. 读取嵌入数据\n",
    "original_embeddings = np.load(\"outputs/emb_data/llama_embeddings_original.npy\")  # 原始数据嵌入\n",
    "generated_embeddings = np.load(\"outputs/emb_data/llama_embeddings_generated_response.npy\")  # 生成数据嵌入\n",
    "\n",
    "# 2. 给数据添加标签\n",
    "original_labels = [\"original\"] * len(original_embeddings)\n",
    "generated_labels = [\"generated\"] * len(generated_embeddings)\n",
    "\n",
    "# 3. 合并数据\n",
    "all_embeddings = np.vstack((original_embeddings, generated_embeddings))\n",
    "all_labels = np.array(original_labels + generated_labels)\n",
    "\n",
    "# 4. 选择降维方法 (可选 PCA, t-SNE, UMAP)\n",
    "def reduce_dimension(embeddings, method=\"umap\"):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=2, n_neighbors=50, min_dist=0.1, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Method should be 'pca', 'tsne', or 'umap'\")\n",
    "    \n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "# 5. 进行降维\n",
    "low_dim_embeddings = reduce_dimension(all_embeddings, method=\"umap\")\n",
    "\n",
    "# 6. 绘制可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=low_dim_embeddings[:, 0], y=low_dim_embeddings[:, 1], hue=all_labels, alpha=0.7)\n",
    "plt.title(\"Embedding Visualization (UMAP)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(title=\"Data Type\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 3D 降维\n",
    "low_dim_embeddings_3d = umap.UMAP(n_components=3, random_state=42).fit_transform(all_embeddings)\n",
    "\n",
    "# 画 3D 图\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "scatter = ax.scatter(low_dim_embeddings_3d[:, 0], low_dim_embeddings_3d[:, 1], low_dim_embeddings_3d[:, 2], c=(all_labels == \"generated\"), cmap=\"coolwarm\", alpha=0.7)\n",
    "ax.set_title(\"3D Embedding Visualization (UMAP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# 1. 读取嵌入数据\n",
    "original_embeddings = np.load(\"outputs/emb_data/llama_embeddings_original.npy\")  # 原始数据嵌入\n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/llama_embeddings_generated_response.npy\")  # 生成数据嵌入\n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/llama_embeddings_generated_knowledge.npy\")  # 知识点嵌入\n",
    "\n",
    "# 2. 给数据添加标签\n",
    "original_labels = [\"original\"] * len(original_embeddings)\n",
    "generated_response_labels = [\"generated_response\"] * len(generated_response_embeddings)\n",
    "generated_knowledge_labels = [\"generated_knowledge\"] * len(generated_knowledge_embeddings)\n",
    "\n",
    "# 3. 合并数据\n",
    "all_embeddings = np.vstack((original_embeddings, generated_response_embeddings, generated_knowledge_embeddings))\n",
    "all_labels = np.array(original_labels + generated_response_labels + generated_knowledge_labels)\n",
    "\n",
    "# 4. 先用 PCA 降到 256 维\n",
    "print(\"🔄 Applying PCA (4096 → 256) ...\")\n",
    "pca_256 = PCA(n_components=256, random_state=42)\n",
    "pca_256_embeddings = pca_256.fit_transform(all_embeddings)\n",
    "\n",
    "# 6. 进行 UMAP 降到 2 维\n",
    "print(\"🔄 Applying UMAP (50 → 2) ...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, n_neighbors=150, min_dist=0.05, random_state=42)\n",
    "low_dim_embeddings = umap_reducer.fit_transform(pca_256_embeddings)\n",
    "\n",
    "# 7. 绘制可视化\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=low_dim_embeddings[:, 0], y=low_dim_embeddings[:, 1], hue=all_labels, alpha=0.7, palette=\"Set1\")\n",
    "plt.title(\"Embedding Visualization (UMAP)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(title=\"Data Type\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
