{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## åˆå¹¶jsonæ–‡ä»¶\n",
    "import json\n",
    "\n",
    "# éœ€è¦åˆå¹¶çš„ JSON æ–‡ä»¶\n",
    "files = [\"qwen_article_output.json\", \"qwen_book_output.json\", \"qwen_web_output.json\"]\n",
    "\n",
    "# å­˜å‚¨åˆå¹¶åçš„æ•°æ®\n",
    "merged_data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # è¯»å– JSON\n",
    "        if isinstance(data, list):\n",
    "            merged_data.extend(data)  # åˆå¹¶åˆ—è¡¨\n",
    "        else:\n",
    "            merged_data.append(data)  # å¦‚æœæ˜¯å­—å…¸ï¼Œä½œä¸ºåˆ—è¡¨é¡¹æ·»åŠ \n",
    "\n",
    "# å†™å…¥åˆå¹¶åçš„ JSON æ–‡ä»¶\n",
    "with open(\"sft_output_all_250220.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º merged_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSONL æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º mateinfo/merged_org_data.jsonl\n"
     ]
    }
   ],
   "source": [
    "## åˆå¹¶jsonlæ–‡ä»¶\n",
    "import json\n",
    "\n",
    "# éœ€è¦åˆå¹¶çš„ JSONL æ–‡ä»¶\n",
    "files = [\"mateinfo/articals-1112.jsonl\", \"mateinfo/books-1113.jsonl\", \"mateinfo/web_deduped-1114.jsonl\"]\n",
    "\n",
    "# ç›®æ ‡åˆå¹¶åçš„æ–‡ä»¶\n",
    "output_file = \"mateinfo/merged_org_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for line in infile:  # é€è¡Œè¯»å–\n",
    "                outfile.write(line)  # ç›´æ¥å†™å…¥ï¼Œä¸æ”¹å˜æ ¼å¼\n",
    "\n",
    "print(f\"JSONL æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59576\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_article_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_book_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_web_output.json\"\n",
    "file_path = \"sft_output_all_250220.json\"  # 59576\n",
    "# ä»æ–‡ä»¶ä¸­è¯»å– JSON æ•°æ®\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# æ‰“å° JSON æ•°æ®çš„é•¿åº¦\n",
    "print(len(json_data))  # å¦‚æœæ˜¯æ•°ç»„ï¼Œè¾“å‡ºæ•°ç»„é•¿åº¦ï¼›å¦‚æœæ˜¯å¯¹è±¡ï¼Œè¾“å‡ºé”®å€¼å¯¹æ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ–‡ä»¶ mateinfo/merged_org_data.jsonl å…±æœ‰ 31162 æ¡æ•°æ®\n"
     ]
    }
   ],
   "source": [
    "\n",
    "file_path = \"mateinfo/merged_org_data.jsonl\"  # æ›¿æ¢ä¸ºä½ çš„ JSONL æ–‡ä»¶å 31162  4001  516  26645 \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"æ–‡ä»¶ {file_path} å…±æœ‰ {line_count} æ¡æ•°æ®\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# æ–‡ä»¶è·¯å¾„\n",
    "file_path = \"/home/wyp/project/ForestLLM/outputs/article/qwen_article_output.json\"\n",
    "# \"/home/wyp/project/ForestLLM/outputs/0113/qwen_web_output.json\"\n",
    "\n",
    "\n",
    "# æŸ¥æ‰¾é‡å¤ ID çš„å‡½æ•°\n",
    "def find_duplicate_ids_with_consistency(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # æå–æ‰€æœ‰çš„ ID å’Œå¯¹åº”æ•°æ®\n",
    "        id_map = defaultdict(list)\n",
    "        for entry in data:\n",
    "            if \"id\" in entry:\n",
    "                id_map[entry[\"id\"]].append(entry)\n",
    "\n",
    "        # ç»Ÿè®¡æ¯ä¸ª ID çš„å‡ºç°æ¬¡æ•°\n",
    "        duplicates = {\n",
    "            id_: entries for id_, entries in id_map.items() if len(entries) > 1\n",
    "        }\n",
    "        duplicate_count = len(duplicates)\n",
    "\n",
    "        # æ£€æŸ¥æ¯ç»„é‡å¤ ID æ•°æ®æ˜¯å¦å®Œå…¨ä¸€è‡´\n",
    "        consistency_results = {}\n",
    "        for id_, entries in duplicates.items():\n",
    "            # ä½¿ç”¨ç¬¬ä¸€æ¡æ•°æ®ä½œä¸ºå‚è€ƒï¼Œé€æ¡æ¯”å¯¹\n",
    "            reference_entry = json.dumps(entries[0], sort_keys=True)\n",
    "            all_consistent = all(\n",
    "                json.dumps(entry, sort_keys=True) == reference_entry\n",
    "                for entry in entries\n",
    "            )\n",
    "            consistency_results[id_] = {\n",
    "                \"count\": len(entries),\n",
    "                \"consistent\": all_consistent,\n",
    "            }\n",
    "\n",
    "        return consistency_results, duplicate_count\n",
    "    except Exception as e:\n",
    "        return str(e), 0\n",
    "\n",
    "\n",
    "# æ‰§è¡Œæ£€æŸ¥\n",
    "consistency_results, duplicate_count = find_duplicate_ids_with_consistency(file_path)\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(f\"é‡å¤çš„ ID æ€»æ•°: {duplicate_count}\")\n",
    "print(\"é‡å¤çš„ ID æ£€æŸ¥ç»“æœ:\")\n",
    "for id_, result in consistency_results.items():\n",
    "    status = \"ä¸€è‡´\" if result[\"consistent\"] else \"ä¸ä¸€è‡´\"\n",
    "    print(f\"- ID: {id_}, å‡ºç°æ¬¡æ•°: {result['count']}, æ•°æ®æ˜¯å¦ä¸€è‡´: {status}\")\n",
    "print(f\"é‡å¤çš„ ID æ€»æ•°: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_mixed_jsonl(input_file, train_output_file, eval_output_file, batch_size=1000):\n",
    "    \"\"\"\n",
    "    å°†æ··åˆçš„ JSONL æ–‡ä»¶æ‹†åˆ†æˆè®­ç»ƒæ•°æ® (train) å’Œ è¯„æµ‹æ•°æ® (eval)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    eval_data = []\n",
    "\n",
    "    # è¯»å–åŸå§‹ JSONL æ–‡ä»¶\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())  # è§£æ JSON\n",
    "                if \"messages\" in data:  # **è®­ç»ƒæ•°æ®**\n",
    "                    train_data.append(data)\n",
    "                elif \"history\" in data:  # **è¯„æµ‹æ•°æ®**\n",
    "                    eval_data.append(data)\n",
    "                else:\n",
    "                    print(f\"âš ï¸ æœªçŸ¥æ•°æ®æ ¼å¼ï¼Œè·³è¿‡ï¼š{data}\")  # é‡åˆ°æ— æ³•è§£æçš„æ•°æ®ï¼Œè·³è¿‡\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ï¼š{line.strip()}\")\n",
    "\n",
    "            # **æ‰¹é‡å†™å…¥ï¼Œå‡å°‘ I/O æ“ä½œ**\n",
    "            if len(train_data) >= batch_size:\n",
    "                with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "                    for entry in train_data:\n",
    "                        train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                train_data = []\n",
    "\n",
    "            if len(eval_data) >= batch_size:\n",
    "                with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "                    for entry in eval_data:\n",
    "                        eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                eval_data = []\n",
    "\n",
    "    # **å†™å…¥å‰©ä½™æ•°æ®**\n",
    "    if train_data:\n",
    "        with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "            for entry in train_data:\n",
    "                train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if eval_data:\n",
    "        with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "            for entry in eval_data:\n",
    "                eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ° {train_output_file}\")\n",
    "    print(f\"âœ… è¯„æµ‹æ•°æ®å·²ä¿å­˜åˆ° {eval_output_file}\")\n",
    "\n",
    "# ç¤ºä¾‹è°ƒç”¨\n",
    "split_mixed_jsonl(\"/home/wyp/project/ForestLLM/data_sft/eval_general_qa_readable.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/train_general_qa.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/eval_general_qa.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# æ•°æ®æå–è‡ªè¡¨æ ¼\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# ç»˜åˆ¶å¤šä¸ªæŸ±çŠ¶å›¾\n",
    "num_benchmarks = len(benchmarks)\n",
    "num_models = len(models)\n",
    "x = np.arange(num_benchmarks)\n",
    "\n",
    "# è®¾ç½®é¢œè‰²\n",
    "colors = [\"#7ea8be\", \"#4a6fa5\", \"#1f4e79\", \"#7ea8be\", \"#4a6fa5\", \"#1f4e79\"]\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(num_models):\n",
    "    plt.bar(x + i * 0.12, df.iloc[:, i], width=0.12, label=models[i], color=colors[i])\n",
    "\n",
    "plt.xticks(x + 0.3, benchmarks, rotation=90)\n",
    "plt.ylabel(\"Score (%)\")\n",
    "plt.title(\"Benchmark Comparison Across AI Models\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# æ•°æ®æå–è‡ªè¡¨æ ¼\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# ç»˜åˆ¶å¤šä¸ªæŸ±çŠ¶å›¾\n",
    "num_benchmarks = len(benchmarks)\n",
    "num_models = len(models)\n",
    "x = np.arange(num_benchmarks)\n",
    "\n",
    "# è®¾ç½®é¢œè‰²\n",
    "colors = [\"#7ea8be\", \"#4a6fa5\", \"#1f4e79\", \"#7ea8be\", \"#4a6fa5\", \"#FFA500\"]\n",
    "\n",
    "# è°ƒæ•´å›¾è¡¨å°ºå¯¸\n",
    "plt.figure(figsize=(18, 12))  # å¢å¤§å›¾è¡¨å°ºå¯¸\n",
    "\n",
    "for i in range(num_models):\n",
    "    plt.bar(x + i * 0.12, df.iloc[:, i], width=0.12, label=models[i], color=colors[i])\n",
    "\n",
    "plt.xticks(x + 0.3, benchmarks, rotation=90, fontsize=12)  # è°ƒæ•´ x è½´å­—ä½“å¤§å°\n",
    "plt.yticks(fontsize=12)  # è°ƒæ•´ y è½´å­—ä½“å¤§å°\n",
    "plt.ylabel(\"Score (%)\", fontsize=14)  # è°ƒæ•´ y è½´æ ‡ç­¾å­—ä½“\n",
    "plt.title(\"Benchmark Comparison Across AI Models\", fontsize=18)  # å¢å¤§æ ‡é¢˜å­—ä½“\n",
    "plt.legend(fontsize=12)  # å¢å¤§å›¾ä¾‹å­—ä½“\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# æ•°æ®æå–è‡ªè¡¨æ ¼\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"DROP\", \"IF-Eval\",\n",
    "    \"GPOA Diamond\", \"SimpleQA\", \"FRAMES\", \"AlpacaEval2.0\", \"ArenaHard\",\n",
    "    \"LiveCodeBench\", \"Codeforces\", \"SWE Verified\", \"Aider-Polyglot\",\n",
    "    \"AIME 2024\", \"MATH-500\", \"CNMO 2024\",\n",
    "    \"CLUEWSC\", \"C-Eval\", \"C-SimpleQA\"\n",
    "]\n",
    "\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [88.3, 83.7, 91.6, 83.9, 90.2, 92.2],  # DROP\n",
    "    [86.5, 84.3, 86.1, 84.8, 0, 83.3],  # IF-Eval\n",
    "    [65.0, 49.9, 59.1, 60.0, 75.7, 71.5],  # GPOA Diamond\n",
    "    [28.4, 38.2, 24.9, 7.0, 47.0, 30.1],  # SimpleQA\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [20.3, 23.6, 58.7, 93.4, 96.6, 96.3],  # Codeforces\n",
    "    [50.8, 38.8, 42.0, 41.6, 48.9, 49.2],  # SWE Verified\n",
    "    [45.3, 16.0, 49.6, 32.9, 61.7, 53.3],  # Aider-Polyglot\n",
    "    [16.0, 9.3, 39.2, 63.6, 79.2, 79.8],   # AIME 2024\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [85.4, 87.9, 90.9, 89.9, 0, 92.8],  # CLUEWSC\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8],  # C-Eval\n",
    "    [55.4, 58.7, 68.0, 40.3, 0, 63.7]   # C-SimpleQA\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# é€ä¸ªåŸºå‡†æµ‹è¯•ç»˜åˆ¶å•ç‹¬çš„æŸ±çŠ¶å›¾\n",
    "for i, benchmark in enumerate(benchmarks):\n",
    "    plt.figure(figsize=(6, 4))  # è®¾ç½®å•ä¸ªå›¾è¡¨å¤§å°\n",
    "    \n",
    "    # è·å–å½“å‰åŸºå‡†æµ‹è¯•çš„æ•°æ®\n",
    "    scores = df.iloc[i, :]\n",
    "    \n",
    "    # é¢œè‰²é£æ ¼ï¼ˆæ·¡ç´«è‰² & æ·±ç´«è‰²ï¼‰ï¼Œæœ€é«˜åˆ†ç”¨æ·±ç´«è‰²ï¼Œå…¶ä½™ç”¨æ·¡ç´«è‰²\n",
    "    # colors = [\"#A7A2FF\" if score < max(scores) else \"#4A3DA3\" for score in scores]\n",
    "    colors = [\"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#A7A2FF\", \"#4A3DA3\"]\n",
    "    # ç»˜åˆ¶æŸ±çŠ¶å›¾\n",
    "    bars = plt.bar(models, scores, color=colors)\n",
    "\n",
    "    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ–¹æ ‡æ³¨æ•°å€¼\n",
    "    for bar, score in zip(bars, scores):\n",
    "        if score == 0:\n",
    "            pass\n",
    "        else:\n",
    "            plt.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}%',\n",
    "                    ha='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "    # å›¾è¡¨æ ‡é¢˜\n",
    "    plt.title(benchmark, fontsize=12)\n",
    "\n",
    "    # y è½´æ ‡ç­¾\n",
    "    plt.ylabel(\"Score (%)\", fontsize=8)\n",
    "    \n",
    "    # éšè— x è½´æ ‡ç­¾ï¼Œä»…å±•ç¤ºæ¨¡å‹å\n",
    "    plt.xticks(rotation=20, ha='right', fontsize=8)\n",
    "\n",
    "    # è°ƒæ•´å¸ƒå±€\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # æ˜¾ç¤ºå›¾è¡¨\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ä¿®æ­£åçš„åŸºå‡†åˆ—è¡¨ï¼ˆä¸ data è¡Œæ•°åŒ¹é…ï¼‰\n",
    "benchmarks = [\n",
    "    \"MMLU\", \"MMLU-Redux\", \"MMLU-Pro\", \"FRAMES\", \"AlpacaEval2.0\",\n",
    "    \"ArenaHard\", \"LiveCodeBench\", \"MATH-500\", \"CNMO 2024\", \"C-Eval\"\n",
    "]\n",
    "\n",
    "# æ¨¡å‹åˆ—è¡¨\n",
    "models = [\"Claude-3.5-Sonnet\", \"GPT-4o\", \"DeepSeek V3\", \"OpenAI o1-mini\", \"OpenAI o1-1217\", \"DeepSeek R1\"]\n",
    "\n",
    "# 10 è¡Œæ•°æ®\n",
    "data = np.array([\n",
    "    [88.3, 87.2, 88.5, 85.2, 91.8, 90.8],  # MMLU\n",
    "    [88.9, 88.0, 89.1, 86.7, 0, 92.9],  # MMLU-Redux\n",
    "    [78.0, 72.6, 75.9, 80.3, 0, 84.0],  # MMLU-Pro\n",
    "    [72.5, 80.5, 73.3, 76.9, 0, 82.5],  # FRAMES\n",
    "    [52.0, 51.1, 70.0, 57.8, 0, 87.6],  # AlpacaEval2.0\n",
    "    [85.2, 80.4, 85.5, 92.0, 0, 92.3],  # ArenaHard\n",
    "    [38.9, 32.9, 36.0, 53.8, 63.4, 65.9],  # LiveCodeBench\n",
    "    [78.3, 74.6, 90.2, 90.0, 96.4, 97.3],  # MATH-500\n",
    "    [13.1, 10.8, 43.2, 67.6, 0, 78.8],  # CNMO 2024\n",
    "    [76.7, 76.0, 86.5, 68.9, 0, 91.8]   # C-Eval\n",
    "])\n",
    "\n",
    "# åˆ›å»ºDataFrame\n",
    "df = pd.DataFrame(data, index=benchmarks, columns=models)\n",
    "\n",
    "# è®¾ç½®å­å›¾å¸ƒå±€ï¼ˆ2è¡Œ5åˆ—ï¼‰\n",
    "num_rows, num_cols = 2, 5\n",
    "fig, axes = plt.subplots(num_rows, num_cols, figsize=(20, 10))\n",
    "fig.suptitle(\"Benchmark Comparison Across AI Models\", fontsize=20)\n",
    "\n",
    "# éå† benchmarks ç»˜åˆ¶å­å›¾\n",
    "for i, (benchmark, ax) in enumerate(zip(benchmarks, axes.flatten())):\n",
    "    scores = df.iloc[i, :]\n",
    "    \n",
    "    # é¢œè‰²é£æ ¼ï¼ˆæœ€é«˜åˆ†ç”¨æ·±ç´«è‰²ï¼‰\n",
    "    colors = [\"#A7A2FF\" if score < max(scores) else \"#4A3DA3\" for score in scores]\n",
    "\n",
    "    # ç»˜åˆ¶æŸ±çŠ¶å›¾\n",
    "    bars = ax.bar(models, scores, color=colors)\n",
    "\n",
    "    # åœ¨æŸ±çŠ¶å›¾ä¸Šæ–¹æ ‡æ³¨æ•°å€¼\n",
    "    for bar, score in zip(bars, scores):\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 1, f'{score:.1f}%',\n",
    "                ha='center', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # è®¾ç½®å­å›¾æ ‡é¢˜\n",
    "    ax.set_title(benchmark, fontsize=12)\n",
    "    \n",
    "    # ç¡®ä¿ x è½´åˆ»åº¦æ­£ç¡®\n",
    "    ax.set_xticks(range(len(models)))\n",
    "    ax.set_xticklabels(models, rotation=20, ha='right', fontsize=8)\n",
    "\n",
    "    # ç»Ÿä¸€ y è½´èŒƒå›´\n",
    "    ax.set_ylim(0, 100)\n",
    "\n",
    "# è°ƒæ•´å¸ƒå±€\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "\n",
    "# æ˜¾ç¤ºå›¾è¡¨\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# 1. è¯»å–åµŒå…¥æ•°æ®\n",
    "original_embeddings = np.load(\"outputs/emb_data/llama_embeddings_original.npy\")  # åŸå§‹æ•°æ®åµŒå…¥\n",
    "generated_embeddings = np.load(\"outputs/emb_data/llama_embeddings_generated_response.npy\")  # ç”Ÿæˆæ•°æ®åµŒå…¥\n",
    "\n",
    "# 2. ç»™æ•°æ®æ·»åŠ æ ‡ç­¾\n",
    "original_labels = [\"original\"] * len(original_embeddings)\n",
    "generated_labels = [\"generated\"] * len(generated_embeddings)\n",
    "\n",
    "# 3. åˆå¹¶æ•°æ®\n",
    "all_embeddings = np.vstack((original_embeddings, generated_embeddings))\n",
    "all_labels = np.array(original_labels + generated_labels)\n",
    "\n",
    "# 4. é€‰æ‹©é™ç»´æ–¹æ³• (å¯é€‰ PCA, t-SNE, UMAP)\n",
    "def reduce_dimension(embeddings, method=\"umap\"):\n",
    "    if method == \"pca\":\n",
    "        reducer = PCA(n_components=2)\n",
    "    elif method == \"tsne\":\n",
    "        reducer = TSNE(n_components=2, perplexity=50, random_state=42)\n",
    "    elif method == \"umap\":\n",
    "        reducer = umap.UMAP(n_components=2, n_neighbors=50, min_dist=0.1, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Method should be 'pca', 'tsne', or 'umap'\")\n",
    "    \n",
    "    return reducer.fit_transform(embeddings)\n",
    "\n",
    "# 5. è¿›è¡Œé™ç»´\n",
    "low_dim_embeddings = reduce_dimension(all_embeddings, method=\"umap\")\n",
    "\n",
    "# 6. ç»˜åˆ¶å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=low_dim_embeddings[:, 0], y=low_dim_embeddings[:, 1], hue=all_labels, alpha=0.7)\n",
    "plt.title(\"Embedding Visualization (UMAP)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(title=\"Data Type\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# 3D é™ç»´\n",
    "low_dim_embeddings_3d = umap.UMAP(n_components=3, random_state=42).fit_transform(all_embeddings)\n",
    "\n",
    "# ç”» 3D å›¾\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111, projection=\"3d\")\n",
    "scatter = ax.scatter(low_dim_embeddings_3d[:, 0], low_dim_embeddings_3d[:, 1], low_dim_embeddings_3d[:, 2], c=(all_labels == \"generated\"), cmap=\"coolwarm\", alpha=0.7)\n",
    "ax.set_title(\"3D Embedding Visualization (UMAP)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "import umap.umap_ as umap\n",
    "\n",
    "# 1. è¯»å–åµŒå…¥æ•°æ®\n",
    "original_embeddings = np.load(\"outputs/emb_data/llama_embeddings_original.npy\")  # åŸå§‹æ•°æ®åµŒå…¥\n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/llama_embeddings_generated_response.npy\")  # ç”Ÿæˆæ•°æ®åµŒå…¥\n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/llama_embeddings_generated_knowledge.npy\")  # çŸ¥è¯†ç‚¹åµŒå…¥\n",
    "\n",
    "# 2. ç»™æ•°æ®æ·»åŠ æ ‡ç­¾\n",
    "original_labels = [\"original\"] * len(original_embeddings)\n",
    "generated_response_labels = [\"generated_response\"] * len(generated_response_embeddings)\n",
    "generated_knowledge_labels = [\"generated_knowledge\"] * len(generated_knowledge_embeddings)\n",
    "\n",
    "# 3. åˆå¹¶æ•°æ®\n",
    "all_embeddings = np.vstack((original_embeddings, generated_response_embeddings, generated_knowledge_embeddings))\n",
    "all_labels = np.array(original_labels + generated_response_labels + generated_knowledge_labels)\n",
    "\n",
    "# 4. å…ˆç”¨ PCA é™åˆ° 256 ç»´\n",
    "print(\"ğŸ”„ Applying PCA (4096 â†’ 256) ...\")\n",
    "pca_256 = PCA(n_components=256, random_state=42)\n",
    "pca_256_embeddings = pca_256.fit_transform(all_embeddings)\n",
    "\n",
    "# 6. è¿›è¡Œ UMAP é™åˆ° 2 ç»´\n",
    "print(\"ğŸ”„ Applying UMAP (50 â†’ 2) ...\")\n",
    "umap_reducer = umap.UMAP(n_components=2, n_neighbors=150, min_dist=0.05, random_state=42)\n",
    "low_dim_embeddings = umap_reducer.fit_transform(pca_256_embeddings)\n",
    "\n",
    "# 7. ç»˜åˆ¶å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=low_dim_embeddings[:, 0], y=low_dim_embeddings[:, 1], hue=all_labels, alpha=0.7, palette=\"Set1\")\n",
    "plt.title(\"Embedding Visualization (UMAP)\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(title=\"Data Type\", loc=\"best\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swift",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
