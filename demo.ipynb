{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åˆå¹¶åæ€»æ¡æ•°: 5718\n",
      "âœ… å·²ä¿å­˜è‡³: /mnt/sda/wyp/forestllm-main/output/step5/merged_step5_dedup.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"åŠ è½½ jsonl æ–‡ä»¶\"\"\"\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line.strip()) for line in f if line.strip()]\n",
    "\n",
    "def save_jsonl(file_path, data):\n",
    "    \"\"\"ä¿å­˜ jsonl æ–‡ä»¶\"\"\"\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "def merge_and_dedup_jsonl(files, output_path):\n",
    "    all_entries = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for file in files:\n",
    "        data = load_jsonl(file)\n",
    "        for entry in data:\n",
    "            entry_id = entry.get(\"id\")\n",
    "            if entry_id and entry_id not in seen_ids:\n",
    "                seen_ids.add(entry_id)\n",
    "                all_entries.append(entry)\n",
    "\n",
    "    print(f\"åˆå¹¶åæ€»æ¡æ•°: {len(all_entries)}\")\n",
    "    save_jsonl(output_path, all_entries)\n",
    "    print(f\"âœ… å·²ä¿å­˜è‡³: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_files = [\n",
    "        \"/mnt/sda/wyp/forestllm-main/output/step5/part1_step5.jsonl\",\n",
    "        \"/mnt/sda/wyp/forestllm-main/output/step5/part2_step5.jsonl\",\n",
    "        \"/mnt/sda/wyp/forestllm-main/output/step5/part3_step5.jsonl\"\n",
    "    ]\n",
    "    output_file = \"/mnt/sda/wyp/forestllm-main/output/step5/merged_step5_dedup.jsonl\"\n",
    "    merge_and_dedup_jsonl(input_files, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Part 1: /mnt/sda/wyp/forestllm-main/output/step5/part1.jsonl (1906 lines)\n",
      "ğŸ“„ Part 2: /mnt/sda/wyp/forestllm-main/output/step5/part2.jsonl (1906 lines)\n",
      "ğŸ“„ Part 3: /mnt/sda/wyp/forestllm-main/output/step5/part3.jsonl (1906 lines)\n",
      "âœ… Split completed: 5718 lines into 3 parts.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "def split_jsonl_three_parts(file_path, output_dir=None):\n",
    "    if output_dir is None:\n",
    "        output_dir = os.path.dirname(file_path)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    total = len(lines)\n",
    "    part_size = math.ceil(total / 3)\n",
    "\n",
    "    parts = [\n",
    "        lines[:part_size],\n",
    "        lines[part_size:part_size * 2],\n",
    "        lines[part_size * 2:]\n",
    "    ]\n",
    "\n",
    "    for i, part in enumerate(parts, 1):\n",
    "        part_path = os.path.join(output_dir, f\"part{i}.jsonl\")\n",
    "        with open(part_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.writelines(part)\n",
    "        print(f\"ğŸ“„ Part {i}: {part_path} ({len(part)} lines)\")\n",
    "\n",
    "    print(f\"âœ… Split completed: {total} lines into 3 parts.\")\n",
    "\n",
    "# è°ƒç”¨\n",
    "split_jsonl_three_parts(\"/mnt/sda/wyp/forestllm-main/output/step5/merged_step4_dedup.jsonl\", \"/mnt/sda/wyp/forestllm-main/output/step5\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹æ•°æ®\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAR-T\", \"MSAR-B\"\n",
    "]\n",
    "\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "params = [41.7, 69.4, 2.5, 3.0, 2.7, 2.6, 9.1, 11.1, 8.0, 9.4, 42.7, 36.4, 47.5, 5.5, 20.7]\n",
    "\n",
    "# å½’ä¸€åŒ–æ°”æ³¡å¤§å°\n",
    "bubble_size = np.array(params) * 10\n",
    "\n",
    "# é¢œè‰²åŒºåˆ†ä¸åŒç®—æ³•\n",
    "colors = [\n",
    "    'blue', 'blue', 'green', 'green', 'green', 'green', 'orange', 'orange', 'orange', 'orange', \n",
    "    'red', 'red', 'purple', 'cyan', 'cyan'\n",
    "]\n",
    "\n",
    "# åˆ›å»ºä¸»å›¾\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    ax.scatter(gflops[i], map_values[i], s=bubble_size[i], color=colors[i], alpha=0.6, edgecolors=\"k\", label=model if model not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# æ ‡æ³¨æ¨¡å‹åç§°\n",
    "for i, model in enumerate(models):\n",
    "    ax.annotate(model, (gflops[i], map_values[i] - 1), fontsize=9, ha='center')\n",
    "\n",
    "# ç”»å¯¹æ¯”è™šçº¿ï¼Œå¹¶åœ¨ä¸­é—´æ ‡æ³¨å·®å€¼\n",
    "comparison_pairs = [\n",
    "    (\"RT-DETRv2\", \"MSAR-B\"),\n",
    "    (\"YOLOV8n\", \"MSAR-T\")\n",
    "]\n",
    "\n",
    "for model1, model2 in comparison_pairs:\n",
    "    i1, i2 = models.index(model1), models.index(model2)\n",
    "    gflops_diff = gflops[i2] - gflops[i1]\n",
    "    map_diff = map_values[i2] - map_values[i1]\n",
    "\n",
    "    if model1 == \"RT-DETRv2\" and model2 == \"MSAR-B\":\n",
    "        ax.plot([gflops[i1], gflops[i2]], [map_values[i1], map_values[i1]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "        ax.plot([gflops[i2], gflops[i2]], [map_values[i1], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.text((gflops[i1] + gflops[i2]) / 2, map_values[i1] - 1, f\"Î”GFLOPs: {gflops_diff:.1f}\", color='blue', fontsize=10, ha='center')\n",
    "        ax.text(gflops[i2] + 1, (map_values[i1] + map_values[i2]) / 2, f\"Î”mAP: {map_diff:.1f}\", color='red', fontsize=10, ha='left')\n",
    "\n",
    "    elif model1 == \"YOLOV8n\" and model2 == \"MSAR-T\":\n",
    "        ax.plot([gflops[i1], gflops[i1]], [map_values[i1], map_values[i2]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "        ax.plot([gflops[i1], gflops[i2]], [map_values[i2], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.text(gflops[i1] - 2, (map_values[i1] + map_values[i2]) / 2, f\"Î”mAP: {map_diff:.1f}\", color='blue', fontsize=10, ha='right')\n",
    "        ax.text((gflops[i1] + gflops[i2]) / 2, map_values[i2] + 1, f\"Î”GFLOPs: {gflops_diff:.1f}\", color='red', fontsize=10, ha='center')\n",
    "\n",
    "# è®¾å®šåæ ‡è½´æ ‡ç­¾å’Œæ ‡é¢˜\n",
    "ax.set_xlabel(\"GFLOPs\")\n",
    "ax.set_ylabel(\"mAP\")\n",
    "ax.set_title(\"GFLOPs vs. mAP (Bubble Size Represents Model Parameters)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾åƒ\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# å®šä¹‰å‡½æ•°å°†æ•°æ®åæ ‡è½¬æ¢ä¸º figure å½’ä¸€åŒ–åæ ‡\n",
    "def data_to_fig_coords(ax, x, y):\n",
    "    return ax.transData.transform((x, y))  # å°†æ•°æ®åæ ‡è½¬æ¢ä¸ºå±å¹•åæ ‡\n",
    "\n",
    "def fig_to_axes_coords(ax, x, y):\n",
    "    inv = ax.transAxes.inverted()  # è·å– figure å½’ä¸€åŒ–åæ ‡\n",
    "    return inv.transform((x, y))  # è½¬æ¢ä¸º axes åæ ‡\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹æ•°æ®\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAD-T\", \"MSAD-B\"\n",
    "]\n",
    "\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "params = [41.7, 69.4, 2.5, 3.0, 2.7, 2.6, 9.1, 11.1, 8.0, 9.4, 42.7, 36.4, 47.5, 5.5, 20.7]\n",
    "\n",
    "# å½’ä¸€åŒ–æ°”æ³¡å¤§å°\n",
    "bubble_size = np.array(params) * 10\n",
    "\n",
    "# é¢œè‰²åŒºåˆ†ä¸åŒç®—æ³•\n",
    "colors = [\n",
    "    'blue', 'blue', 'green', 'green', 'green', 'green', 'orange', 'orange', 'orange', 'orange', \n",
    "    'red', 'red', 'purple', 'cyan', 'cyan'\n",
    "]\n",
    "\n",
    "# åˆ›å»ºä¸»å›¾\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    ax.scatter(gflops[i], map_values[i], s=bubble_size[i], color=colors[i], alpha=0.6, edgecolors=\"k\", label=model if model not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# æ ‡æ³¨æ¨¡å‹åç§°\n",
    "for i, model in enumerate(models):\n",
    "    ax.annotate(model, (gflops[i], map_values[i] - 1), fontsize=9, ha='center')\n",
    "\n",
    "# ç”»å¯¹æ¯”è™šçº¿ï¼Œå¹¶åœ¨ä¸­é—´æ ‡æ³¨å·®å€¼\n",
    "comparison_pairs = [\n",
    "    (\"RT-DETRv2\", \"MSAD-B\"),\n",
    "    (\"YOLOV8n\", \"MSAD-T\")\n",
    "]\n",
    "\n",
    "for model1, model2 in comparison_pairs:\n",
    "    i1, i2 = models.index(model1), models.index(model2)\n",
    "    gflops_diff = gflops[i2] - gflops[i1]\n",
    "    map_diff = map_values[i2] - map_values[i1]\n",
    "\n",
    "    if model1 == \"RT-DETRv2\" and model2 == \"MSAD-B\":\n",
    "        ax.plot([gflops[i1], gflops[i2]], [map_values[i1], map_values[i1]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "        ax.plot([gflops[i2], gflops[i2]], [map_values[i1], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "        \n",
    "        ax.text((gflops[i1] + gflops[i2]) / 2, map_values[i1] - 1.2, f\"Î”GFLOPs: {np.abs(gflops_diff):.1f}\", color='blue', fontsize=10, ha='center')\n",
    "        ax.text(gflops[i2] + 1, (map_values[i1] + map_values[i2]) / 2, f\"Î”mAP: {map_diff:.1f}\", color='red', fontsize=10, ha='left')\n",
    "\n",
    "# è®¡ç®—æ”¾å¤§åŒºåŸŸçš„è¾¹ç•Œ\n",
    "i1, i2 = models.index(\"YOLOV8n\"), models.index(\"MSAD-T\")\n",
    "x_min, x_max = min(gflops[i1], gflops[i2]) - 1, max(gflops[i1], gflops[i2]) + 2\n",
    "y_min, y_max = min(map_values[i1], map_values[i2]) - 1.5, max(map_values[i1], map_values[i2]) + 1\n",
    "\n",
    "# åœ¨ä¸»å›¾ä¸Šæ¡†é€‰å‡ºæ”¾å¤§åŒºåŸŸ\n",
    "ax.plot([x_min, x_max, x_max, x_min, x_min], [y_min, y_min, y_max, y_max, y_min], linestyle=\"dashed\", color=\"black\", alpha=0.4)\n",
    "\n",
    "# è®¡ç®—ä¸»å›¾æ¡†é€‰åŒºåŸŸçš„å³ä¸Šè§’å’Œå³ä¸‹è§’ï¼ˆæ•°æ®åæ ‡ç³»ï¼‰\n",
    "ax.plot([x_max, 84.2], [y_max, 79.5], linestyle=\"dashed\", color=\"black\", alpha=0.4)\n",
    "ax.plot([x_max, 84.2], [y_min, 70], linestyle=\"dashed\", color=\"black\", alpha=0.4)\n",
    "\n",
    "# æ·»åŠ æ”¾å¤§å›¾ï¼Œå¹¶è°ƒæ•´å¤§å°\n",
    "axins = fig.add_axes([0.6, 0.6, 0.2, 0.2])  \n",
    "\n",
    "# åªç»˜åˆ¶YOLOV8n å’Œ MSAR-T\n",
    "axins.scatter(gflops[i1], map_values[i1], s=bubble_size[i1], color='green', alpha=0.6, edgecolors=\"k\", label=\"YOLOV8n\")\n",
    "axins.scatter(gflops[i2], map_values[i2], s=bubble_size[i2], color='cyan', alpha=0.6, edgecolors=\"k\", label=\"MSAR-T\")\n",
    "\n",
    "# ç¼©æ”¾æ”¾å¤§å›¾èŒƒå›´ï¼Œä½¿å…¶ä¸ä¸»å›¾æ¡†é€‰åŒºåŸŸä¸€è‡´\n",
    "axins.set_xlim(x_min, x_max)\n",
    "axins.set_ylim(y_min, y_max)\n",
    "\n",
    "# ç”»è™šçº¿å¯¹æ¯”ï¼ˆå…ˆç«–ç›´åæ°´å¹³ï¼‰\n",
    "axins.plot([gflops[i1], gflops[i1]], [map_values[i1], map_values[i2]], linestyle='dotted', color='red', alpha=0.7)\n",
    "axins.plot([gflops[i1], gflops[i2]], [map_values[i2], map_values[i2]], linestyle='dotted', color='blue', alpha=0.7)\n",
    "\n",
    "# åœ¨æ”¾å¤§å›¾ä¸Šæ ‡æ³¨å·®å€¼\n",
    "axins.text(gflops[i1] + 2.2, (map_values[i1] + map_values[i2]) / 2 -0.2, f\"Î”mAP: {map_diff:.1f}\", color='red', fontsize=8, ha='right')\n",
    "axins.text((gflops[i1] + gflops[i2]) / 2, map_values[i2] + 0.2, f\"Î”GFLOPs: {gflops_diff:.1f}\", color='blue', fontsize=8, ha='center')\n",
    "\n",
    "# æ”¾å¤§å›¾å†…éƒ¨æ ‡é¢˜\n",
    "axins.set_title(\"YOLOV8n vs. MSAD-T\", fontsize=10)\n",
    "axins.set_xticks([])\n",
    "axins.set_yticks([])\n",
    "\n",
    "# è®¾å®šåæ ‡è½´æ ‡ç­¾å’Œæ ‡é¢˜\n",
    "ax.set_xlabel(\"GFLOPs\")\n",
    "ax.set_ylabel(\"mAP(%)\")\n",
    "# ax.set_title(\"GFLOPs vs. mAP (Bubble Size Represents Model Parameters)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾åƒ\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# å®šä¹‰æ¨¡å‹æ•°æ®\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAR-T\", \"MSAR-B\"\n",
    "]\n",
    "\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "fps_values = [55.6, 43.3, 158.7, 82.0, 238.1, 86.2, 156.3, 70.9, 188.7, 78.7, 59.9, 65.8, 55.6, 125.0, 64.1]\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "\n",
    "# å½’ä¸€åŒ–æ°”æ³¡å¤§å°ï¼ˆåŸºäº GFLOPsï¼‰\n",
    "bubble_size = np.array(gflops) * 5\n",
    "\n",
    "# é¢œè‰²åŒºåˆ†ä¸åŒç®—æ³•\n",
    "colors = [\n",
    "    'blue', 'blue', 'green', 'green', 'green', 'green', 'orange', 'orange', 'orange', 'orange', \n",
    "    'red', 'red', 'purple', 'cyan', 'cyan'\n",
    "]\n",
    "\n",
    "# åˆ›å»ºä¸»å›¾\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "for i, model in enumerate(models):\n",
    "    ax.scatter(fps_values[i], map_values[i], s=bubble_size[i], color=colors[i], alpha=0.6, edgecolors=\"k\", label=model if model not in ax.get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "# æ ‡æ³¨æ¨¡å‹åç§°\n",
    "for i, model in enumerate(models):\n",
    "    ax.annotate(model, (fps_values[i], map_values[i] - 1), fontsize=9, ha='center')\n",
    "\n",
    "# è®¾å®šåæ ‡è½´æ ‡ç­¾å’Œæ ‡é¢˜\n",
    "ax.set_xlabel(\"FPS\")\n",
    "ax.set_ylabel(\"mAP\")\n",
    "ax.set_title(\"FPS vs. mAP (Bubble Size Represents GFLOPs)\")\n",
    "ax.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# æ˜¾ç¤ºå›¾åƒ\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define model data\n",
    "models = [\n",
    "    \"Faster R-CNN\", \"Cascade R-CNN\", \"YOLOV5n\", \"YOLOV8n\", \"YOLOV10n\", \"YOLOV11n\", \n",
    "    \"YOLOV5s\", \"YOLOV8s\", \"YOLOV10s\", \"YOLOV11s\", \"RT-DETR\", \"RT-DETRv2\", \"DINO\", \"MSAR-T\", \"MSAR-B\"\n",
    "]\n",
    "\n",
    "gflops = [91.3, 119.0, 7.1, 8.1, 8.4, 6.4, 23.8, 28.4, 24.4, 21.6, 130.5, 100.5, 119.0, 12.2, 47.8]\n",
    "map_values = [51.7, 47.9, 67.2, 74.2, 63.4, 68.8, 68.9, 79.9, 73.4, 76.1, 53.8, 64.0, 57.0, 76.3, 81.9]\n",
    "params = [41.7, 69.4, 2.5, 3.0, 2.7, 2.6, 9.1, 11.1, 8.0, 9.4, 42.7, 36.4, 47.5, 5.5, 20.7]\n",
    "\n",
    "# Normalize bubble sizes based on parameters (params M)\n",
    "bubble_size = np.array(params) * 10\n",
    "\n",
    "# Create scatter plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(gflops, map_values, s=bubble_size, alpha=0.6, edgecolors=\"k\")\n",
    "\n",
    "# Add annotations for each model (position below circles)\n",
    "for i, model in enumerate(models):\n",
    "    plt.annotate(model, (gflops[i], map_values[i] - 1), fontsize=9, ha='center')  # Shift text below\n",
    "\n",
    "# Add comparison arrows with value difference\n",
    "comparison_pairs = [\n",
    "    (\"RT-DETRv2\", \"MSAR-B\"),  # Comparing RT-DETRv2 and MSAR-B\n",
    "    (\"YOLOV8n\", \"MSAR-T\")     # Comparing YOLOV8n and MSAR-T\n",
    "]\n",
    "\n",
    "for model1, model2 in comparison_pairs:\n",
    "    i1, i2 = models.index(model1), models.index(model2)\n",
    "\n",
    "    # Compute differences\n",
    "    gflops_diff = np.abs(gflops[i2] - gflops[i1])\n",
    "    map_diff = map_values[i2] - map_values[i1]\n",
    "\n",
    "    # Draw horizontal arrow for GFLOPs difference\n",
    "    plt.arrow(\n",
    "        gflops[i1], map_values[i1],\n",
    "        gflops_diff, 0,\n",
    "        head_width=0.8, head_length=1, fc='blue', ec='blue', alpha=0.7, linestyle='dotted'\n",
    "    )\n",
    "    plt.text(\n",
    "        (gflops[i1] + gflops[i2]) / 2, map_values[i1] - 1.5,\n",
    "        f\"Î”GFLOPs: {gflops_diff:.1f}\", color='blue', fontsize=10, ha='center'\n",
    "    )\n",
    "\n",
    "    # Draw vertical arrow for mAP difference\n",
    "    plt.arrow(\n",
    "        gflops[i2], map_values[i1],\n",
    "        0, map_diff,\n",
    "        head_width=1, head_length=0.8, fc='red', ec='red', alpha=0.7, linestyle='dotted'\n",
    "    )\n",
    "    plt.text(\n",
    "        gflops[i2] + 2, (map_values[i1] + map_values[i2]) / 2,\n",
    "        f\"Î”mAP: {map_diff:.1f}\", color='red', fontsize=10, ha='left'\n",
    "    )\n",
    "\n",
    "# Labels and title\n",
    "plt.xlabel(\"GFLOPs\")\n",
    "plt.ylabel(\"mAP\")\n",
    "plt.title(\"GFLOPs vs. mAP (Bubble Size Represents Model Parameters)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## åˆå¹¶jsonæ–‡ä»¶\n",
    "import json\n",
    "\n",
    "# éœ€è¦åˆå¹¶çš„ JSON æ–‡ä»¶\n",
    "files = [\"outputs/sft_data/qwen_article_output.json\", \"outputs/sft_data/qwen_book_output.json\", \"outputs/sft_data/qwen_web_output.json\"]\n",
    "\n",
    "# å­˜å‚¨åˆå¹¶åçš„æ•°æ®\n",
    "merged_data = []\n",
    "\n",
    "for file in files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)  # è¯»å– JSON\n",
    "        if isinstance(data, list):\n",
    "            merged_data.extend(data)  # åˆå¹¶åˆ—è¡¨\n",
    "        else:\n",
    "            merged_data.append(data)  # å¦‚æœæ˜¯å­—å…¸ï¼Œä½œä¸ºåˆ—è¡¨é¡¹æ·»åŠ \n",
    "\n",
    "# å†™å…¥åˆå¹¶åçš„ JSON æ–‡ä»¶\n",
    "with open(\"outputs/sft_data/sft_output_all_250220.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(merged_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"JSON æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º merged_output.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## åˆå¹¶jsonlæ–‡ä»¶\n",
    "import json\n",
    "\n",
    "# éœ€è¦åˆå¹¶çš„ JSONL æ–‡ä»¶\n",
    "# files = [\"mateinfo/articals-1112.jsonl\", \"mateinfo/books-1113.jsonl\", \"mateinfo/web_deduped-1114.jsonl\"]\n",
    "\n",
    "files = [\"outputs/sft_data/final/train_article_data.jsonl\", \"outputs/sft_data/final/train_book_data.jsonl\", \"outputs/sft_data/final/train_web_data.jsonl\"]\n",
    "\n",
    "# ç›®æ ‡åˆå¹¶åçš„æ–‡ä»¶\n",
    "output_file = \"outputs/sft_data/final/train_data.jsonl\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for file in files:\n",
    "        with open(file, \"r\", encoding=\"utf-8\") as infile:\n",
    "            for line in infile:  # é€è¡Œè¯»å–\n",
    "                outfile.write(line)  # ç›´æ¥å†™å…¥ï¼Œä¸æ”¹å˜æ ¼å¼\n",
    "\n",
    "print(f\"JSONL æ–‡ä»¶åˆå¹¶å®Œæˆï¼Œç»“æœå·²ä¿å­˜ä¸º {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_article_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_book_output.json\"\n",
    "# file_path = \"/home/wyp/project/forest/forestllm-main/qwen_web_output.json\"\n",
    "file_path = \"sft_output_all_250220.json\"  # 59576\n",
    "# ä»æ–‡ä»¶ä¸­è¯»å– JSON æ•°æ®\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# æ‰“å° JSON æ•°æ®çš„é•¿åº¦\n",
    "print(len(json_data))  # å¦‚æœæ˜¯æ•°ç»„ï¼Œè¾“å‡ºæ•°ç»„é•¿åº¦ï¼›å¦‚æœæ˜¯å¯¹è±¡ï¼Œè¾“å‡ºé”®å€¼å¯¹æ•°é‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "file_path = \"outputs/sft_data/final/train_data.jsonl\"  # æ›¿æ¢ä¸ºä½ çš„ JSONL æ–‡ä»¶å 31162  4001  516  26645   59576 \n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    line_count = sum(1 for _ in f)\n",
    "\n",
    "print(f\"æ–‡ä»¶ {file_path} å…±æœ‰ {line_count} æ¡æ•°æ®\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "# æ–‡ä»¶è·¯å¾„\n",
    "file_path = \"/home/wyp/project/ForestLLM/outputs/article/qwen_article_output.json\"\n",
    "# \"/home/wyp/project/ForestLLM/outputs/0113/qwen_web_output.json\"\n",
    "\n",
    "\n",
    "# æŸ¥æ‰¾é‡å¤ ID çš„å‡½æ•°\n",
    "def find_duplicate_ids_with_consistency(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # æå–æ‰€æœ‰çš„ ID å’Œå¯¹åº”æ•°æ®\n",
    "        id_map = defaultdict(list)\n",
    "        for entry in data:\n",
    "            if \"id\" in entry:\n",
    "                id_map[entry[\"id\"]].append(entry)\n",
    "\n",
    "        # ç»Ÿè®¡æ¯ä¸ª ID çš„å‡ºç°æ¬¡æ•°\n",
    "        duplicates = {\n",
    "            id_: entries for id_, entries in id_map.items() if len(entries) > 1\n",
    "        }\n",
    "        duplicate_count = len(duplicates)\n",
    "\n",
    "        # æ£€æŸ¥æ¯ç»„é‡å¤ ID æ•°æ®æ˜¯å¦å®Œå…¨ä¸€è‡´\n",
    "        consistency_results = {}\n",
    "        for id_, entries in duplicates.items():\n",
    "            # ä½¿ç”¨ç¬¬ä¸€æ¡æ•°æ®ä½œä¸ºå‚è€ƒï¼Œé€æ¡æ¯”å¯¹\n",
    "            reference_entry = json.dumps(entries[0], sort_keys=True)\n",
    "            all_consistent = all(\n",
    "                json.dumps(entry, sort_keys=True) == reference_entry\n",
    "                for entry in entries\n",
    "            )\n",
    "            consistency_results[id_] = {\n",
    "                \"count\": len(entries),\n",
    "                \"consistent\": all_consistent,\n",
    "            }\n",
    "\n",
    "        return consistency_results, duplicate_count\n",
    "    except Exception as e:\n",
    "        return str(e), 0\n",
    "\n",
    "\n",
    "# æ‰§è¡Œæ£€æŸ¥\n",
    "consistency_results, duplicate_count = find_duplicate_ids_with_consistency(file_path)\n",
    "\n",
    "# è¾“å‡ºç»“æœ\n",
    "print(f\"é‡å¤çš„ ID æ€»æ•°: {duplicate_count}\")\n",
    "print(\"é‡å¤çš„ ID æ£€æŸ¥ç»“æœ:\")\n",
    "for id_, result in consistency_results.items():\n",
    "    status = \"ä¸€è‡´\" if result[\"consistent\"] else \"ä¸ä¸€è‡´\"\n",
    "    print(f\"- ID: {id_}, å‡ºç°æ¬¡æ•°: {result['count']}, æ•°æ®æ˜¯å¦ä¸€è‡´: {status}\")\n",
    "print(f\"é‡å¤çš„ ID æ€»æ•°: {duplicate_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def split_mixed_jsonl(input_file, train_output_file, eval_output_file, batch_size=1000):\n",
    "    \"\"\"\n",
    "    å°†æ··åˆçš„ JSONL æ–‡ä»¶æ‹†åˆ†æˆè®­ç»ƒæ•°æ® (train) å’Œ è¯„æµ‹æ•°æ® (eval)\n",
    "    \"\"\"\n",
    "    train_data = []\n",
    "    eval_data = []\n",
    "\n",
    "    # è¯»å–åŸå§‹ JSONL æ–‡ä»¶\n",
    "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line.strip())  # è§£æ JSON\n",
    "                if \"messages\" in data:  # **è®­ç»ƒæ•°æ®**\n",
    "                    train_data.append(data)\n",
    "                elif \"history\" in data:  # **è¯„æµ‹æ•°æ®**\n",
    "                    eval_data.append(data)\n",
    "                else:\n",
    "                    print(f\"âš ï¸ æœªçŸ¥æ•°æ®æ ¼å¼ï¼Œè·³è¿‡ï¼š{data}\")  # é‡åˆ°æ— æ³•è§£æçš„æ•°æ®ï¼Œè·³è¿‡\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"âŒ JSON è§£æå¤±è´¥ï¼Œè·³è¿‡ï¼š{line.strip()}\")\n",
    "\n",
    "            # **æ‰¹é‡å†™å…¥ï¼Œå‡å°‘ I/O æ“ä½œ**\n",
    "            if len(train_data) >= batch_size:\n",
    "                with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "                    for entry in train_data:\n",
    "                        train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                train_data = []\n",
    "\n",
    "            if len(eval_data) >= batch_size:\n",
    "                with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "                    for entry in eval_data:\n",
    "                        eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "                eval_data = []\n",
    "\n",
    "    # **å†™å…¥å‰©ä½™æ•°æ®**\n",
    "    if train_data:\n",
    "        with open(train_output_file, \"a\", encoding=\"utf-8\") as train_f:\n",
    "            for entry in train_data:\n",
    "                train_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    if eval_data:\n",
    "        with open(eval_output_file, \"a\", encoding=\"utf-8\") as eval_f:\n",
    "            for entry in eval_data:\n",
    "                eval_f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜åˆ° {train_output_file}\")\n",
    "    print(f\"âœ… è¯„æµ‹æ•°æ®å·²ä¿å­˜åˆ° {eval_output_file}\")\n",
    "\n",
    "# ç¤ºä¾‹è°ƒç”¨\n",
    "split_mixed_jsonl(\"/home/wyp/project/ForestLLM/data_sft/eval_general_qa_readable.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/train_general_qa.jsonl\", \"/home/wyp/project/ForestLLM/data_sft/eval_general_qa.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap.umap_ as umap\n",
    "import os\n",
    "\n",
    "# 1. è¯»å–åµŒå…¥æ•°æ®\n",
    "original_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_original_text.npy\")  \n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_response.npy\")  \n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_knowledge.npy\")  \n",
    "\n",
    "# 2. ç»™æ•°æ®æ·»åŠ æ ‡ç­¾\n",
    "original_labels = [\"original\"] * len(original_embeddings)\n",
    "generated_response_labels = [\"generated_response\"] * len(generated_response_embeddings)\n",
    "generated_knowledge_labels = [\"generated_knowledge\"] * len(generated_knowledge_embeddings)\n",
    "\n",
    "# 3. åˆå¹¶æ•°æ®\n",
    "all_embeddings = np.vstack((original_embeddings, generated_response_embeddings, generated_knowledge_embeddings))\n",
    "all_labels = np.array(original_labels + generated_response_labels + generated_knowledge_labels)\n",
    "\n",
    "# **4. é¢œè‰²æ¸è¿›è®¾ç½®**\n",
    "gradient_palette = sns.blend_palette([\"blue\", \"purple\", \"red\"], n_colors=3, as_cmap=False)\n",
    "\n",
    "# **5. å½¢çŠ¶æ§åˆ¶**\n",
    "style_map = {\n",
    "    \"original\": \"o\",\n",
    "    \"generated_response\": \"s\",\n",
    "    \"generated_knowledge\": \"D\"\n",
    "}\n",
    "\n",
    "# 6. å…ˆç”¨ PCA é™åˆ° 50 ç»´ï¼ˆåŠ é€Ÿ t-SNEï¼‰\n",
    "pca_path = \"outputs/emb_data/reduced_pca_50.npy\"\n",
    "if os.path.exists(pca_path):\n",
    "    pca_50_embeddings = np.load(pca_path)\n",
    "else:\n",
    "    pca_50 = PCA(n_components=50, random_state=42)\n",
    "    pca_50_embeddings = pca_50.fit_transform(all_embeddings)\n",
    "    np.save(pca_path, pca_50_embeddings)\n",
    "\n",
    "# 7. ç”¨ t-SNE æˆ– UMAP è¿›ä¸€æ­¥é™åˆ° 2 ç»´\n",
    "method = \"tsne\"\n",
    "low_dim_path = f\"outputs/emb_data/reduced_{method}_2d.npy\"\n",
    "\n",
    "if os.path.exists(low_dim_path):\n",
    "    low_dim_embeddings = np.load(low_dim_path)\n",
    "else:\n",
    "    if method == \"tsne\":\n",
    "        tsne_reducer = TSNE(n_components=2, perplexity=30, random_state=42)\n",
    "        low_dim_embeddings = tsne_reducer.fit_transform(pca_50_embeddings)\n",
    "    elif method == \"umap\":\n",
    "        umap_reducer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.3, random_state=42)\n",
    "        low_dim_embeddings = umap_reducer.fit_transform(pca_50_embeddings)\n",
    "    np.save(low_dim_path, low_dim_embeddings)\n",
    "\n",
    "# 8. ç»˜åˆ¶å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    x=low_dim_embeddings[:, 0], \n",
    "    y=low_dim_embeddings[:, 1], \n",
    "    hue=all_labels, \n",
    "    style=all_labels,  # **æ§åˆ¶å½¢çŠ¶**\n",
    "    palette=gradient_palette,  # **æ¸å˜é¢œè‰²**\n",
    "    markers=style_map,  # **è‡ªå®šä¹‰å½¢çŠ¶**\n",
    "    alpha=0.6,  # **é€æ˜åº¦**\n",
    "    s=20  # **ç‚¹å¤§å°**\n",
    ")\n",
    "\n",
    "plt.title(f\"Embedding Visualization ({method.upper()})\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "plt.legend(title=\"Data Type\", loc=\"best\")\n",
    "\n",
    "# **ä¿å­˜å›¾ç‰‡**\n",
    "img_path = f\"outputs/emb_data/embedding_{method}.png\"\n",
    "plt.savefig(img_path, dpi=300)\n",
    "print(f\"ğŸ“¸ Visualization saved as {img_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "\n",
    "# 1. è¯»å–åµŒå…¥æ•°æ®\n",
    "original_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_original_text.npy\")  \n",
    "generated_response_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_response.npy\")  \n",
    "generated_knowledge_embeddings = np.load(\"outputs/emb_data/bert/llama_embeddings_generated_knowledge.npy\")  \n",
    "\n",
    "# 2. è®¡ç®—â€œç”Ÿæˆæ•°æ®â€ä¸â€œåŸå§‹æ•°æ®â€çš„ç›¸ä¼¼æ€§\n",
    "print(\"ğŸ”„ Computing Cosine Similarity with Original Data ...\")\n",
    "response_similarity = cosine_similarity(generated_response_embeddings, original_embeddings)\n",
    "knowledge_similarity = cosine_similarity(generated_knowledge_embeddings, original_embeddings)\n",
    "\n",
    "# 3. è·å–æœ€é«˜ç›¸ä¼¼åº¦ï¼ˆæœ€æ¥è¿‘çš„è®­ç»ƒæ•°æ®ï¼‰\n",
    "max_response_similarity = np.max(response_similarity, axis=1)  # æ¯ä¸ª response å–æœ€é«˜ç›¸ä¼¼åº¦\n",
    "max_knowledge_similarity = np.max(knowledge_similarity, axis=1)  # æ¯ä¸ª knowledge å–æœ€é«˜ç›¸ä¼¼åº¦\n",
    "\n",
    "# 4. è®¡ç®— Sequence Identity to Trainingï¼ˆè½¬æ¢ä¸º 0-100%ï¼‰\n",
    "sequence_identity = np.concatenate((max_response_similarity, max_knowledge_similarity)) * 100  # è½¬æ¢ä¸ºç™¾åˆ†æ¯”\n",
    "\n",
    "# 5. UMAP é™ç»´\n",
    "umap_path = \"outputs/emb_data/reduced_umap_2d.npy\"\n",
    "if os.path.exists(umap_path):\n",
    "    print(\"âœ… Loading UMAP-reduced embeddings from file...\")\n",
    "    low_dim_embeddings = np.load(umap_path)\n",
    "else:\n",
    "    print(\"ğŸ”„ Applying UMAP on Similarity Data ...\")\n",
    "    umap_reducer = umap.UMAP(n_components=2, metric=\"cosine\", n_neighbors=15, min_dist=0.3, random_state=42)\n",
    "    low_dim_embeddings = umap_reducer.fit_transform(sequence_identity.reshape(-1, 1))\n",
    "    np.save(umap_path, low_dim_embeddings)\n",
    "    print(f\"âœ… UMAP embeddings saved to {umap_path}\")\n",
    "\n",
    "# 6. å®šä¹‰é¢œè‰²æ˜ å°„ï¼ˆæ¸å˜è‰²ï¼‰\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# é¢œè‰²èŒƒå›´ï¼šä» (100, 125, 125) æ·±è‰² â†’ (160, 200, 180) äº®è‰²\n",
    "cmap = mcolors.LinearSegmentedColormap.from_list(\"custom_cmap\", [\n",
    "    (100/255, 125/255, 125/255),  # ä½ç›¸ä¼¼åº¦\n",
    "    (160/255, 200/255, 180/255)   # é«˜ç›¸ä¼¼åº¦\n",
    "])\n",
    "\n",
    "# 7. ç»˜åˆ¶å¯è§†åŒ–\n",
    "plt.figure(figsize=(10, 6))\n",
    "sc = plt.scatter(\n",
    "    low_dim_embeddings[:, 0], \n",
    "    low_dim_embeddings[:, 1], \n",
    "    c=sequence_identity,  # ç”¨ Sequence Identity ä½œä¸ºé¢œè‰²\n",
    "    cmap=cmap,  # æ¸å˜è‰²\n",
    "    alpha=0.7,  \n",
    "    s=30,  \n",
    "    edgecolors=\"black\"\n",
    ")\n",
    "\n",
    "# 8. æ·»åŠ é¢œè‰²æ¡ï¼ˆColorbarï¼‰\n",
    "cbar = plt.colorbar(sc)\n",
    "cbar.set_label(\"% Sequence Identity to Training\")\n",
    "\n",
    "plt.title(\"UMAP Visualization Based on Similarity to Original Data\")\n",
    "plt.xlabel(\"Dimension 1\")\n",
    "plt.ylabel(\"Dimension 2\")\n",
    "\n",
    "# **ä¿å­˜å›¾ç‰‡**\n",
    "img_path = \"outputs/emb_data/embedding_umap_similarity.png\"\n",
    "plt.savefig(img_path, dpi=300)\n",
    "print(f\"ğŸ“¸ Visualization saved as {img_path}\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def is_empty_block(block: dict, keys: list):\n",
    "    \"\"\"åˆ¤æ–­æŒ‡å®š keys ä¸­çš„å­—æ®µæ˜¯å¦éƒ½ä¸ºç©º\"\"\"\n",
    "    return all(k in block and (not block[k]) for k in keys)\n",
    "\n",
    "# è¯»å–åŸå§‹ JSON æ–‡ä»¶\n",
    "with open('/home/wyp/project/forest/forestllm-main/outputs/0321/æ¬ è´¹ä¸­æ–­çš„ç¬¬ä¸€ç‰ˆqwen_book_output.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# å¦‚æœé¡¶å±‚æ˜¯åˆ—è¡¨ï¼Œå¤„ç†æ¯ä¸ªå…ƒç´ \n",
    "if isinstance(data, list):\n",
    "    for item in data:\n",
    "        if (\n",
    "            is_empty_block(item.get(\"question_setter\", {}), [\"questions\"]) and\n",
    "            is_empty_block(item.get(\"expert_agent\", {}), [\"refined_questions\"]) and\n",
    "            is_empty_block(item.get(\"virtual_teacher\", {}), [\"processed_results\"])\n",
    "        ):\n",
    "            item.pop(\"question_setter\", None)\n",
    "            item.pop(\"expert_agent\", None)\n",
    "            item.pop(\"virtual_teacher\", None)\n",
    "else:\n",
    "    # å•ä¸ªå¯¹è±¡ï¼ˆéåˆ—è¡¨ï¼‰æƒ…å†µ\n",
    "    if (\n",
    "        is_empty_block(data.get(\"question_setter\", {}), [\"questions\"]) and\n",
    "        is_empty_block(data.get(\"expert_agent\", {}), [\"refined_questions\"]) and\n",
    "        is_empty_block(data.get(\"virtual_teacher\", {}), [\"processed_results\"])\n",
    "    ):\n",
    "        data.pop(\"question_setter\", None)\n",
    "        data.pop(\"expert_agent\", None)\n",
    "        data.pop(\"virtual_teacher\", None)\n",
    "\n",
    "# å†™å…¥å¤„ç†ç»“æœ\n",
    "with open('qwen_book_output.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(\"å¤„ç†å®Œæˆï¼šå·²æ¸…ç†æ‰€æœ‰å®Œå…¨ä¸ºç©ºçš„ä¸‰å…ƒå­—æ®µã€‚\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"outputs/0321/qwen_book_output.json\"\n",
    "output_path = \"outputs/0321/qwen_book_output.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile:\n",
    "    data = json.load(infile)  # åŠ è½½æ•´ä¸ª JSON æ•°ç»„\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as outfile:\n",
    "    for item in data:\n",
    "        outfile.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"å·²æˆåŠŸå°† {len(data)} æ¡æ•°æ®ä» JSON è½¬æ¢ä¸º JSONL æ ¼å¼ï¼š{output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
