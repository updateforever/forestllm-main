import os
import sys
import json
import torch
import argparse
import logging
import re
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from utils.global_methods import run_agent  # æ›¿æ¢ä¸ºä½ çš„æ¨ç†å‡½æ•°
from data.dataset import get_dataloader

# === æ—¥å¿—é…ç½® ===
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")
logger = logging.getLogger(__name__)

def extract_answer_from_tags(output_text):
    """
    æå– <think> å’Œ <answer> å†…å®¹ã€‚
    - æœ‰ <think> æå–æ€è€ƒè¿‡ç¨‹ã€‚
    - æœ‰ <answer> æå–ç­”æ¡ˆã€‚
    - æ²¡æœ‰ <answer>ï¼Œä½†æœ‰ <think>ï¼Œæå– think åçš„å‰©ä½™æ–‡æœ¬ä½œä¸ºç­”æ¡ˆã€‚
    - ä¸¤è€…éƒ½æ²¡æœ‰æ—¶ï¼ŒæŠŠæ•´ä¸ªæ–‡æœ¬ä½œä¸ºç­”æ¡ˆã€‚
    """
    think_match = re.search(r"<think>(.*?)</think>", output_text, re.DOTALL)
    answer_match = re.search(r"<answer>(.*?)</answer>", output_text, re.DOTALL)

    thought_process = think_match.group(1).strip() if think_match else ""

    if answer_match:
        answer_candidate = answer_match.group(1).strip()
    elif think_match:
        after_think = output_text[think_match.end():].strip()
        answer_candidate = after_think
    else:
        answer_candidate = output_text.strip()

    return thought_process, answer_candidate


def extract_first_option(text):
    """ä¼˜å…ˆæå– \boxed{}ã€<answer>ã€"answer": "X"ã€å† fallback çº¯å­—æ¯"""
    text_upper = text.strip()
<<<<<<< HEAD
    for pattern in [ 
        r"\\boxed\{([ABCD])\}",                # åŒ¹é… \boxed{A}
        r"<answer>\s*([ABCD])\s*</answer>",    # åŒ¹é… <answer>A</answer>
        r'"answer"\s*:\s*"([ABCD])"',          # åŒ¹é… "answer": "A"
        r'"ç­”æ¡ˆ"\s*:\s*"([ABCD])"',            # âœ… æ–°å¢ï¼ŒåŒ¹é… "ç­”æ¡ˆ": "A" è¿™ç§ä¸­æ–‡çš„
        r"\b([ABCD])[\s\).ï¼Œã€ã€‚]?"             # fallbackï¼Œç›´æ¥æ‰¾å•å­—æ¯
=======
    for pattern in [
        r"\\boxed\{([ABCD])\}",           # åŒ¹é… \boxed{A}
        r"<answer>\s*([ABCD])\s*</answer>", # åŒ¹é… <answer>A</answer>
        r'"answer"\s*:\s*"([ABCD])"',       # åŒ¹é… "answer": "A"
        r"\b([ABCD])[\s\).ï¼Œã€ã€‚]?"          # fallbackï¼Œç›´æ¥æ‰¾å•å­—æ¯
>>>>>>> 9ce0f14f46524713196e7f72c74d8dd781d1007f
    ]:
        match = re.search(pattern, text_upper)
        if match:
            return match.group(1)
    return None


def is_valid_option(answer):
    return answer.strip() in ["A", "B", "C", "D"] or extract_first_option(answer) is not None

def infer_answer_with_gpt4(model_output):
    """å¦‚æœç­”æ¡ˆä¸æ˜¯æœ‰æ•ˆé€‰é¡¹ï¼Œè°ƒç”¨ GPT-4 è¿›è¡Œæ¨ç†"""
    logger.info(f"è°ƒç”¨ GPT-4 è¿›è¡Œç­”æ¡ˆä¿®æ­£: {model_output}")
    prompt = f"""
    ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„è¯„åˆ†å‘˜ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æ¨¡å‹è¾“å‡ºï¼Œåˆ¤æ–­æ¨¡å‹æœ€æœ‰å¯èƒ½é€‰æ‹©çš„é€‰é¡¹ï¼ˆA/B/C/Dï¼‰ã€‚
    - **æ¨¡å‹åŸå§‹è¾“å‡º**: {model_output}
    
    é¢˜ç›®ä¸ºå•é€‰é¢˜ï¼Œè¯·ç›´æ¥è¾“å‡ºæœ€å¯èƒ½çš„é€‰é¡¹ï¼ˆA/B/C/Dï¼‰ï¼Œä¸éœ€è¦è§£é‡Šã€‚
    """
    try:
        gpt4_answer = run_agent(prompt, model="qwen")
        if is_valid_option(gpt4_answer):
            return gpt4_answer
    except Exception as e:
        logger.error(f"GPT-4 æ¨ç†å¤±è´¥: {e}")
    
    return "æœªçŸ¥"  # GPT-4 å¤±è´¥æ—¶è¿”å› "æœªçŸ¥"

def generate_text(model, tokenizer, input_texts, batch_msg, max_new_tokens=8192, temperature=0.6):
    """ç”Ÿæˆå¹¶è§£æ batch ç»“æœ"""
    # inputs1 = tokenizer(input_texts, padding=True, truncation=True, max_length=2048, return_tensors="pt").to(model.device)

    rendered_texts = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=True, enable_thinking=False) for m in batch_msg]
    inputs = tokenizer(rendered_texts, padding=True, return_tensors="pt").to(model.device)
    
    input_len = inputs["input_ids"].shape[1]
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,   
            max_new_tokens=max_new_tokens,
            temperature=temperature,
            do_sample=True
        )
    outputs = tokenizer.batch_decode(output_ids[:, input_len:], skip_special_tokens=True)
    outputs = [o.strip() for o in outputs]
    return outputs

def load_model(model_path):
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True, padding_side='left')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    model = AutoModelForCausalLM.from_pretrained(
        model_path, device_map="auto", torch_dtype="auto", trust_remote_code=True
    ).eval()
    return model, tokenizer

def load_existing_ids(save_file):
    if not os.path.exists(save_file): return set()
    with open(save_file, 'r', encoding='utf-8') as f:
        return set(int(json.loads(line)["id"]) for line in f if line.strip())

def evaluate_and_save(model, tokenizer, dataloader, total_batches, output_path, batch_size=8, temperature=0.3):
    """
    è¯„ä¼°å¹¶ä¿å­˜ç»“æœåˆ° JSONLï¼Œæ”¯æŒ batch æ¨ç†ã€æ–­ç‚¹æ¢å¤ã€‚
    """
    save_file = os.path.join(output_path, "results.jsonl")
    os.makedirs(output_path, exist_ok=True)

    done_ids = load_existing_ids(save_file)
    logger.info(f"ğŸ”¹ å·²å®Œæˆ {len(done_ids)} æ¡ï¼Œå°†è·³è¿‡è¿™äº›æ ·æœ¬...")

    with open(save_file, "a", encoding="utf-8") as fout, tqdm(desc="æ¨ç†ä¸­", unit="sample") as pbar:
        for batch in dataloader:
            prompts = [item["prompt"] for item in batch]
            messages = [item["messages"] for item in batch]
            answers = [item["answer"] for item in batch]
            ids = [item["id"] for item in batch]  # â­ å–çœŸå® id

            batch_prompts = []
            batch_msg = []
            batch_infos = []

            for prompt, msg, ref, id_ in zip(prompts, messages, answers, ids):
                if id_ in done_ids:
                    continue
                batch_prompts.append(prompt)
                batch_msg.append(msg)
                batch_infos.append((id_, prompt, msg, ref))

            if not batch_prompts:
                continue  # è¿™ä¸ª batch å…¨è·³è¿‡äº†

            outputs = generate_text(model, tokenizer, batch_prompts, batch_msg, temperature=temperature)

            for (id_, prompt, msg, ref), output in zip(batch_infos, outputs):
                thought, candidate = extract_answer_from_tags(output)
                candidate = candidate.strip()

                if candidate in ["A", "B", "C", "D"]:
                    pred = candidate
                else:
                    extracted = extract_first_option(candidate)
                    if extracted:
                        pred = extracted
                    else:
                        pred = infer_answer_with_gpt4(candidate)

                record = {
                    "id": id_,
                    "prompt": prompt,
                    "message": msg,
                    "reference": ref,
                    "raw_output": output,
                    "predicted": pred,
                    "correct": pred == ref,
                }
                fout.write(json.dumps(record, ensure_ascii=False) + "\n")
                fout.flush()
                pbar.update(1)



def main():
    parser = argparse.ArgumentParser()
<<<<<<< HEAD
    parser.add_argument("--model_path", type=str, default="/mnt/sda/wyp/models/Qwen3-8B", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_file", type=str, default="/mnt/sda/wyp/forestllm-main/forest_eval/forest_book_mcq_1k3.csv", help="è¾“å…¥æ•°æ®æ–‡ä»¶")
=======
    parser.add_argument("--model_path", type=str, default="/mnt/sda/wyp/models/DeepSeek-R1-Distill-Qwen-7B", help="æœ¬åœ°æ¨¡å‹è·¯å¾„")
    parser.add_argument("--input_file", type=str, default="/mnt/sda/wyp/forestllm-main/outputs/compare_subsets/books_mcq_1k5_v1.csv", help="è¾“å…¥æ•°æ®æ–‡ä»¶")
>>>>>>> 9ce0f14f46524713196e7f72c74d8dd781d1007f
    parser.add_argument("--output_dir", type=str, default="/mnt/sda/wyp/forestllm-main/outputs/eval/", help="è¯„ä¼°ç»“æœå­˜å‚¨ç›®å½•")
    parser.add_argument("--task_type", type=str, choices=["mcq", "qa"], default="mcq", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--evaluation_method", type=str, choices=["metrics", "gpt4", "manual"], default="metrics", help="è¯„ä¼°æ–¹å¼")
    parser.add_argument("--batch_size", type=int, default=4, help="æ‰¹é‡æ¨ç†å¤§å°")
<<<<<<< HEAD
    parser.add_argument("--max_new_tokens", type=int, default=8192, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.6, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--model_mode", type=str, default='normal', help="æ¨¡æ¿é€‰æ‹©")
=======
    parser.add_argument("--max_new_tokens", type=int, default=4096, help="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    parser.add_argument("--temperature", type=float, default=0.6, help="ç”Ÿæˆæ¸©åº¦")
    parser.add_argument("--model_mode", type=str, default='cot', help="æ¨¡æ¿é€‰æ‹©")
>>>>>>> 9ce0f14f46524713196e7f72c74d8dd781d1007f

    args = parser.parse_args()
    # /mnt/sda/wyp/forestllm-main/forest_eval/forest_zero_shot.csv
    # /mnt/sda/wyp/forestllm-main/forest_eval/book/eval_multiple_choice_filtered.csv
    # /mnt/sda/wyp/models/checkpoint-12277
    # /mnt/sda/wyp/models/qwen25
    # /mnt/sda/wyp/models/llama
    # /mnt/sda/wyp/models/Qwen3-8B-Base
<<<<<<< HEAD
    # /mnt/sda/wyp/models/DeepSeek-R1-Distill-Qwen-7B
=======
>>>>>>> 9ce0f14f46524713196e7f72c74d8dd781d1007f
    model, tokenizer = load_model(args.model_path)
    tag = os.path.basename(args.model_path).split("/")[-1]
    eval_split = os.path.splitext(os.path.basename(args.input_file))[0]
    output_path = os.path.join(args.output_dir, tag, eval_split)

    dataloader, total_batches = get_dataloader(
        args.input_file,
        batch_size=args.batch_size,
        task_type=args.task_type,
        model_mode=args.model_mode
    )

    evaluate_and_save(model, tokenizer, dataloader, total_batches, output_path, temperature=args.temperature)

if __name__ == "__main__":
    main()
<<<<<<< HEAD


# CUDA_VISIBLE_DEVICES=3 python eval_new.py --model_path /mnt/sda/wyp/models/qwen3_8b_sft_ep3 --input_file /mnt/sda/wyp/forestllm-main/forest_eval/compare_subsets/forest_zero_shot_v1.csv --temperature 0.6
# CUDA_VISIBLE_DEVICES=2 python eval_new.py --model_path /mnt/sda/wyp/models/Qwen3-8B --input_file /mnt/sda/wyp/forestllm-main/forest_eval/compare_subsets/forest_zero_shot_v1.csv --temperature 0.6
=======
>>>>>>> 9ce0f14f46524713196e7f72c74d8dd781d1007f
