import os
import json
import hashlib
import logging
import argparse
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from agents import (
    QuestionSetter,
    ExpertAgent,
    VirtualTeacherAgent,
    SimulatedLearner,
    GradingTeacher,
)
from utils.toolkit import clean_book_text, filter_web_text


# é…ç½® logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("process.log", mode='w', encoding='utf-8'),  # è¾“å‡ºåˆ°æ–‡ä»¶
        # logging.StreamHandler()  # å¦‚æœä¸éœ€è¦åœ¨ç»ˆç«¯æ˜¾ç¤ºæ—¥å¿—ï¼Œå¯ä»¥æ³¨é‡Šæ‰è¿™è¡Œ
    ]
)


# æ›¿æ¢ logging çš„è¾“å‡ºæ–¹å¼
class TqdmLoggingHandler(logging.Handler):
    def emit(self, record):
        # å°†æ—¥å¿—è®°å½•è¾“å‡ºåˆ° tqdm çš„ write æ–¹æ³•ä¸­
        msg = self.format(record)
        tqdm.write(msg)

# æ·»åŠ è‡ªå®šä¹‰çš„ TqdmLoggingHandler
# logging.getLogger().addHandler(TqdmLoggingHandler())

def process_entry_with_logging(entry, *args):
    """
    åŒ…è£… process_entry å‡½æ•°ï¼Œæ·»åŠ æ—¥å¿—è®°å½•ã€‚
    """
    try:
        text_info = entry["text"][:20]
        logging.info(f"Processing entry: {text_info}")
        process_entry(entry, *args)
        return f"Completed: {text_info}"
    except Exception as e:
        logging.error(f"Error processing entry: {entry['text'][:20]}. Details: {e}")
        return f"Failed: {entry['text'][:20]}"
    

# ç”Ÿæˆå”¯ä¸€ ID
def generate_entry_id(entry):
    entry_str = json.dumps(entry, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(entry_str.encode("utf-8")).hexdigest()


def load_data(data_file):
    """ä»JSONLæ–‡ä»¶ä¸­åŠ è½½æ•°æ®"""
    data = []
    with open(data_file, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line.strip()))
    logging.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®")
    return data


def infer_data_class(data_file):
    """
    ä»æ•°æ®æ–‡ä»¶çš„ç¬¬ä¸€æ¡æ•°æ®ä¸­æ¨æ–­ data_classã€‚
    :param data_file: JSONL æ–‡ä»¶è·¯å¾„
    :return: æ¨æ–­å‡ºçš„ data_class æˆ– 'unknown'
    """
    try:
        with open(data_file, "r", encoding="utf-8") as f:
            first_line = f.readline().strip()
            if not first_line:
                logging.warning("æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ¨æ–­ data_class")
                return "unknown"

            first_entry = json.loads(first_line)
            data_class = first_entry.get("class", "unknown")
            logging.info(f"è‡ªåŠ¨æ¨æ–­åˆ° data_class: {data_class}")
            return data_class
    except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
        logging.error(f"æ¨æ–­ data_class æ—¶å‡ºé”™: {e}")
        return "unknown"


def load_existing_data(out_folder, model_name, data_class):
    """
    åŠ è½½å·²å­˜åœ¨çš„æ¨¡å‹ç”Ÿæˆæ•°æ®ï¼ˆæ”¯æŒ JSON å’Œ JSONL æ ¼å¼ï¼‰
    :param out_folder: è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„
    :param model_name: æ¨¡å‹åç§°
    :param data_class: æ•°æ®ç±»åˆ«ï¼ˆå¦‚ web, article, bookï¼‰
    :return: (out_file, id_set, existing_data)
    """
    out_file = os.path.join(out_folder, f"{model_name}_{data_class}_output.json")

    existing_data = []
    id_set = set()

    if os.path.exists(out_file):
        logging.info(f"æ£€æµ‹åˆ°æ–‡ä»¶: {out_file}ï¼Œå¼€å§‹åŠ è½½å·²ç”Ÿæˆæ•°æ®...")
        try:
            with open(out_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                for entry in data:
                    if "id" in entry:
                        id_set.add(entry["id"])
                        existing_data.append(entry)
        except json.JSONDecodeError:
            logging.error(f"æ–‡ä»¶è§£æå¤±è´¥: {out_file}")
        except Exception as e:
            logging.error(f"åŠ è½½æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {e}")
    else:
        logging.warning(f"æœªæ‰¾åˆ°æ¨¡å‹ {model_name} å¯¹åº”çš„è¾“å‡ºæ–‡ä»¶: {out_file}")

    logging.info(f"æˆåŠŸåŠ è½½ {len(existing_data)} æ¡æ•°æ®ï¼Œæ¥è‡ªæ–‡ä»¶: {out_file}")
    return out_file, id_set, existing_data


# ä¿å­˜æ•°æ®åˆ° JSON æ–‡ä»¶
def save_data(data_file, data):
    with open(data_file, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=4)


# æ£€æŸ¥å½“å‰æ•°æ®æ˜¯å¦å­˜åœ¨
def find_entry_by_id(out_file, entry_id):
    """
    åœ¨å·²ä¿å­˜çš„ JSON æ–‡ä»¶ä¸­æŸ¥æ‰¾æŒ‡å®š ID çš„æ•°æ®æ¡ç›®ï¼ŒåŒ…å«æ­¥éª¤çŠ¶æ€ä¿¡æ¯ã€‚
    :param out_file: ä¿å­˜æ•°æ®çš„ JSON æ–‡ä»¶è·¯å¾„
    :param entry_id: è¦æŸ¥æ‰¾çš„å”¯ä¸€ ID
    :return: æ‰¾åˆ°çš„å®Œæ•´æ•°æ®æ¡ç›®ï¼ˆåŒ…æ‹¬ steps ä¿¡æ¯ï¼‰ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› {"id": entry_id, "steps": {}}
    """
    if not os.path.exists(out_file):
        logging.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {out_file}")
        return {"id": entry_id, "steps": {}}

    try:
        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•åï¼Œåˆ¤æ–­æ˜¯ JSON è¿˜æ˜¯ JSONL æ ¼å¼
        if out_file.endswith(".json"):
            with open(out_file, "r", encoding="utf-8") as f:
                data = json.load(f)
                for entry in data:
                    if entry.get("id") == entry_id:
                        return entry

        elif out_file.endswith(".jsonl"):
            with open(out_file, "r", encoding="utf-8") as f:
                for line in f:
                    try:
                        entry = json.loads(line.strip())
                        if entry.get("id") == entry_id:
                            return entry
                    except json.JSONDecodeError:
                        logging.warning(f"è·³è¿‡æ— æ•ˆçš„ JSON è¡Œ: {line.strip()}")
                        continue

        else:
            logging.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {out_file}")
            return {"id": entry_id, "steps": {}}

    except json.JSONDecodeError as e:
        logging.error(f"JSON è§£æå¤±è´¥: {e}")
    except Exception as e:
        logging.error(f"æŸ¥æ‰¾ ID æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")

    return {"id": entry_id, "steps": {}}


# æ£€æŸ¥å‰ç½®æ•°æ®æ˜¯å¦å­˜åœ¨
def check_required_keys(entry, required_keys):
    return all(key in entry and entry[key] for key in required_keys)


def preprocess_text(entry):
    """
    æ ¹æ®æ•°æ®ç±»å‹ï¼ˆdata_classï¼‰å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
    """
    data_class = entry.get("class", "")
    text = entry.get("text", "")

    if data_class == "article":
        MAX_TEXT_LENGTH = 10000
        text = text[:MAX_TEXT_LENGTH]
        # logging.info(f"æ–‡ç« ç±»æ•°æ®å·²æˆªæ–­åˆ° {MAX_TEXT_LENGTH} å­—ç¬¦")

    elif data_class == "web":
        text = filter_web_text(entry)
        if text is None:
            # logging.info("è·³è¿‡ä½ä»·å€¼ Web æ•°æ®")
            return None

    elif data_class == "book":
        MAX_TEXT_LENGTH = 10000
        text = clean_book_text(text, max_length=MAX_TEXT_LENGTH)
        # logging.info(f"ä¹¦ç±ç±»æ•°æ®å·²æ¸…æ´—å¹¶æˆªæ–­åˆ° {MAX_TEXT_LENGTH} å­—ç¬¦")

    else:
        logging.warning(f"æœªè¯†åˆ«çš„æ•°æ®ç±»å‹: {data_class}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")

    return text


# 1ï¸âƒ£ **Question Setter**
def process_question_setter(entry, question_setter):
    """
    ç›´æ¥å¤„ç† QuestionSetter é€»è¾‘ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param question_setter: QuestionSetter å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    questions = question_setter.generate_response(
        entry["text"], entry.get("class", "")
    )  # ç”Ÿæˆé—®é¢˜

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["question_setter"] = {"questions": questions}
    # logging.info(f"QuestionSetter å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 2ï¸âƒ£ **Expert Agent**
def process_expert_agent(entry, expert_agent):
    """
    ç›´æ¥å¤„ç† ExpertAgent é€»è¾‘ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param expert_agent: ExpertAgent å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    refined_questions = [
        expert_agent.evaluate_and_refine_question(
            entry["text"], q, entry.get("class", "")
        )
        for q in entry["question_setter"]["questions"]
    ]

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["expert_agent"] = {"refined_questions": refined_questions}
    # logging.info(f"ExpertAgent å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 3ï¸âƒ£ **Virtual Teacher**
def process_virtual_teacher(entry, virtual_teacher):
    """
    ç›´æ¥å¤„ç† VirtualTeacher é€»è¾‘ï¼Œç¡®ä¿æ•°æ®é¡ºåºä¸€è‡´ï¼Œæ— éœ€é¢å¤–ä¿å­˜ç´¢å¼•ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param virtual_teacher: VirtualTeacher å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    processed_results = []

    # è·å– question_setter å’Œ refined_questions
    questions = entry["question_setter"]["questions"]
    refined_questions = entry.get("expert_agent", {}).get("refined_questions", [])

    # éå†è¯•é¢˜ï¼Œæ ¹æ®ç´¢å¼•å¤„ç†ä¼˜åŒ–è¯•é¢˜æˆ–åŸå§‹è¯•é¢˜
    for index, question_data in enumerate(questions):
        # åˆ¤æ–­æ˜¯å¦æœ‰ä¼˜åŒ–åçš„è¯•é¢˜
        if index < len(refined_questions) and refined_questions[index].get(
            "requires_refinement", False
        ):
            current_question = refined_questions[index]["refined_response"]
        else:
            current_question = question_data["response"]

        # åˆ¤æ–­é¢˜å‹ï¼Œæ‰§è¡Œç›¸åº”å¤„ç†
        if question_data.get("question_type") == "multiple_choice":
            conversational_form = virtual_teacher.convert_to_conversational_form(
                entry["text"], current_question, entry.get("class", "")
            )
        else:
            conversational_form = ""

        # ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰
        cot = virtual_teacher.generate_thinking_chain(
            entry["text"], current_question, entry.get("class", "")
        )

        # ä¿å­˜ç»“æœ
        processed_results.append(
            {"conversational_form": conversational_form, "CoT": cot}
        )

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["virtual_teacher"] = {"processed_results": processed_results}
    # logging.info(f"VirtualTeacher å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 4ï¸âƒ£ **Simulated Learner**
def process_learner(entry, learner):
    """
    ç›´æ¥å¤„ç† SimulatedLearner é€»è¾‘ï¼Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–è¯•é¢˜æˆ–åŸå§‹è¯•é¢˜ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param learner: SimulatedLearner å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    learner_answers = []

    # è·å– question_setter å’Œ refined_questions
    questions = entry["question_setter"]["questions"]
    refined_questions = entry.get("expert_agent", {}).get("refined_questions", [])

    # éå†è¯•é¢˜ï¼Œæ ¹æ®ç´¢å¼•å¤„ç†ä¼˜åŒ–è¯•é¢˜æˆ–åŸå§‹è¯•é¢˜
    for index, question_data in enumerate(questions):
        # åˆ¤æ–­æ˜¯å¦æœ‰ä¼˜åŒ–åçš„è¯•é¢˜
        if index < len(refined_questions) and refined_questions[index].get(
            "requires_refinement", False
        ):
            current_question = refined_questions[index]["refined_response"]
        else:
            current_question = question_data["response"]

        # ç”Ÿæˆç­”æ¡ˆ
        learner_answer = learner.answer_question(current_question)
        learner_answers.append({"answer": learner_answer})

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["simulated_learner"] = {"learner_answers": learner_answers}
    # logging.info(f"SimulatedLearner å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 5ï¸âƒ£ **Grading Teacher**
def process_grader(entry, grader):
    """
    ç›´æ¥å¤„ç† GradingTeacher é€»è¾‘ï¼ŒåŸºäºä¸“å®¶çš„åŸå§‹è¯•é¢˜æˆ–ä¼˜åŒ–è¯•é¢˜ï¼Œä¸å­¦ç”Ÿä½œç­”è¿›è¡Œè¯„ä¼°æ¨ç†ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param grader: GradingTeacher å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    evaluations = []

    # è·å–åŸå§‹è¯•é¢˜å’Œä¼˜åŒ–è¯•é¢˜
    questions = entry["question_setter"]["questions"]
    refined_questions = entry.get("expert_agent", {}).get("refined_questions", [])
    learner_answers = entry["simulated_learner"]["learner_answers"]

    # éå†æ¯ä¸ªè¯•é¢˜
    for index, question_data in enumerate(questions):
        # åˆ¤æ–­æ˜¯å¦æœ‰ä¼˜åŒ–åçš„è¯•é¢˜
        if index < len(refined_questions) and refined_questions[index].get(
            "requires_refinement", False
        ):
            current_question = refined_questions[index]["refined_response"]
        else:
            current_question = question_data["response"]

        # è·å–å¯¹åº”çš„å­¦ç”Ÿä½œç­”
        learner_answer = learner_answers[index]["answer"]

        # ç¡®ä¿æœ‰å­¦ç”Ÿä½œç­”åå†è¯„ä¼°
        if learner_answer:
            evaluation = grader.evaluate_answer(
                entry["text"], current_question, learner_answer, entry.get("class", "")
            )
            evaluations.append({"evaluation": evaluation})
        else:
            # å¦‚æœæ²¡æœ‰å¯¹åº”å­¦ç”Ÿä½œç­”ï¼Œè®°å½•ç©ºè¯„ä¼°
            evaluations.append({"evaluation": None})

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["grading_teacher"] = {"evaluations": evaluations}
    # logging.info(f"GradingTeacher å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# **Entry Processing Pipeline**
def process_entry(
    entry,
    out_file,
    question_setter,
    expert_agent,
    virtual_teacher,
    learner,
    grader,
    step,
    data_class,
):
    """
    å¤„ç†å•ä¸ªæ•°æ®æ¡ç›®ï¼ŒåŒ…æ‹¬æ‰€æœ‰é˜¶æ®µï¼Œæ”¯æŒé˜¶æ®µæ€§æ‰§è¡Œï¼Œå¹¶åŠ¨æ€è¡¥å…¨å‰ç½®æ­¥éª¤ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param out_file: ä¿å­˜çš„ JSON æ–‡ä»¶è·¯å¾„
    :param question_setter: QuestionSetter å®ä¾‹
    :param expert_agent: ExpertAgent å®ä¾‹
    :param virtual_teacher: VirtualTeacher å®ä¾‹
    :param learner: SimulatedLearner å®ä¾‹
    :param grader: GradingTeacher å®ä¾‹
    :param step: å½“å‰æ‰§è¡Œçš„æ­¥éª¤ (1-5)
    :data_class: æ•°æ®ç±»å‹ web article book
    """

    # å¦‚æœ entry ä¸­æ²¡æœ‰ IDï¼Œåˆ™ç”Ÿæˆå¹¶å­˜å‚¨
    if "id" not in entry:
        entry["id"] = generate_entry_id(entry)
    entry_id = entry["id"]  # ç›´æ¥ä» entry ä¸­è·å– ID

    # å¦‚æœ entry ä¸­æ²¡æœ‰ data_classï¼Œåˆ™å­˜å‚¨
    if "class" not in entry:
        entry["class"] = data_class

    # æ–‡æœ¬é¢„å¤„ç†  ğŸ“ŒğŸ“ŒğŸ“Œ  text åœ¨æ­¤å¤„è¢«æ”¹å˜
    text_info = entry["text"][:20]
    entry["text"] = preprocess_text(entry)
    if entry["text"] is None:
        logging.info(f"æ•°æ®ç”±äºä½ä»·å€¼æˆ–æ— æ•ˆè€Œè¢«è·³è¿‡")
        return

    # 1ï¸âƒ£ **åŠ è½½æ•°æ®çŠ¶æ€**
    existing_entry = find_entry_by_id(out_file, entry_id)
    if not existing_entry:
        existing_entry = {"id": entry_id, "steps": {}}

    # åˆå¹¶ entry æ•°æ®åˆ° existing_entry
    existing_entry.update(entry)

    steps = existing_entry.get("steps", {})

    # 2ï¸âƒ£ **å®šä¹‰æ­¥éª¤ä¾èµ–å…³ç³»**
    step_dependencies = {1: [], 2: [1], 3: [1, 2], 4: [1, 2], 5: [1, 2, 3, 4]}

    # 3ï¸âƒ£ **é€æ­¥æ£€æŸ¥å’Œæ‰§è¡Œæ­¥éª¤**
    for required_step in step_dependencies[step]:
        if str(required_step) not in steps or steps[str(required_step)] != "completed":
            if required_step == 1:
                existing_entry = process_question_setter(
                    existing_entry, question_setter
                )
                steps["1"] = "completed"
                logging.info(f"Step 1: QuestionSetter è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

            if required_step == 2:
                existing_entry = process_expert_agent(existing_entry, expert_agent)
                steps["2"] = "completed"
                logging.info(f"Step 2: ExpertAgent è‡ªåŠ¨è¡¥å…¨å®Œæˆ")
            
            if required_step == 3:
                existing_entry = process_virtual_teacher(existing_entry, virtual_teacher)
                steps["3"] = "completed"
                logging.info(f"Step 3: VirtualTeacher è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

            if required_step == 4:
                existing_entry = process_learner(existing_entry, learner)
                steps["4"] = "completed"
                logging.info(f"Step 4: SimulatedLearner è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

    # 4ï¸âƒ£ **æ‰§è¡Œç›®æ ‡æ­¥éª¤**
    if str(step) not in steps or steps[str(step)] != "completed":
        if step == 1:
            existing_entry = process_question_setter(existing_entry, question_setter)
        elif step == 2:
            existing_entry = process_expert_agent(existing_entry, expert_agent)
        elif step == 3:
            existing_entry = process_virtual_teacher(existing_entry, virtual_teacher)
        elif step == 4:
            existing_entry = process_learner(existing_entry, learner)
        elif step == 5:
            existing_entry = process_grader(existing_entry, grader)

        steps[str(step)] = "completed"
        logging.info(f"Step {step}: å¤„ç†å®Œæˆ: {text_info}")
    else:
        logging.info(f"Step {step}: å·²å­˜åœ¨ï¼Œè·³è¿‡")

    # 5ï¸âƒ£ **ä¿å­˜æ•°æ®çŠ¶æ€**
    existing_entry["steps"] = steps
    if "text" in existing_entry:
        del existing_entry["text"]  # åˆ é™¤ text æ•°æ®ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´

    if os.path.exists(out_file):
        with open(out_file, "r+", encoding="utf-8") as f:
            data = json.load(f)
            data = [e for e in data if e.get("id") != entry_id]
            data.append(existing_entry)
            f.seek(0)
            json.dump(data, f, ensure_ascii=False, indent=4)
    else:
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump([existing_entry], f, ensure_ascii=False, indent=4)

    logging.info(f"æ•°æ®çŠ¶æ€å·²ä¿å­˜")


# ğŸ”§ **å‚æ•°è§£æ**
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--prompt-path",
        help="Promptæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåŠ è½½å‡ºé¢˜äººã€ä¸“å®¶ã€åŸ¹è®­æœºæ„ä¸“å®¶ã€æ¨¡æ‹Ÿè€ƒç”Ÿå’Œè¯„å·è€å¸ˆçš„Prompt",
        default="/home/wyp/project/ForestLLM/prompts",
    )
    parser.add_argument(
        "--data-file",
        help="JSONLæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåŠ è½½åŸå§‹æ•°æ®",
    )
    parser.add_argument(
        "--out-dir",
        help="è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œç”¨äºä¿å­˜ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®é›†",
    )
    parser.add_argument(
        "--data_class",
        default="web",
        help="æ•°æ®ç±»åˆ«ï¼ˆå¦‚ web, article, bookï¼‰",
    )
    parser.add_argument(
        "--model",
        default="qwen",
        choices=[
            "chatgpt_o1-preview",
            "gpt-4",
            "chatgpt",
            "claude",
            "gemini",
            "qwen",
            "gpt-3.5-turbo",
        ],
        type=str,
    )
    parser.add_argument("--batch-size", default=1, type=int)
    parser.add_argument(
        "--step",
        type=int,
        choices=[1, 2, 3, 4, 5],
        required=True,
        help="é€‰æ‹©è¦æ‰§è¡Œçš„é˜¶æ®µ: 1=QuestionSetter, 2=ExpertAgent, 3=VirtualTeacher, 4=SimulatedLearner, 5=GradingTeacher",
    )
    return parser.parse_args()


# ğŸ“ **ä¸»å‡½æ•°**
def main():
    args = parse_args()
    data_file = args.data_file
    out_folder = args.out_dir

    # åŠ¨æ€ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
    out_file = os.path.join(out_folder, f"{args.model}_{args.data_class}_output.json")

    logging.info(f"ä½¿ç”¨æ¨¡å‹: {args.model}")
    logging.info(f"æ•°æ®æ–‡ä»¶: {data_file}")
    logging.info(f"è¾“å‡ºè·¯å¾„: {out_file}")
    logging.info(f"å½“å‰æ‰§è¡Œé˜¶æ®µ: {args.step}")

    # è‡ªåŠ¨æ¨æ–­ data_class
    data_class = infer_data_class(data_file)
    if data_class == "unknown":
        logging.error("æ— æ³•æ¨æ–­ data_classï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
        return

    # åŠ è½½å·²å­˜åœ¨çš„æ•°æ®
    out_file, existing_ids, existing_data = load_existing_data(
        out_folder, args.model, data_class
    )

    # åŠ è½½åŸå§‹æ•°æ®
    data = load_data(data_file)

    # åˆå§‹åŒ– Agents
    question_setter = QuestionSetter(model=args.model)
    expert_agent = ExpertAgent(model=args.model)
    virtual_teacher = VirtualTeacherAgent(model=args.model)
    if args.step >= 4:
        learner = SimulatedLearner(
            model_api=list(args.model),
            model_paths=[
                "/home/wyp/project/swift/models/qwen25_7b_ins",
                "/home/wyp/project/swift/models/minicpm3-4b",
                "/home/wyp/project/swift/models/llama_3_1_8b_ins",
            ],
            model_platforms=["modelscope", "modelscope", "modelscope"],
        )
    else:
        learner = SimulatedLearner(
            model_api=list(args.model),
        )
    grader = GradingTeacher(model="gpt-4")

     # ä½¿ç”¨ ThreadPoolExecutor å¹¶å‘å¤„ç†æ•°æ®
    with ThreadPoolExecutor(max_workers=24) as executor:  # è®¾ç½®æœ€å¤§çº¿ç¨‹æ•°ï¼Œå¯æ ¹æ®ç¡¬ä»¶é…ç½®è°ƒæ•´
        futures = {
            executor.submit(
                process_entry_with_logging,
                entry,
                out_file,
                question_setter,
                expert_agent,
                virtual_teacher,
                learner,
                grader,
                args.step,
                data_class,
            ): entry
            for entry in data
        }

        # ä½¿ç”¨ tqdm ç›‘æ§è¿›åº¦æ¡
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing Entries", unit="entry"):
            try:
                result = future.result()
                logging.info(result)
            except Exception as e:
                logging.error(f"Error in future result: {e}")

    # for entry in tqdm(data, desc="Processing Entries", unit="entry"):
    #     try:
    #         text_info = entry["text"][:20]
    #         logging.info(f"Processing entry: {text_info}")
    #         process_entry(
    #             entry,
    #             out_file,
    #             question_setter,
    #             expert_agent,
    #             virtual_teacher,
    #             learner,
    #             grader,
    #             args.step,
    #             data_class,
    #         )
    #     except Exception as e:
    #         logging.error(f"Error processing entry: {text_info}, skipping. Details: {e}")

    # # é€æ¡å¤„ç†æ•°æ®ï¼Œæ ¹æ® step æ§åˆ¶æ‰§è¡Œé˜¶æ®µ
    # for entry in data:
    #     text_info = entry["text"][:20]
    #     logging.info(f"Processing entry: {text_info}")
    #     process_entry(
    #         entry,
    #         out_file,
    #         question_setter,
    #         expert_agent,
    #         virtual_teacher,
    #         learner,
    #         grader,
    #         args.step,
    #         data_class,
    #     )


    logging.info(f"æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ° {out_file}")





if __name__ == "__main__":
    main()
