import json
import hashlib
import numpy as np
import pandas as pd
from tqdm import tqdm
import torch
import argparse
from transformers import AutoTokenizer, AutoModel
from accelerate import infer_auto_device_map

# ç”Ÿæˆå”¯ä¸€ ID
def generate_entry_id(entry):
    """é€šè¿‡ JSON åºåˆ—åŒ–æ–¹å¼ç”Ÿæˆå”¯ä¸€ ID"""
    entry_str = json.dumps(entry, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(entry_str.encode("utf-8")).hexdigest()

def load_model(model_path):
    print(f"ğŸ”„ Loading model from {model_path} ...")

    # **è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç±»å‹**
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    
    # **é¿å… padding é”™è¯¯ï¼ˆLLaMA æ²¡æœ‰ pad_tokenï¼‰**
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    # **å¼ºåˆ¶å³æˆªæ–­**
    tokenizer.truncation_side = "right"

    # **å°è¯•ç”¨ `device_map="auto"` è‡ªåŠ¨åˆ†é…**
    try:
        model = AutoModel.from_pretrained(model_path, device_map="auto")
        device = next(model.parameters()).device
        print("Model loaded with `device_map='auto'` (multi-GPU enabled)")
    except TypeError:
        # **å¦‚æœ `auto` å¤±è´¥ï¼ˆä¸€èˆ¬æ˜¯ BERTï¼‰ï¼Œåˆ™æ‰‹åŠ¨ `to(device)`**
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = AutoModel.from_pretrained(model_path).to(device)
        print("Model loaded and moved to device manually (BERT / smaller models)")

    return tokenizer, model, device

# è¯»å–æ•°æ®ï¼ˆæ”¯æŒåŸå§‹æ•°æ® & ç”Ÿæˆæ•°æ®ï¼‰
def load_data(mode, original_file, generated_file, generated_ids=None):
    """ æ ¹æ®æ¨¡å¼ ('original' or 'generated') è¯»å–æ•°æ® """
    texts, ids, knowledge_texts = [], [], []

    if mode == "original":
        print(f"ğŸ“‚ Loading original dataset from {original_file} ...")
        with open(original_file, "r", encoding="utf-8") as f:
            for line in tqdm(f, desc="Processing Original JSONL"):
                try:
                    data = json.loads(line)
                    if "text" in data:
                        text = data["text"]
                        unique_id = generate_entry_id(data)
                        # # **âœ… æ–°å¢ï¼šID è¿‡æ»¤**
                        # if generated_ids and unique_id not in generated_ids:
                        #     continue  # è·³è¿‡æ— å…³æ•°æ®
                        texts.append(text)
                        ids.append(unique_id)
                except json.JSONDecodeError:
                    print("âš ï¸ Skipping invalid JSON line.")
    
    elif mode == "generated":
        print(f"ğŸ“‚ Loading generated dataset from {generated_file} ...")
        with open(generated_file, "r", encoding="utf-8") as f:
            generated_data = json.load(f)
        for item in tqdm(generated_data, desc="Processing Generated JSON"):
            texts.append(item['question_setter']["response"])
            ids.append(item["id"])
            knowledge_texts.append(item["question_setter"].get("knowledge", ""))  # å¯èƒ½æ²¡æœ‰ knowledge

    print(f"âœ… Total {len(texts)} documents loaded for '{mode}' dataset.")
    return texts, ids, knowledge_texts

# è®¡ç®—æ–‡æœ¬åµŒå…¥
def get_embedding(texts, tokenizer, model, device, batch_size=2, max_length=2048):
    """ ä½¿ç”¨ LLaMA è®¡ç®—æ–‡æœ¬åµŒå…¥ """
    embeddings = []
    for i in tqdm(range(0, len(texts), batch_size), desc="ğŸ” Generating embeddings"):
        batch_texts = texts[i:i + batch_size]
        
        # å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç 
        encoded_input = tokenizer(batch_texts, padding=True, truncation=True, return_tensors="pt", max_length=max_length)
        encoded_input = {key: value.to(device) for key, value in encoded_input.items()}  # ä¼ è¾“åˆ° GPU
        
        # è·å–æ¨¡å‹è¾“å‡º
        with torch.no_grad():
            model_output = model(**encoded_input)
        
        # å–æœ€åä¸€å±‚ [CLS] token å‘é‡ä½œä¸ºåµŒå…¥
        sentence_embeddings = model_output.last_hidden_state[:, 0, :].cpu().numpy()
        embeddings.append(sentence_embeddings)

    return np.vstack(embeddings)

# ä¿å­˜åµŒå…¥åˆ°æ–‡ä»¶
def save_embeddings(embeddings, ids, mode, output_dir, prefix="response"):
    """ å­˜å‚¨ NumPy & CSV æ ¼å¼çš„åµŒå…¥æ•°æ® """
    npy_path = f"{output_dir}/llama_embeddings_{mode}_{prefix}.npy"
    csv_path = f"{output_dir}/llama_embeddings_{mode}_{prefix}.csv"

    np.save(npy_path, embeddings)
    print(f"ğŸ’¾ Embeddings saved to {npy_path}")

    df = pd.DataFrame(embeddings)
    df.insert(0, "id", ids)
    df.to_csv(csv_path, index=False, encoding="utf-8")
    print(f"ğŸ“œ CSV file saved to {csv_path}")

# ä¸»å‡½æ•°
def main():
    """ ä¸»å…¥å£å‡½æ•°ï¼Œæ”¯æŒåŠ¨æ€è·¯å¾„ """
    parser = argparse.ArgumentParser(description="Extract embeddings using LLaMA & BERT")
    
    # å¯é…ç½®çš„è·¯å¾„
    parser.add_argument("--model_path", type=str, required=True,
                        help="Path to the LLaMA/BERT model directory")
    parser.add_argument("--output_dir", type=str, required=True,
                        help="Directory to save output files")
    parser.add_argument("--original_file", type=str, required=True,
                        help="Path to the original dataset JSONL file")
    parser.add_argument("--generated_file", type=str, required=True,
                        help="Path to the generated dataset JSON file")
    parser.add_argument("--max_length", type=int, default=None, 
                        help="Max sequence length (auto-detected)")
    parser.add_argument("--batch_size", type=int, default=2, 
                        help="batch size")
    # è¿è¡Œæ¨¡å¼
    parser.add_argument("--mode", type=str, choices=["original", "generated", "both"], default="both",
                        help="Choose which dataset to process: 'original', 'generated', or 'both'")

    args = parser.parse_args()

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    import os
    os.makedirs(args.output_dir, exist_ok=True)

    # åŠ è½½æ¨¡å‹
    tokenizer, model, device = load_model(args.model_path)

    # **è‡ªåŠ¨è°ƒæ•´ max_length**
    if args.max_length is None:
        model_type = model.config.model_type
        if "bert" in model_type or "roberta" in model_type:
            args.max_length = 512  # BERT & RoBERTa æ¨¡å‹é»˜è®¤ 512
        elif "llama" in model_type or "gpt" in model_type:
            args.max_length = 2048  # LLaMA / GPT é»˜è®¤ 2048
        else:
            args.max_length = 1024  # å…¶ä»–æ¨¡å‹é»˜è®¤ 1024
    print(f"âš™ï¸ Using max_length={args.max_length} for model type: {model_type}")

    # å¤„ç†åŸå§‹æ•°æ®
    if args.mode in ["original", "both"]:
        # è¯»å–ç”Ÿæˆæ•°æ®çš„ ID
        print("ğŸ” Extracting IDs from generated data...")
        with open(args.generated_file, "r", encoding="utf-8") as f:
            generated_data = json.load(f)
        generated_ids = {item["id"] for item in generated_data}
        print(f"âœ… Found {len(generated_ids)} IDs in generated data.")

        # å¤„ç† `text` å­—æ®µ
        texts, ids, _ = load_data("original", args.original_file, args.generated_file, generated_ids)
        embeddings = get_embedding(texts, tokenizer, model, device, args.batch_size, args.max_length)
        save_embeddings(embeddings, ids, "original", args.output_dir, "text")

    # å¤„ç†ç”Ÿæˆæ•°æ®
    if args.mode in ["generated", "both"]:
        texts, ids, knowledge_texts = load_data("generated", args.original_file, args.generated_file)

        # å¤„ç† `response` å­—æ®µ
        embeddings = get_embedding(texts, tokenizer, model, device, args.batch_size, args.max_length)
        save_embeddings(embeddings, ids, "generated", args.output_dir, "response")

        # å¤„ç† `knowledge` å­—æ®µ
        knowledge_embeddings = get_embedding(knowledge_texts, tokenizer, model, device, args.batch_size, args.max_length)
        save_embeddings(knowledge_embeddings, ids, "generated", args.output_dir, "knowledge")

    print("âœ… All processing complete!")


# è¿è¡Œå…¥å£
if __name__ == "__main__":
    main()


'''
python get_emb_llm.py \
    --model_path "/home/wyp/project/swift/models/llama_3_1_8b_ins/" \
    --output_dir "outputs/emb_data/" \
    --original_file "mateinfo/merged_org_data.jsonl" \
    --generated_file "sft_output_all_250220.json" \
    --mode both

python tools/get_emb_llm.py \
    --model_path "/home/wyp/project/forest/forestllm-main/models/bert-base-chinese/" \
    --output_dir "outputs/emb_data/bert/" \
    --original_file "mateinfo/merged_org_data.jsonl" \
    --generated_file "outputs/emb_data/sft_output_all_250220.json" \
    --batch_size 32 \
    --mode both    
'''

