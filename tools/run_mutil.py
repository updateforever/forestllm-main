import os
import sys
prj_path = os.path.join(os.path.dirname(__file__), '..')
if prj_path not in sys.path:
    sys.path.append(prj_path)

import json
import hashlib
import logging
import argparse
from queue import Queue, Empty
from threading import Thread, Event
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
from agents import (
    QuestionSetter,
    ExpertAgent,
    VirtualTeacherAgent,
    SimulatedLearner,
    GradingTeacher,
)
from utils.toolkit import clean_book_text, filter_web_text
from datetime import datetime
import time

# è·å–å½“å‰æ—¶é—´ï¼Œæ ¼å¼åŒ–ä¸ºæ–‡ä»¶åå‹å¥½çš„å­—ç¬¦ä¸²
timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
# é…ç½®æ—¥å¿—è®°å½•
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler(
            f"/home/wyp/project/forest/forestllm-main/outputs/logs/process_{timestamp}.log",
            mode="w",
            encoding="utf-8",
        ),
    ],
)


# ===== å¤šçº¿ç¨‹æ“ä½œ ===== #
# ä¿å­˜æ•°æ®çš„çº¿ç¨‹å‡½æ•°
def data_saver(queue: Queue, output_file: str, stop_event: Event, batch_size: int = 5):
    """
    ä»é˜Ÿåˆ—ä¸­è¯»å–æ•°æ®å¹¶æ‰¹é‡ä¿å­˜åˆ° JSONL æ–‡ä»¶ã€‚
    æ–‡ä»¶å†™å…¥é‡‡ç”¨â€œè¯»-æ”¹-å†™â€ç­–ç•¥ï¼Œæ”¯æŒæ ¹æ® ID æ›´æ–° stepsã€‚
    
    :param queue: å­˜å‚¨æ•°æ®çš„çº¿ç¨‹å®‰å…¨é˜Ÿåˆ—
    :param output_file: è¾“å‡º JSONL æ–‡ä»¶è·¯å¾„
    :param stop_event: åœæ­¢ä¿¡å·ï¼Œç”¨äºå®‰å…¨å…³é—­çº¿ç¨‹
    :param batch_size: æ¯æ¬¡å†™å…¥çš„æœ€å°æ•°æ®é‡
    """
    buffer = []

    while not stop_event.is_set() or not queue.empty():
        try:
            data = queue.get(timeout=0.1)
            buffer.append(data)
            queue.task_done()
        except Empty:
            continue

        if len(buffer) >= batch_size:
            _write_to_file(output_file, buffer)
            buffer.clear()

    # ç¨‹åºç»“æŸæ—¶å†™å…¥å‰©ä½™æ•°æ®
    if buffer:
        _write_to_file(output_file, buffer)


def _write_to_file(output_file: str, new_data: list):
    """
    å°†æ•°æ®å†™å…¥ .jsonl æ–‡ä»¶ï¼Œç›´æ¥è¿½åŠ å†™å…¥ï¼Œä¸åšå»é‡ã€‚
    åç»­å¯é€šè¿‡ç‹¬ç«‹è„šæœ¬å»é‡ã€‚
    """
    with open(output_file, "a", encoding="utf-8") as f:
        for entry in new_data:
            f.write(json.dumps(entry, ensure_ascii=False) + "\n")

# ==== tools ==== #
def load_data(data_file):
    """ä»JSONLæ–‡ä»¶ä¸­åŠ è½½æ•°æ®"""
    data = []
    with open(data_file, "r", encoding="utf-8") as f:
        for line in f:
            data.append(json.loads(line.strip()))
    logging.info(f"åŠ è½½äº† {len(data)} æ¡æ•°æ®")
    return data


def infer_data_class(data_file):
    """
    ä»æ•°æ®æ–‡ä»¶çš„ç¬¬ä¸€æ¡æ•°æ®ä¸­æ¨æ–­ data_classã€‚
    :param data_file: JSONL æ–‡ä»¶è·¯å¾„
    :return: æ¨æ–­å‡ºçš„ data_class æˆ– 'unknown'
    """
    try:
        with open(data_file, "r", encoding="utf-8") as f:
            first_line = f.readline().strip()
            if not first_line:
                logging.warning("æ•°æ®æ–‡ä»¶ä¸ºç©ºï¼Œæ— æ³•æ¨æ–­ data_class")
                return "unknown"

            first_entry = json.loads(first_line)
            if "meta_info" not in first_entry:
                data_class = first_entry.get("class", "unknown")
                logging.info(f"è‡ªåŠ¨æ¨æ–­åˆ° data_class: {data_class}")
                return data_class
            else:
                data_class = first_entry.get("meta_info", {}).get(
                    "data_class", "unknown"
                )
                logging.info(f"è‡ªåŠ¨æ¨æ–­åˆ° data_class: {data_class}")
                return data_class
    except (json.JSONDecodeError, FileNotFoundError, KeyError) as e:
        logging.error(f"æ¨æ–­ data_class æ—¶å‡ºé”™: {e}")
        return "unknown"

# åŠ è½½å­˜åœ¨æ•°æ®
def load_existing_data(out_folder, model_name, data_class):
    """
    å…¼å®¹ä» .json/.jsonl æ–‡ä»¶è¯»å–å·²æœ‰æ•°æ®ï¼Œç»Ÿä¸€è¾“å‡ºä¸º jsonlã€‚
    """
    if out_folder.endswith("jsonl"): 
        out_file = out_folder  # ç›´æ¥æ˜¯å®Œæ•´çš„ jsonl æ–‡ä»¶è·¯å¾„
    else:
        out_file = os.path.join(out_folder, f"{model_name}_{data_class}_output.jsonl")  # è‡ªåŠ¨æ‹¼æ¥å‘½å

    id_set = set()
    existing_data = []

    if os.path.exists(out_file):
        logging.info(f"åŠ è½½å·²æœ‰å¤„ç†æ•°æ®æ–‡ä»¶: {out_file}")
        with open(out_file, "r", encoding="utf-8") as f:
            existing_data = [json.loads(line.strip()) for line in f if line.strip()]

    # æå–å·²å­˜åœ¨çš„ id
    for entry in existing_data:
        if "id" in entry:
            id_set.add(entry["id"])

    logging.info(f"å·²æœ‰æ•°æ®æ•°é‡: {len(existing_data)}ï¼Œå°†ç»§ç»­ä¿å­˜åˆ°: {out_file}")
    return out_file, id_set, existing_data



# æ£€æŸ¥å½“å‰æ•°æ®æ˜¯å¦å­˜åœ¨
def find_entry_by_id(out_file, entry_id):
    """
    åœ¨å·²ä¿å­˜çš„ JSON æ–‡ä»¶ä¸­æŸ¥æ‰¾æŒ‡å®š ID çš„æ•°æ®æ¡ç›®ï¼ŒåŒ…å«æ­¥éª¤çŠ¶æ€ä¿¡æ¯ã€‚
    :param out_file: ä¿å­˜æ•°æ®çš„ JSON æ–‡ä»¶è·¯å¾„
    :param entry_id: è¦æŸ¥æ‰¾çš„å”¯ä¸€ ID
    :return: æ‰¾åˆ°çš„å®Œæ•´æ•°æ®æ¡ç›®ï¼ˆåŒ…æ‹¬ steps ä¿¡æ¯ï¼‰ï¼Œæœªæ‰¾åˆ°åˆ™è¿”å› {"id": entry_id, "steps": {}}
    """
    if not os.path.exists(out_file):
        logging.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {out_file}")
        return {"id": entry_id, "steps": {}}

    with open(out_file, "r", encoding="utf-8") as f:
        for line in f:
            entry = json.loads(line.strip())
            if entry.get("id") == entry_id:
                return entry

    return {"id": entry_id, "steps": {}}


def preprocess_text(entry):
    """
    æ ¹æ®æ•°æ®ç±»å‹ï¼ˆdata_classï¼‰å¯¹æ–‡æœ¬è¿›è¡Œé¢„å¤„ç†
    """
    data_class = entry.get("class", "")
    text = entry.get("text", "")
    # å…³é”®å­—åˆ—è¡¨ï¼Œç”¨äºåˆ¤æ–­ä½è´¨é‡æ•°æ®
    skip_keywords = ["å°æ¹¾", "æ¯’", "å¹¿å‘Š", "ç¨¿"]

    if data_class == "article":
        MAX_TEXT_LENGTH = 10000
        text = text[:MAX_TEXT_LENGTH]
        for keyword in skip_keywords:
            if keyword in text:
                return None

    elif data_class == "web":
        text = filter_web_text(entry)
        if text is None:
            # logging.info("è·³è¿‡ä½ä»·å€¼ Web æ•°æ®")
            return None

    elif data_class == "book":
        MAX_TEXT_LENGTH = 10000
        text = clean_book_text(text, max_length=MAX_TEXT_LENGTH)
        # logging.info(f"ä¹¦ç±ç±»æ•°æ®å·²æ¸…æ´—å¹¶æˆªæ–­åˆ° {MAX_TEXT_LENGTH} å­—ç¬¦")

    else:
        logging.warning(f"æœªè¯†åˆ«çš„æ•°æ®ç±»å‹: {data_class}ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬")

    return text


# ç”Ÿæˆå”¯ä¸€ ID
def generate_entry_id(entry):
    entry_str = json.dumps(entry, sort_keys=True, ensure_ascii=False)
    return hashlib.sha256(entry_str.encode("utf-8")).hexdigest()


# 1ï¸âƒ£ **Question Setter**
def process_question_setter(entry, question_setter):
    """
    ç›´æ¥å¤„ç† QuestionSetter é€»è¾‘ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param question_setter: QuestionSetter å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    questions = question_setter.generate_response(
        entry["text"], entry.get("class", "")
    )  # ç”Ÿæˆé—®é¢˜

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["question_setter"] = {"questions": questions}
    # logging.info(f"QuestionSetter å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 2ï¸âƒ£ **Expert Agent**
def process_expert_agent(entry, expert_agent):
    """
    ç›´æ¥å¤„ç† ExpertAgent é€»è¾‘ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param expert_agent: ExpertAgent å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    refined_questions = [
        expert_agent.evaluate_and_refine_question(
            entry["text"], q, entry.get("class", "")
        )
        for q in entry["question_setter"]["questions"]
    ]

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["expert_agent"] = {"refined_questions": refined_questions}
    # logging.info(f"ExpertAgent å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 3ï¸âƒ£ **Virtual Teacher**
def process_virtual_teacher(entry, virtual_teacher):
    """
    ç›´æ¥å¤„ç† VirtualTeacher é€»è¾‘ï¼Œç¡®ä¿æ•°æ®é¡ºåºä¸€è‡´ï¼Œæ— éœ€é¢å¤–ä¿å­˜ç´¢å¼•ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param virtual_teacher: VirtualTeacher å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    processed_results = []

    # è·å– question_setter å’Œ refined_questions
    questions = entry["question_setter"]["questions"]
    refined_questions = entry.get("expert_agent", {}).get("refined_questions", [])

    # éå†è¯•é¢˜ï¼Œæ ¹æ®ç´¢å¼•å¤„ç†ä¼˜åŒ–è¯•é¢˜æˆ–åŸå§‹è¯•é¢˜
    for index, question_data in enumerate(questions):
        # åˆ¤æ–­æ˜¯å¦æœ‰ä¼˜åŒ–åçš„è¯•é¢˜
        if index < len(refined_questions) and refined_questions[index].get(
            "requires_refinement", False
        ):
            current_question = refined_questions[index]["refined_response"]
        else:
            current_question = question_data["response"]

        # åˆ¤æ–­é¢˜å‹ï¼Œæ‰§è¡Œç›¸åº”å¤„ç†
        if question_data.get("question_type") == "multiple_choice":
            conversational_form = virtual_teacher.convert_to_conversational_form(
                entry["text"], current_question, entry.get("class", "")
            )
            cot = virtual_teacher.generate_thinking_chain(
                entry["text"], conversational_form, entry.get("class", "")
            )
        else:
            conversational_form = ""
            # ç”Ÿæˆæ€ç»´é“¾ï¼ˆCoTï¼‰
            cot = virtual_teacher.generate_thinking_chain(
                entry["text"], current_question, entry.get("class", "")
            )

        # ä¿å­˜ç»“æœ
        processed_results.append(
            {"conversational_form": conversational_form, "CoT": cot}
        )

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["virtual_teacher"] = {"processed_results": processed_results}
    # logging.info(f"VirtualTeacher å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 4ï¸âƒ£ **Simulated Learner**
def process_learner(entry, learner):
    """
    ç›´æ¥å¤„ç† SimulatedLearner é€»è¾‘ï¼Œåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ä¼˜åŒ–è¯•é¢˜æˆ–åŸå§‹è¯•é¢˜ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param learner: SimulatedLearner å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    learner_answers = []

    # è·å– question_setter å’Œ refined_questions
    questions = entry["question_setter"]["questions"]
    refined_questions = entry.get("expert_agent", {}).get("refined_questions", [])

    # éå†è¯•é¢˜ï¼Œæ ¹æ®ç´¢å¼•å¤„ç†ä¼˜åŒ–è¯•é¢˜æˆ–åŸå§‹è¯•é¢˜
    for index, question_data in enumerate(questions):
        # åˆ¤æ–­æ˜¯å¦æœ‰ä¼˜åŒ–åçš„è¯•é¢˜
        if index < len(refined_questions) and refined_questions[index].get(
            "requires_refinement", False
        ):
            current_question = refined_questions[index]["refined_response"]
        else:
            current_question = question_data["response"]

        # ç”Ÿæˆç­”æ¡ˆ
        learner_answer = learner.answer_question(current_question)
        learner_answers.append({"answer": learner_answer})

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["simulated_learner"] = {"learner_answers": learner_answers}
    # logging.info(f"SimulatedLearner å¤„ç†å®Œæˆ: {entry_id}")

    return entry


# 5ï¸âƒ£ **Grading Teacher**
def process_grader(entry, grader):
    """
    ç›´æ¥å¤„ç† GradingTeacher é€»è¾‘ï¼ŒåŸºäºä¸“å®¶çš„åŸå§‹è¯•é¢˜æˆ–ä¼˜åŒ–è¯•é¢˜ï¼Œä¸å­¦ç”Ÿä½œç­”è¿›è¡Œè¯„ä¼°æ¨ç†ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param grader: GradingTeacher å®ä¾‹
    """
    entry_id = entry["id"]  # ä½¿ç”¨ entry ä¸­çš„ ID
    evaluations = []

    # è·å–åŸå§‹è¯•é¢˜å’Œä¼˜åŒ–è¯•é¢˜
    questions = entry["question_setter"]["questions"]
    refined_questions = entry.get("expert_agent", {}).get("refined_questions", [])
    learner_answers = entry["simulated_learner"]["learner_answers"]

    # éå†æ¯ä¸ªè¯•é¢˜
    for index, question_data in enumerate(questions):
        # åˆ¤æ–­æ˜¯å¦æœ‰ä¼˜åŒ–åçš„è¯•é¢˜
        if index < len(refined_questions) and refined_questions[index].get(
            "requires_refinement", False
        ):
            current_question = refined_questions[index]["refined_response"]
        else:
            current_question = question_data["response"]

        # è·å–å¯¹åº”çš„å­¦ç”Ÿä½œç­”
        learner_answer = learner_answers[index]["answer"]

        # ç¡®ä¿æœ‰å­¦ç”Ÿä½œç­”åå†è¯„ä¼°
        if learner_answer:
            evaluation = grader.evaluate_answer(
                entry["text"], current_question, learner_answer, entry.get("class", "")
            )
            evaluations.append({"evaluation": evaluation})
        else:
            # å¦‚æœæ²¡æœ‰å¯¹åº”å­¦ç”Ÿä½œç­”ï¼Œè®°å½•ç©ºè¯„ä¼°
            evaluations.append({"evaluation": None})

    # æ·»åŠ å¤„ç†ç»“æœåˆ° entry
    entry["grading_teacher"] = {"evaluations": evaluations}
    # logging.info(f"GradingTeacher å¤„ç†å®Œæˆ: {entry_id}")

    return entry

# ===== tools end ===== #


# ===== main start ===== #
# åŒ…è£… process_entryï¼Œæ·»åŠ åˆ°é˜Ÿåˆ—
def process_entry_with_logging(entry, queue: Queue, *args):
    try:
        text_info = entry["text"][:20]
        logging.info(f"Processing entry: {text_info}")
        result = process_entry(entry, *args)  # è°ƒç”¨ä¸»å¤„ç†å‡½æ•°

        # æ£€æŸ¥è¿”å›å€¼ï¼Œé¿å… None è¢«åŠ å…¥é˜Ÿåˆ—
        if result is not None:
            queue.put(result)
        else:
            logging.warning(
                f"è·³è¿‡ç©ºè¿”å›å€¼æ•°æ®: {text_info}"
            )
    except Exception as e:
        logging.error(f"Error processing entry: {text_info}. Details: {e}")


# åŠ è½½æ•°æ®å‡½æ•°çœç•¥
def process_entry(entry, out_file, question_setter,
    expert_agent, virtual_teacher, learner,
    grader, step, data_class):
    """
    å¤„ç†å•ä¸ªæ•°æ®æ¡ç›®ï¼ŒåŒ…æ‹¬æ‰€æœ‰é˜¶æ®µï¼Œæ”¯æŒé˜¶æ®µæ€§æ‰§è¡Œï¼Œå¹¶åŠ¨æ€è¡¥å…¨å‰ç½®æ­¥éª¤ã€‚
    :param entry: å•æ¡æ•°æ®æ¡ç›®
    :param out_file: ä¿å­˜çš„ JSON æ–‡ä»¶è·¯å¾„
    :param question_setter: QuestionSetter å®ä¾‹
    :param expert_agent: ExpertAgent å®ä¾‹
    :param virtual_teacher: VirtualTeacher å®ä¾‹
    :param learner: SimulatedLearner å®ä¾‹
    :param grader: GradingTeacher å®ä¾‹
    :param step: å½“å‰æ‰§è¡Œçš„æ­¥éª¤ (1-5)
    :data_class: æ•°æ®ç±»å‹ web article book
    """

    # å¦‚æœ entry ä¸­æ²¡æœ‰ IDï¼Œåˆ™ç”Ÿæˆå¹¶å­˜å‚¨
    if "id" not in entry:
        entry["id"] = generate_entry_id(entry)
    entry_id = entry["id"]  # ç›´æ¥ä» entry ä¸­è·å– ID

    # å¦‚æœ entry ä¸­æ²¡æœ‰ data_classï¼Œåˆ™å­˜å‚¨
    if "class" not in entry:
        entry["class"] = data_class

    # æ–‡æœ¬é¢„å¤„ç†  ğŸ“ŒğŸ“ŒğŸ“Œ  text åœ¨æ­¤å¤„è¢«æ”¹å˜
    text_info = entry["text"][:20]
    entry["text"] = preprocess_text(entry)
    if entry["text"] is None:
        # logging.info(f"æ•°æ®ç”±äºä½ä»·å€¼æˆ–æ— æ•ˆè€Œè¢«è·³è¿‡")
        return None

    # 1ï¸âƒ£ **æ£€æŸ¥æ˜¯å¦å·²å¤„ç†è¿‡è¯¥æ¡æ•°æ®**
    existing_entry = find_entry_by_id(out_file, entry_id)
    if existing_entry and existing_entry.get("steps", {}).get(str(step)) == "completed":
        logging.info(f"Step {step}: å·²å®Œæˆï¼Œè·³è¿‡ entry_id={entry_id}")
        return None

    # åˆå¹¶ entry æ•°æ®åˆ° existing_entry ä¸ºäº†æŠŠåŸå§‹è¯­æ–™åŠ è¿›å»
    existing_entry.update(entry)

    steps = existing_entry.get("steps", {})

    # 2ï¸âƒ£ **å®šä¹‰æ­¥éª¤ä¾èµ–å…³ç³»**
    step_dependencies = {1: [], 2: [1], 3: [1, 2], 4: [1, 2], 5: [1, 2, 3, 4]}

    # 3ï¸âƒ£ **é€æ­¥æ£€æŸ¥å’Œæ‰§è¡Œæ­¥éª¤**
    for required_step in step_dependencies[step]:
        if str(required_step) not in steps or steps[str(required_step)] != "completed":
            if required_step == 1:
                existing_entry = process_question_setter(
                    existing_entry, question_setter
                )
                steps["1"] = "completed"
                logging.info(f"Step 1: QuestionSetter è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

            if required_step == 2:
                existing_entry = process_expert_agent(existing_entry, expert_agent)
                steps["2"] = "completed"
                logging.info(f"Step 2: ExpertAgent è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

            if required_step == 3:
                existing_entry = process_virtual_teacher(
                    existing_entry, virtual_teacher
                )
                steps["3"] = "completed"
                logging.info(f"Step 3: VirtualTeacher è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

            if required_step == 4:
                existing_entry = process_learner(existing_entry, learner)
                steps["4"] = "completed"
                logging.info(f"Step 4: SimulatedLearner è‡ªåŠ¨è¡¥å…¨å®Œæˆ")

    # 4ï¸âƒ£ **æ‰§è¡Œç›®æ ‡æ­¥éª¤**
    if str(step) not in steps or steps[str(step)] != "completed":
        if step == 1:
            existing_entry = process_question_setter(existing_entry, question_setter)
        elif step == 2:
            existing_entry = process_expert_agent(existing_entry, expert_agent)
        elif step == 3:
            existing_entry = process_virtual_teacher(existing_entry, virtual_teacher)
        elif step == 4:
            existing_entry = process_learner(existing_entry, learner)
        elif step == 5:
            existing_entry = process_grader(existing_entry, grader)

        steps[str(step)] = "completed"
        logging.info(f"Step {step}: å¤„ç†å®Œæˆ: {text_info}")
    else:
        logging.info(f"Step {step}: å·²å­˜åœ¨ï¼Œè·³è¿‡")

    # 5ï¸âƒ£ **ä¿å­˜æ•°æ®çŠ¶æ€**
    existing_entry["steps"] = steps
    if "text" in existing_entry:
        del existing_entry["text"]  # åˆ é™¤ text æ•°æ®ä»¥èŠ‚çœå­˜å‚¨ç©ºé—´

    return existing_entry  # è¿”å›å¤„ç†å®Œæˆçš„æ•°æ®æ¡ç›®


# ===== main end ===== #


# ğŸ”§ **å‚æ•°è§£æ**
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--prompt-path", default="/home/wyp/project/ForestLLM/prompts", help="Promptæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--data-file",help="JSONLæ–‡ä»¶è·¯å¾„ï¼Œç”¨äºåŠ è½½åŸå§‹æ•°æ®")
    parser.add_argument("--out-dir", help="è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ï¼Œç”¨äºä¿å­˜ç”Ÿæˆçš„æŒ‡ä»¤æ•°æ®é›†",)
    parser.add_argument("--data_class", default="book", help="æ•°æ®ç±»åˆ«ï¼ˆå¦‚ web, article, bookï¼‰")
    parser.add_argument("--model", default="qwen", choices=["chatgpt_o1-preview", "gpt-4", "chatgpt", "qwen"], type=str)
    parser.add_argument("--num_works", default=1, type=int)
    parser.add_argument("--step", type=int, choices=[1, 2, 3, 4, 5], required=True, help="æ‰§è¡Œé˜¶æ®µ",)
    return parser.parse_args()


# ä¸»å‡½æ•°
def main():
    args = parse_args()
    data_file = args.data_file
    out_folder = args.out_dir

    logging.info(f"ä½¿ç”¨æ¨¡å‹: {args.model}")
    logging.info(f"æ•°æ®æ–‡ä»¶: {data_file}")
    # logging.info(f"è¾“å‡ºè·¯å¾„: {out_file}")
    logging.info(f"å½“å‰æ‰§è¡Œé˜¶æ®µ: {args.step}")

    # è‡ªåŠ¨æ¨æ–­ data_class
    # data_class = infer_data_class(data_file)
    # if data_class == "unknown":
    #     logging.error("æ— æ³•æ¨æ–­ data_classï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶ã€‚")
        # data_class = args.data_class

    # åŠ è½½å·²å­˜åœ¨çš„æ•°æ®
    out_file, existing_ids, existing_data = load_existing_data(
        out_folder, args.model, args.data_class
    )

    # åŠ è½½æ•°æ®å’Œåˆå§‹åŒ–
    data = load_data(data_file)

    # åˆå§‹åŒ–ä»£ç†
    question_setter = QuestionSetter(model="qwen")
    expert_agent = ExpertAgent(model="qwen")
    virtual_teacher = VirtualTeacherAgent(model="qwen")
    if args.step >= 4:
        learner = SimulatedLearner(
            model_api=list(args.model),
            model_paths=[
                "/home/wyp/project/swift/models/qwen25_7b_ins",
                "/home/wyp/project/swift/models/minicpm3-4b",
                "/home/wyp/project/swift/models/llama_3_1_8b_ins",
            ],
            model_platforms=["modelscope", "modelscope", "modelscope"],
        )
    else:
        learner = SimulatedLearner(
            model_api=list(args.model),
        )
    grader = GradingTeacher(model="gpt-4")

    # åˆå§‹åŒ–é˜Ÿåˆ—å’Œä¿å­˜çº¿ç¨‹
    data_queue = Queue()
    stop_event = Event()
    saver_thread = Thread(target=data_saver, args=(data_queue, out_file, stop_event, args.num_works))
    saver_thread.start()

    # å¤šçº¿ç¨‹å¤„ç†æ•°æ®
    with ThreadPoolExecutor(max_workers=args.num_works) as executor:  # æ ¹æ®ç¡¬ä»¶è°ƒæ•´çº¿ç¨‹æ•°
        futures = {executor.submit(process_entry_with_logging, entry, data_queue, out_file,
                                   question_setter, expert_agent, virtual_teacher,
                                   learner, grader, args.step, args.data_class, ): entry 
                    for entry in data
                    }

        # ä½¿ç”¨ tqdm ç›‘æ§è¿›åº¦
        for future in tqdm(as_completed(futures), total=len(futures), desc="Processing Entries", unit="entry"):
            try:
                future.result()
            except Exception as e:
                logging.error(f"Error in future result: {e}")

    # ç­‰å¾…é˜Ÿåˆ—å®Œæˆæ‰€æœ‰ä»»åŠ¡
    data_queue.join()
    stop_event.set()
    saver_thread.join()

    logging.info(f"æ‰€æœ‰æ•°æ®å·²ä¿å­˜åˆ° {out_file}")


if __name__ == "__main__":
    main()


# python tools/run_mutil.py --data-file /home/wyp/project/forest/forestllm-main/mateinfo/all_book.jsonl --out-dir /home/wyp/project/forest/forestllm-main/outputs/0321 --step 3 --data_class book --num_works 8